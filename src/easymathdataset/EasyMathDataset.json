{
  "Linear Algebra": [
    {
      "id": "1",
      "statement": "The sum of two vectors in a vector space is a vector in the same vector space.",
      "proof": "To prove that the sum of two vectors in a vector space is also a vector in the same vector space, we need to refer to the definition of a vector space and its axioms.\n\nLet $V$ be a vector space over a field $\\mathbb{F}$. By definition, a vector space is a set equipped with two operations: vector addition and scalar multiplication, satisfying certain axioms.\n\nOne of these axioms is the **closure under addition**. This axiom states that for any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $V$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $V$.\n\n**Proof:**\n\n1. **Given:** $\\mathbf{u}, \\mathbf{v} \\in V$.\n\n2. **Axiom of Closure under Addition:** By the definition of a vector space, for any vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $V$, the sum $\\mathbf{u} + \\mathbf{v}$ must also be in $V$.\n\n3. **Conclusion:** Therefore, the sum of two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in the vector space $V$ is also a vector in $V$.\n\nThis property is fundamental to the structure of a vector space and is one of the key reasons why vector spaces are closed under addition. Thus, the sum of two vectors in a vector space is indeed a vector in the same vector space."
    },
    {
      "id": "2",
      "statement": "The dot product of a vector with itself is non-negative.",
      "proof": "To prove that the dot product of a vector with itself is non-negative, consider a vector $\\mathbf{v} = (v_1, v_2, \\ldots, v_n)$ in $\\mathbb{R}^n$. The dot product of $\\mathbf{v}$ with itself is given by:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\cdots + v_n^2.\n\\]\n\nEach term $v_i^2$ in this sum is the square of a real number $v_i$. Since the square of any real number is non-negative, we have $v_i^2 \\geq 0$ for all $i = 1, 2, \\ldots, n$.\n\nTherefore, the entire sum $v_1^2 + v_2^2 + \\cdots + v_n^2$ is non-negative, as it is a sum of non-negative terms. Thus, we conclude:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{v} \\geq 0.\n\\]\n\nThis completes the proof that the dot product of a vector with itself is non-negative."
    },
    {
      "id": "3",
      "statement": "Two vectors in the plane are parallel if one is a scalar multiple of the other.",
      "proof": "To prove that two vectors in the plane are parallel if and only if one is a scalar multiple of the other, let's consider two vectors $\\mathbf{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$ and $\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ in $\\mathbb{R}^2$.\n\n**Proof:**\n\n**(If direction):** Assume that $\\mathbf{u}$ and $\\mathbf{v}$ are parallel. By definition, two vectors are parallel if they lie on the same line through the origin, which means there exists a scalar $\\lambda \\in \\mathbb{R}$ such that $\\mathbf{v} = \\lambda \\mathbf{u}$. This implies:\n\n\\[\n\\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\lambda \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{pmatrix} \\lambda u_1 \\\\ \\lambda u_2 \\end{pmatrix}\n\\]\n\nThus, $v_1 = \\lambda u_1$ and $v_2 = \\lambda u_2$, showing that $\\mathbf{v}$ is a scalar multiple of $\\mathbf{u}$.\n\n**(Only if direction):** Conversely, assume that $\\mathbf{v}$ is a scalar multiple of $\\mathbf{u}$, i.e., there exists a scalar $\\lambda \\in \\mathbb{R}$ such that $\\mathbf{v} = \\lambda \\mathbf{u}$. This means:\n\n\\[\n\\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} \\lambda u_1 \\\\ \\lambda u_2 \\end{pmatrix}\n\\]\n\nThis implies $v_1 = \\lambda u_1$ and $v_2 = \\lambda u_2$. Therefore, $\\mathbf{v}$ lies on the line through the origin in the direction of $\\mathbf{u}$, which means $\\mathbf{u}$ and $\\mathbf{v}$ are parallel.\n\nThus, we have shown that two vectors in the plane are parallel if and only if one is a scalar multiple of the other."
    },
    {
      "id": "4",
      "statement": "A square matrix with zeros everywhere except its main diagonal is called a diagonal matrix.",
      "proof": "To prove that a square matrix with zeros everywhere except its main diagonal is a diagonal matrix, we start by considering the definition of a diagonal matrix.\n\nA diagonal matrix is a square matrix $A = [a_{ij}]$ of order $n \\times n$ such that all its off-diagonal elements are zero. This means that for all $i \\neq j$, $a_{ij} = 0$. The elements on the main diagonal, $a_{ii}$, can be any real or complex numbers.\n\nNow, let's consider a square matrix $A = [a_{ij}]$ of order $n \\times n$ with zeros everywhere except its main diagonal. This means that for all $i \\neq j$, $a_{ij} = 0$, and for $i = j$, $a_{ii}$ can be any value.\n\nSince the condition $a_{ij} = 0$ for all $i \\neq j$ is satisfied, this matrix $A$ meets the definition of a diagonal matrix. Therefore, a square matrix with zeros everywhere except its main diagonal is indeed a diagonal matrix. \n\nIn LaTeX, we can represent a diagonal matrix as follows:\n\n\\[\nA = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\\]\n\nThis matrix $A$ is a diagonal matrix because all off-diagonal elements are zero, and the main diagonal elements $a_{11}, a_{22}, \\ldots, a_{nn}$ can be any values."
    },
    {
      "id": "5",
      "statement": "The zero vector is the additive identity in any vector space.",
      "proof": "To prove that the zero vector is the additive identity in any vector space, we need to show that for any vector $\\mathbf{v}$ in a vector space $V$, the equation $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ holds, where $\\mathbf{0}$ is the zero vector in $V$.\n\n**Proof:**\n\n1. **Definition of a Vector Space:**\n   A vector space $V$ over a field $\\mathbb{F}$ is a set equipped with two operations: vector addition and scalar multiplication, satisfying certain axioms. One of these axioms is the existence of an additive identity.\n\n2. **Existence of the Zero Vector:**\n   By the axioms of a vector space, there exists a vector $\\mathbf{0} \\in V$ such that for every vector $\\mathbf{v} \\in V$, the equation $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ holds. This vector $\\mathbf{0}$ is called the zero vector or the additive identity.\n\n3. **Verification:**\n   Let $\\mathbf{v}$ be any vector in $V$. By the definition of the zero vector, we have:\n   \\[\n   \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n   \\]\n   This equation confirms that adding the zero vector to any vector $\\mathbf{v}$ results in the vector $\\mathbf{v}$ itself, satisfying the property of an additive identity.\n\n4. **Uniqueness:**\n   Suppose there exists another vector $\\mathbf{0}' \\in V$ such that for every vector $\\mathbf{v} \\in V$, $\\mathbf{v} + \\mathbf{0}' = \\mathbf{v}$. We need to show that $\\mathbf{0} = \\mathbf{0}'$. Consider the vector $\\mathbf{0}$:\n   \\[\n   \\mathbf{0} + \\mathbf{0}' = \\mathbf{0} \\quad \\text{(since $\\mathbf{0}'$ is an additive identity)}\n   \\]\n   \\[\n   \\mathbf{0} + \\mathbf{0}' = \\mathbf{0}' \\quad \\text{(since $\\mathbf{0}$ is an additive identity)}\n   \\]\n   Therefore, $\\mathbf{0} = \\mathbf{0}'$, proving the uniqueness of the zero vector.\n\nThus, the zero vector $\\mathbf{0}$ is indeed the additive identity in any vector space $V$."
    },
    {
      "id": "6",
      "statement": "The determinant of a 2x2 identity matrix is 1.",
      "proof": "To prove that the determinant of a \\(2 \\times 2\\) identity matrix is 1, let's first define the \\(2 \\times 2\\) identity matrix:\n\n\\[\nI_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\]\n\nThe determinant of a \\(2 \\times 2\\) matrix \n\n\\[\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\]\n\nis given by the formula:\n\n\\[\n\\det(A) = ad - bc\n\\]\n\nApplying this formula to the identity matrix \\(I_2\\), we have:\n\n\\[\n\\det(I_2) = (1)(1) - (0)(0) = 1 - 0 = 1\n\\]\n\nTherefore, the determinant of the \\(2 \\times 2\\) identity matrix is indeed 1."
    },
    {
      "id": "7",
      "statement": "If a matrix has a row of zeros, its determinant is zero.",
      "proof": "To prove that if a matrix has a row of zeros, its determinant is zero, we will use the properties of determinants.\n\nLet $A$ be an $n \\times n$ matrix with a row of zeros. Without loss of generality, assume that the $i$-th row of $A$ is a row of zeros. We want to show that $\\det(A) = 0$.\n\nThe determinant of a matrix can be computed using the Laplace expansion (cofactor expansion) along any row or column. Let's expand the determinant along the $i$-th row, which is the row of zeros. The formula for the determinant using the $i$-th row is:\n\n\\[\n\\det(A) = \\sum_{j=1}^{n} a_{ij} C_{ij}\n\\]\n\nwhere $a_{ij}$ is the element in the $i$-th row and $j$-th column of $A$, and $C_{ij}$ is the cofactor of $a_{ij}$.\n\nSince the $i$-th row is a row of zeros, we have $a_{ij} = 0$ for all $j = 1, 2, \\ldots, n$. Therefore, each term in the sum is zero:\n\n\\[\na_{ij} C_{ij} = 0 \\cdot C_{ij} = 0\n\\]\n\nThus, the entire sum is zero:\n\n\\[\n\\det(A) = \\sum_{j=1}^{n} 0 = 0\n\\]\n\nTherefore, if a matrix has a row of zeros, its determinant is zero. This completes the proof."
    },
    {
      "id": "8",
      "statement": "Subtracting a vector from itself results in the zero vector.",
      "proof": "To prove that subtracting a vector from itself results in the zero vector, let's consider a vector $\\mathbf{v}$ in a vector space $V$. The operation of vector subtraction is defined as the addition of a vector and the additive inverse (or negative) of another vector. \n\nThe additive inverse of a vector $\\mathbf{v}$ is denoted as $-\\mathbf{v}$, and it satisfies the property:\n\n\\[\n\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n\\]\n\nwhere $\\mathbf{0}$ is the zero vector in the vector space $V$.\n\nNow, consider the subtraction of the vector $\\mathbf{v}$ from itself:\n\n\\[\n\\mathbf{v} - \\mathbf{v} = \\mathbf{v} + (-\\mathbf{v})\n\\]\n\nBy the property of the additive inverse, we have:\n\n\\[\n\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n\\]\n\nThus, subtracting a vector from itself results in the zero vector:\n\n\\[\n\\mathbf{v} - \\mathbf{v} = \\mathbf{0}\n\\]\n\nThis completes the proof."
    },
    {
      "id": "9",
      "statement": "The transpose of a transpose of a matrix is the original matrix.",
      "proof": "To prove that the transpose of the transpose of a matrix is the original matrix, let's consider a matrix $A$ of size $m \\times n$. The transpose of $A$, denoted by $A^T$, is an $n \\times m$ matrix where the rows of $A$ become the columns of $A^T$.\n\nNow, we need to show that the transpose of $A^T$, denoted by $(A^T)^T$, is equal to $A$.\n\nLet $A = [a_{ij}]$, where $1 \\leq i \\leq m$ and $1 \\leq j \\leq n$. The element in the $i$-th row and $j$-th column of $A$ is $a_{ij}$.\n\nThe transpose $A^T$ is given by $A^T = [a_{ji}]$, where $1 \\leq j \\leq n$ and $1 \\leq i \\leq m$. This means that the element in the $j$-th row and $i$-th column of $A^T$ is $a_{ji}$.\n\nNow, consider the transpose of $A^T$, which is $(A^T)^T$. The element in the $i$-th row and $j$-th column of $(A^T)^T$ is the element in the $j$-th row and $i$-th column of $A^T$, which is $a_{ji}$. Therefore, $(A^T)^T = [a_{ij}]$, which is exactly the original matrix $A$.\n\nThus, we have shown that $(A^T)^T = A$. Therefore, the transpose of the transpose of a matrix is the original matrix. \n\nIn summary, the proof can be written as:\n\n\\[\n(A^T)^T = A\n\\]\n\nThis completes the proof."
    },
    {
      "id": "10",
      "statement": "The inverse of a 2x2 identity matrix is the identity matrix itself.",
      "proof": "To prove that the inverse of a \\(2 \\times 2\\) identity matrix is the identity matrix itself, we start by considering the \\(2 \\times 2\\) identity matrix, denoted as \\(I_2\\):\n\n\\[\nI_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\]\n\nThe definition of an inverse matrix \\(A^{-1}\\) for a matrix \\(A\\) is such that:\n\n\\[\nA \\cdot A^{-1} = A^{-1} \\cdot A = I\n\\]\n\nwhere \\(I\\) is the identity matrix of the same size as \\(A\\).\n\nIn this case, we want to show that the inverse of \\(I_2\\) is itself, i.e., \\(I_2^{-1} = I_2\\).\n\nLet's verify this by computing the product of \\(I_2\\) with itself:\n\n\\[\nI_2 \\cdot I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 & 1 \\cdot 0 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 0 & 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n\\]\n\nSince \\(I_2 \\cdot I_2 = I_2\\), it follows that \\(I_2\\) is its own inverse. Therefore, the inverse of the \\(2 \\times 2\\) identity matrix is the identity matrix itself:\n\n\\[\nI_2^{-1} = I_2\n\\]"
    },
    {
      "id": "11",
      "statement": "Matrix multiplication is associative but not necessarily commutative.",
      "proof": "To prove that matrix multiplication is associative but not necessarily commutative, we need to demonstrate two things:\n\n1. **Associativity of Matrix Multiplication:**\n\n   Let $A$, $B$, and $C$ be matrices such that the products $AB$ and $BC$ are defined. We need to show that $(AB)C = A(BC)$.\n\n   The product of matrices is defined as follows: if $A$ is an $m \\times n$ matrix, $B$ is an $n \\times p$ matrix, and $C$ is a $p \\times q$ matrix, then the product $AB$ is an $m \\times p$ matrix, and $BC$ is an $n \\times q$ matrix. The product $(AB)C$ is an $m \\times q$ matrix, and $A(BC)$ is also an $m \\times q$ matrix.\n\n   The $(i, j)$-th entry of $(AB)C$ is given by:\n\n   $$\n   \\left((AB)C\\right)_{ij} = \\sum_{k=1}^{p} (AB)_{ik} C_{kj} = \\sum_{k=1}^{p} \\left( \\sum_{l=1}^{n} A_{il} B_{lk} \\right) C_{kj}.\n   $$\n\n   The $(i, j)$-th entry of $A(BC)$ is given by:\n\n   $$\n   \\left(A(BC)\\right)_{ij} = \\sum_{l=1}^{n} A_{il} (BC)_{lj} = \\sum_{l=1}^{n} A_{il} \\left( \\sum_{k=1}^{p} B_{lk} C_{kj} \\right).\n   $$\n\n   By rearranging the order of summation, we see that:\n\n   $$\n   \\sum_{k=1}^{p} \\left( \\sum_{l=1}^{n} A_{il} B_{lk} \\right) C_{kj} = \\sum_{l=1}^{n} A_{il} \\left( \\sum_{k=1}^{p} B_{lk} C_{kj} \\right).\n   $$\n\n   Therefore, $(AB)C = A(BC)$, proving that matrix multiplication is associative.\n\n2. **Non-commutativity of Matrix Multiplication:**\n\n   To show that matrix multiplication is not necessarily commutative, we need to find matrices $A$ and $B$ such that $AB \\neq BA$.\n\n   Consider the matrices:\n\n   $$\n   A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}.\n   $$\n\n   Then,\n\n   $$\n   AB = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix},\n   $$\n\n   and\n\n   $$\n   BA = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}.\n   $$\n\n   Clearly, $AB \\neq BA$. Thus, matrix multiplication is not necessarily commutative.\n\nIn conclusion, matrix multiplication is associative, as shown by the equality $(AB)C = A(BC)$, but it is not necessarily commutative, as demonstrated by the example where $AB \\neq BA$."
    },
    {
      "id": "12",
      "statement": "The eigenvectors of a matrix corresponding to distinct eigenvalues are linearly independent.",
      "proof": "To prove that the eigenvectors of a matrix corresponding to distinct eigenvalues are linearly independent, let's consider a matrix $A \\in \\mathbb{C}^{n \\times n}$ and suppose it has $k$ distinct eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_k$ with corresponding eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k$. We want to show that the set $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}$ is linearly independent.\n\nAssume, for the sake of contradiction, that these eigenvectors are linearly dependent. Then there exist scalars $c_1, c_2, \\ldots, c_k$, not all zero, such that:\n\n\\[\nc_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k = \\mathbf{0}.\n\\]\n\nWithout loss of generality, assume $c_1 \\neq 0$. We can express $\\mathbf{v}_1$ as a linear combination of the other eigenvectors:\n\n\\[\n\\mathbf{v}_1 = -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k.\n\\]\n\nNow, apply the matrix $A$ to both sides of the equation:\n\n\\[\nA \\mathbf{v}_1 = A \\left( -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k \\right).\n\\]\n\nSince $\\mathbf{v}_1$ is an eigenvector corresponding to the eigenvalue $\\lambda_1$, we have $A \\mathbf{v}_1 = \\lambda_1 \\mathbf{v}_1$. Substituting this into the equation gives:\n\n\\[\n\\lambda_1 \\mathbf{v}_1 = -\\frac{c_2}{c_1} A \\mathbf{v}_2 - \\frac{c_3}{c_1} A \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} A \\mathbf{v}_k.\n\\]\n\nSince each $\\mathbf{v}_i$ is an eigenvector of $A$ with eigenvalue $\\lambda_i$, we have $A \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$ for $i = 2, 3, \\ldots, k$. Substituting these into the equation gives:\n\n\\[\n\\lambda_1 \\mathbf{v}_1 = -\\frac{c_2}{c_1} \\lambda_2 \\mathbf{v}_2 - \\frac{c_3}{c_1} \\lambda_3 \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\lambda_k \\mathbf{v}_k.\n\\]\n\nSubstitute the expression for $\\mathbf{v}_1$ from the linear dependence assumption:\n\n\\[\n\\lambda_1 \\left( -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k \\right) = -\\frac{c_2}{c_1} \\lambda_2 \\mathbf{v}_2 - \\frac{c_3}{c_1} \\lambda_3 \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\lambda_k \\mathbf{v}_k.\n\\]\n\nThis simplifies to:\n\n\\[\n-\\frac{c_2}{c_1} (\\lambda_1 - \\lambda_2) \\mathbf{v}_2 - \\frac{c_3}{c_1} (\\lambda_1 - \\lambda_3) \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} (\\lambda_1 - \\lambda_k) \\mathbf{v}_k = \\mathbf{0}.\n\\]\n\nSince the eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_k$ are distinct, each term $(\\lambda_1 - \\lambda_i) \\neq 0$ for $i = 2, 3, \\ldots, k$. Therefore, each coefficient $-\\frac{c_i}{c_1} (\\lambda_1 - \\lambda_i)$ must be zero, which implies $c_i = 0$ for $i = 2, 3, \\ldots, k$. This contradicts our assumption that not all $c_i$ are zero. Hence, the eigenvectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}$ must be linearly independent."
    },
    {
      "id": "13",
      "statement": "The rank of a matrix is equal to the number of pivot columns in its row echelon form.",
      "proof": "To prove that the rank of a matrix is equal to the number of pivot columns in its row echelon form, we need to understand the concepts of rank, pivot columns, and row echelon form.\n\n**Definitions:**\n\n1. **Rank of a Matrix:** The rank of a matrix $A$, denoted as $\\text{rank}(A)$, is the dimension of the column space (or the row space) of the matrix. It is the maximum number of linearly independent column vectors (or row vectors) in the matrix.\n\n2. **Row Echelon Form (REF):** A matrix is in row echelon form if:\n   - All nonzero rows are above any rows of all zeros.\n   - The leading entry of each nonzero row (called a pivot) is to the right of the leading entry of the previous row.\n   - The entries below each pivot are zeros.\n\n3. **Pivot Columns:** In the row echelon form of a matrix, the pivot columns are the columns that contain the leading entries (pivots) of the nonzero rows.\n\n**Proof:**\n\n1. **Transforming to Row Echelon Form:**\n   - Any matrix $A$ can be transformed into a row echelon form $U$ using a series of elementary row operations. These operations do not change the row space of the matrix, and hence, do not change the rank of the matrix.\n\n2. **Identifying Pivot Columns:**\n   - In the row echelon form $U$, each pivot corresponds to a leading entry in a nonzero row. The number of such pivots is equal to the number of nonzero rows in $U$.\n\n3. **Rank and Pivot Columns:**\n   - The number of nonzero rows in $U$ is equal to the number of pivot columns because each nonzero row has exactly one pivot.\n   - Since the row operations do not change the row space, the rank of the original matrix $A$ is equal to the rank of the matrix $U$.\n   - The rank of $U$ is the number of linearly independent rows, which is exactly the number of nonzero rows, and hence, the number of pivot columns.\n\nTherefore, the rank of the matrix $A$ is equal to the number of pivot columns in its row echelon form $U$.\n\nThus, we have shown that the rank of a matrix is equal to the number of pivot columns in its row echelon form."
    },
    {
      "id": "14",
      "statement": "A vector is said to have scalar components if all entries within it are scalar values.",
      "proof": "To prove that a vector has scalar components if all entries within it are scalar values, we need to understand the definitions involved.\n\nA vector in a vector space $V$ over a field $\\mathbb{F}$ is an ordered list of elements from $\\mathbb{F}$. These elements are called the components or entries of the vector. A scalar is an element of the field $\\mathbb{F$.\n\n**Proof:**\n\nLet $\\mathbf{v} = (v_1, v_2, \\ldots, v_n)$ be a vector in the vector space $V$ over the field $\\mathbb{F}$. By definition, each $v_i$ (for $i = 1, 2, \\ldots, n$) is an element of $\\mathbb{F}$, which means each $v_i$ is a scalar.\n\nTherefore, if all entries $v_1, v_2, \\ldots, v_n$ of the vector $\\mathbf{v}$ are elements of the field $\\mathbb{F}$, then $\\mathbf{v}$ is said to have scalar components.\n\nThis completes the proof that a vector has scalar components if all its entries are scalar values."
    },
    {
      "id": "15",
      "statement": "The space spanned by a single non-zero vector is a line through the origin.",
      "proof": "To prove that the space spanned by a single non-zero vector is a line through the origin, let's consider a vector space $V$ over a field $\\mathbb{F}$ and a non-zero vector $\\mathbf{v} \\in V$. The span of $\\mathbf{v}$, denoted by $\\text{span}(\\mathbf{v})$, is defined as the set of all scalar multiples of $\\mathbf{v}$:\n\n\\[\n\\text{span}(\\mathbf{v}) = \\{ \\alpha \\mathbf{v} \\mid \\alpha \\in \\mathbb{F} \\}\n\\]\n\nWe need to show that this set forms a line through the origin in the vector space $V$.\n\n1. **Contains the Origin**: The zero vector $\\mathbf{0}$ is in $\\text{span}(\\mathbf{v})$ because we can choose the scalar $\\alpha = 0$, giving $\\alpha \\mathbf{v} = 0 \\cdot \\mathbf{v} = \\mathbf{0}$. Thus, the origin is included in the span.\n\n2. **Linearity**: For any two scalars $\\alpha, \\beta \\in \\mathbb{F}$, the linear combination $\\alpha \\mathbf{v} + \\beta \\mathbf{v}$ is also in $\\text{span}(\\mathbf{v})$. Specifically, $\\alpha \\mathbf{v} + \\beta \\mathbf{v} = (\\alpha + \\beta) \\mathbf{v}$, which is a scalar multiple of $\\mathbf{v}$, hence in the span.\n\n3. **One-Dimensional**: The set $\\text{span}(\\mathbf{v})$ is one-dimensional because any vector in this set can be expressed as a scalar multiple of $\\mathbf{v}$. There are no other independent directions in this set, as any other vector in $\\text{span}(\\mathbf{v})$ is a linear combination of $\\mathbf{v}$.\n\n4. **Geometric Interpretation**: Geometrically, the set of all scalar multiples of a non-zero vector $\\mathbf{v}$ forms a line through the origin. This is because for any point on this line, there exists a scalar $\\alpha$ such that the point can be represented as $\\alpha \\mathbf{v}$. The line extends infinitely in both directions, corresponding to positive and negative values of $\\alpha$.\n\nTherefore, the space spanned by a single non-zero vector $\\mathbf{v}$ is indeed a line through the origin in the vector space $V$."
    }
  ],
  "Algebra": [
    {
      "id": "1",
      "statement": "The sum of any number and zero is the number itself.",
      "proof": "To prove the statement \"The sum of any number and zero is the number itself,\" we will use the properties of addition in the context of algebra.\n\nLet $a$ be any arbitrary number. We want to show that $a + 0 = a$.\n\nThis property is known as the **additive identity property**. In the set of real numbers (or any set with a defined addition operation that includes an additive identity), the number $0$ is defined as the additive identity. This means that for any number $a$, adding $0$ to $a$ does not change the value of $a$.\n\n**Proof:**\n\n1. **Definition of Additive Identity:** By definition, the additive identity is a number which, when added to any number $a$, results in the number $a$ itself. In the real numbers, this additive identity is $0$.\n\n2. **Using the Definition:** According to the definition of the additive identity, for any number $a$, we have:\n   \\[\n   a + 0 = a\n   \\]\n\n3. **Conclusion:** Since $0$ is the additive identity, the sum of any number $a$ and $0$ is $a$. Therefore, the statement is true.\n\nThus, we have shown that the sum of any number and zero is the number itself, as required."
    },
    {
      "id": "2",
      "statement": "If you multiply any number by one, the result is the number itself.",
      "proof": "To prove that multiplying any number by one results in the number itself, we need to show that for any real number $a$, the equation $a \\cdot 1 = a$ holds true.\n\nThis property is known as the \"multiplicative identity property\" of real numbers. The number 1 is called the multiplicative identity because it does not change the value of any number it is multiplied by.\n\n**Proof:**\n\n1. **Definition of Multiplicative Identity:**\n\n   By definition, the multiplicative identity is a number $e$ such that for any real number $a$, the equation $a \\cdot e = a$ holds true. In the real number system, this identity is the number 1.\n\n2. **Verification:**\n\n   Let $a$ be any real number. We need to verify that $a \\cdot 1 = a$.\n\n   - Consider the expression $a \\cdot 1$.\n   - By the definition of the multiplicative identity, we have $a \\cdot 1 = a$.\n\n3. **Conclusion:**\n\n   Since the definition of the multiplicative identity holds for any real number $a$, we conclude that multiplying any number by one results in the number itself. Therefore, for any real number $a$, $a \\cdot 1 = a$.\n\nThis completes the proof that multiplying any number by one results in the number itself."
    },
    {
      "id": "3",
      "statement": "The sum of two identical numbers is twice that number.",
      "proof": "To prove the statement \"The sum of two identical numbers is twice that number,\" let's denote the identical number by $x$.\n\nThe sum of two identical numbers $x$ is given by:\n\n\\[ x + x. \\]\n\nThis can be simplified using the properties of addition:\n\n\\[ x + x = 2x. \\]\n\nThe expression $2x$ represents \"twice the number $x$,\" as multiplying a number by 2 gives twice its value.\n\nThus, the sum of two identical numbers $x$ is indeed twice that number, which completes the proof."
    },
    {
      "id": "4",
      "statement": "The product of any number and zero is zero.",
      "proof": "To prove that the product of any number and zero is zero, we can use the properties of real numbers, specifically the distributive property and the additive identity.\n\nLet $a$ be any real number. We want to show that $a \\cdot 0 = 0$.\n\nConsider the expression $a \\cdot 0$. We can write $0$ as $0 + 0$. Thus, we have:\n\n\\[\na \\cdot 0 = a \\cdot (0 + 0)\n\\]\n\nUsing the distributive property of multiplication over addition, we can expand the right-hand side:\n\n\\[\na \\cdot (0 + 0) = a \\cdot 0 + a \\cdot 0\n\\]\n\nThis gives us:\n\n\\[\na \\cdot 0 = a \\cdot 0 + a \\cdot 0\n\\]\n\nNow, let's subtract $a \\cdot 0$ from both sides of the equation:\n\n\\[\na \\cdot 0 - a \\cdot 0 = (a \\cdot 0 + a \\cdot 0) - a \\cdot 0\n\\]\n\nSimplifying both sides, we get:\n\n\\[\n0 = a \\cdot 0\n\\]\n\nThus, we have shown that the product of any number $a$ and zero is zero."
    },
    {
      "id": "5",
      "statement": "When you subtract a number from itself, the result is zero.",
      "proof": "To prove that subtracting a number from itself results in zero, we can use the properties of real numbers, specifically the properties of addition and subtraction.\n\nLet $a$ be any real number. We want to show that $a - a = 0$.\n\n1. **Definition of Subtraction**: Subtraction is defined as the addition of the additive inverse. That is, for any real number $a$, $a - a$ is defined as $a + (-a)$, where $-a$ is the additive inverse of $a$.\n\n2. **Additive Inverse Property**: By the definition of the additive inverse, $a + (-a) = 0$. This is because the additive inverse of a number is the number that, when added to the original number, results in the additive identity, which is 0.\n\nTherefore, $a - a = a + (-a) = 0$.\n\nThis completes the proof that subtracting a number from itself results in zero."
    },
    {
      "id": "6",
      "statement": "Adding a negative number is the same as subtracting its positive counterpart.",
      "proof": "To prove that adding a negative number is the same as subtracting its positive counterpart, we need to show that for any real number $a$ and any positive real number $b$, the expression $a + (-b)$ is equal to $a - b$.\n\n**Proof:**\n\n1. **Definition of Subtraction:**\n\n   By definition, subtraction of a number is equivalent to adding its additive inverse. That is, for any real numbers $a$ and $b$, the expression $a - b$ is defined as $a + (-b)$.\n\n2. **Expression Equivalence:**\n\n   Given the definition above, we have:\n   \\[\n   a - b = a + (-b)\n   \\]\n\n3. **Conclusion:**\n\n   Therefore, adding a negative number $-b$ to $a$ is the same as subtracting the positive number $b$ from $a$. This completes the proof.\n\nThus, the statement is mathematically verified: adding a negative number is equivalent to subtracting its positive counterpart."
    },
    {
      "id": "7",
      "statement": "In any expression, you can change the order of addition without affecting the result: a + b = b + a.",
      "proof": "The statement that you can change the order of addition without affecting the result is known as the commutative property of addition. We will provide a proof for this property using the axioms of arithmetic.\n\n**Proof:**\n\nThe commutative property of addition states that for any two real numbers $a$ and $b$, the equation $a + b = b + a$ holds true.\n\nTo prove this, we rely on the axioms of arithmetic, particularly the axioms that define the properties of addition. These axioms include:\n\n1. **Closure of Addition**: For any two real numbers $a$ and $b$, the sum $a + b$ is also a real number.\n2. **Associative Property of Addition**: For any three real numbers $a$, $b$, and $c$, the equation $(a + b) + c = a + (b + c)$ holds.\n3. **Existence of Additive Identity**: There exists a real number $0$ such that for any real number $a$, $a + 0 = a$.\n4. **Existence of Additive Inverses**: For every real number $a$, there exists a real number $-a$ such that $a + (-a) = 0$.\n\nThe commutative property is often taken as an axiom itself, but we can provide an intuitive justification based on the properties of numbers and operations:\n\nConsider the number line representation of addition. When we add $a + b$, we start at point $0$, move $a$ units to the right, and then move $b$ units further to the right. The final position on the number line is $a + b$.\n\nNow, consider adding $b + a$. We start at point $0$, move $b$ units to the right, and then move $a$ units further to the right. The final position on the number line is $b + a$.\n\nIn both cases, the final position on the number line is the same, demonstrating that $a + b = b + a$.\n\nThus, the commutative property of addition is verified, and we conclude that the order of addition does not affect the result. Therefore, for any real numbers $a$ and $b$, $a + b = b + a$."
    },
    {
      "id": "8",
      "statement": "In any expression, you can change the order of multiplication without affecting the result: ab = ba.",
      "proof": "The statement that in any expression, you can change the order of multiplication without affecting the result is known as the commutative property of multiplication. We will provide a proof for this property using the axioms of arithmetic.\n\n**Proof:**\n\nThe commutative property of multiplication states that for any two numbers $a$ and $b$, the equation $ab = ba$ holds true. We will prove this using the axioms of a field, which include the properties of addition and multiplication.\n\n1. **Axioms of a Field:**\n   - **Associative Property of Addition:** For all $a, b, c$, $(a + b) + c = a + (b + c)$.\n   - **Commutative Property of Addition:** For all $a, b$, $a + b = b + a$.\n   - **Additive Identity:** There exists an element $0$ such that for all $a$, $a + 0 = a$.\n   - **Additive Inverse:** For every $a$, there exists an element $-a$ such that $a + (-a) = 0$.\n   - **Associative Property of Multiplication:** For all $a, b, c$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n   - **Multiplicative Identity:** There exists an element $1$ such that for all $a$, $a \\cdot 1 = a$.\n   - **Distributive Property:** For all $a, b, c$, $a \\cdot (b + c) = a \\cdot b + a \\cdot c$.\n\n2. **Proof of Commutative Property of Multiplication:**\n\n   We want to show that for any $a$ and $b$, $ab = ba$.\n\n   Consider the expression $a(b + c) = ab + ac$ by the distributive property. Similarly, $b(a + c) = ba + bc$.\n\n   Now, let's set $c = 0$. Then we have:\n   \\[\n   a(b + 0) = ab + a \\cdot 0 = ab\n   \\]\n   and\n   \\[\n   b(a + 0) = ba + b \\cdot 0 = ba\n   \\]\n\n   Since $b + 0 = b$ and $a + 0 = a$, we have:\n   \\[\n   a \\cdot b = ab\n   \\]\n   and\n   \\[\n   b \\cdot a = ba\n   \\]\n\n   By the distributive property, $a(b + 0) = ab$ and $b(a + 0) = ba$ imply that $ab = ba$.\n\nThus, the commutative property of multiplication is proven using the axioms of arithmetic. Therefore, for any two numbers $a$ and $b$, the order of multiplication does not affect the result, i.e., $ab = ba$."
    },
    {
      "id": "9",
      "statement": "Multiplying a number by a positive power of ten shifts the decimal point to the right by that power.",
      "proof": "To prove this statement, let's consider a number $x$ and a positive integer $n$. We want to show that multiplying $x$ by $10^n$ shifts the decimal point of $x$ to the right by $n$ places.\n\nFirst, express $x$ in its decimal form. Suppose $x = a_k a_{k-1} \\ldots a_1 a_0 . b_1 b_2 \\ldots b_m$, where $a_i$ and $b_j$ are digits (0 through 9), and the dot represents the decimal point. This can be written as:\n\n\\[\nx = \\sum_{i=0}^{k} a_i \\cdot 10^i + \\sum_{j=1}^{m} b_j \\cdot 10^{-j}\n\\]\n\nNow, consider multiplying $x$ by $10^n$:\n\n\\[\nx \\cdot 10^n = \\left( \\sum_{i=0}^{k} a_i \\cdot 10^i + \\sum_{j=1}^{m} b_j \\cdot 10^{-j} \\right) \\cdot 10^n\n\\]\n\nDistribute $10^n$:\n\n\\[\nx \\cdot 10^n = \\sum_{i=0}^{k} a_i \\cdot 10^{i+n} + \\sum_{j=1}^{m} b_j \\cdot 10^{n-j}\n\\]\n\nThe effect of multiplying by $10^n$ is to increase the exponent of each power of 10 in the representation of $x$ by $n$. This effectively shifts the decimal point $n$ places to the right. \n\nFor example, if $x = 123.456$ and $n = 2$, then:\n\n\\[\nx \\cdot 10^2 = 123.456 \\cdot 100 = 12345.6\n\\]\n\nThe decimal point has moved 2 places to the right, confirming the statement.\n\nThus, multiplying a number by a positive power of ten shifts the decimal point to the right by that power."
    },
    {
      "id": "10",
      "statement": "Factoring is the reverse process of expanding an expression.",
      "proof": "To prove that factoring is the reverse process of expanding an expression, we need to understand both processes and show how they are inverses of each other.\n\n### Expanding an Expression\n\nExpanding an expression involves using the distributive property to remove parentheses and combine like terms. For example, consider the expression:\n\n\\[\n(a + b)(c + d)\n\\]\n\nTo expand this expression, we apply the distributive property:\n\n\\[\n(a + b)(c + d) = a(c + d) + b(c + d) = ac + ad + bc + bd\n\\]\n\n### Factoring an Expression\n\nFactoring is the process of expressing a polynomial as a product of its factors. It is essentially the reverse of expanding. For example, given the expanded expression:\n\n\\[\nac + ad + bc + bd\n\\]\n\nWe can factor it by grouping terms and applying the distributive property in reverse:\n\n1. Group terms: \\((ac + ad) + (bc + bd)\\)\n2. Factor out common factors from each group:\n   - From \\(ac + ad\\), factor out \\(a\\): \\(a(c + d)\\)\n   - From \\(bc + bd\\), factor out \\(b\\): \\(b(c + d)\\)\n\n3. Notice that \\((c + d)\\) is a common factor:\n   \\[\n   a(c + d) + b(c + d) = (a + b)(c + d)\n   \\]\n\nThus, we have factored the expression back to its original form \\((a + b)(c + d)\\).\n\n### Conclusion\n\nThe process of expanding involves distributing and combining like terms, while factoring involves identifying common factors and grouping terms to express the polynomial as a product. Since each step in expanding can be reversed by factoring, and vice versa, factoring is indeed the reverse process of expanding an expression."
    },
    {
      "id": "11",
      "statement": "The square of any real number is always non-negative.",
      "proof": "To prove that the square of any real number is always non-negative, let's consider a real number $x$. We need to show that $x^2 \\geq 0$ for all real numbers $x$.\n\n1. **Case 1: $x = 0$**\n\n   If $x = 0$, then $x^2 = 0^2 = 0$, which is non-negative.\n\n2. **Case 2: $x > 0$**\n\n   If $x$ is a positive real number, then $x^2 = x \\cdot x$. Since $x > 0$, both factors $x$ are positive, and the product of two positive numbers is positive. Therefore, $x^2 > 0$, which is non-negative.\n\n3. **Case 3: $x < 0$**\n\n   If $x$ is a negative real number, then $x = -y$ where $y = -x > 0$. Thus, $x^2 = (-y)^2 = (-y) \\cdot (-y) = y \\cdot y = y^2$. Since $y > 0$, $y^2 > 0$, which is non-negative.\n\nIn all cases, $x^2 \\geq 0$. Therefore, the square of any real number is always non-negative."
    },
    {
      "id": "12",
      "statement": "You can always factor a number as a product of its prime factors.",
      "proof": "To prove that any natural number greater than 1 can be factored as a product of prime numbers, we use the Fundamental Theorem of Arithmetic. This theorem states that every integer greater than 1 is either a prime number itself or can be uniquely factored into prime numbers, up to the order of the factors.\n\n**Proof:**\n\n1. **Base Case:** Consider the smallest prime number, which is 2. The number 2 is already a prime number, so it is trivially a product of prime numbers (itself).\n\n2. **Inductive Step:** Assume that every natural number greater than 1 and less than or equal to some integer $n$ can be factored into a product of prime numbers. We need to show that $n+1$ can also be factored into a product of prime numbers.\n\n   - If $n+1$ is a prime number, then it is already a product of prime numbers (itself).\n   \n   - If $n+1$ is not a prime number, then it is a composite number. By definition, a composite number can be expressed as a product of two smaller natural numbers, say $a$ and $b$, where $1 < a, b < n+1$. By the induction hypothesis, both $a$ and $b$ can be factored into a product of prime numbers. Therefore, $n+1 = a \\cdot b$ can also be expressed as a product of prime numbers.\n\nBy the principle of mathematical induction, every natural number greater than 1 can be factored into a product of prime numbers. This completes the proof."
    },
    {
      "id": "13",
      "statement": "The absolute value of any non-zero number is positive.",
      "proof": "To prove that the absolute value of any non-zero number is positive, let's consider a non-zero real number $x$. The absolute value of $x$, denoted by $|x|$, is defined as:\n\n\\[\n|x| = \n\\begin{cases} \nx, & \\text{if } x \\geq 0, \\\\\n-x, & \\text{if } x < 0.\n\\end{cases}\n\\]\n\nWe need to show that $|x| > 0$ for any non-zero $x$.\n\n**Case 1:** $x > 0$\n\nIn this case, by definition, $|x| = x$. Since $x > 0$, it follows directly that $|x| = x > 0$.\n\n**Case 2:** $x < 0$\n\nIn this case, by definition, $|x| = -x$. Since $x < 0$, the negative of $x$ is positive, i.e., $-x > 0$. Therefore, $|x| = -x > 0$.\n\nIn both cases, we have shown that $|x| > 0$ for any non-zero $x$. Thus, the absolute value of any non-zero number is positive."
    },
    {
      "id": "14",
      "statement": "The result of raising any non-zero number to the power of zero is one.",
      "proof": "To prove that any non-zero number raised to the power of zero is one, we can use the properties of exponents.\n\nLet $a$ be a non-zero number. We want to show that $a^0 = 1$.\n\nConsider the property of exponents that states:\n\n\\[ a^m \\cdot a^n = a^{m+n} \\]\n\nSet $m = n$ and consider the expression $a^m \\cdot a^{-m}$:\n\n\\[ a^m \\cdot a^{-m} = a^{m + (-m)} = a^0 \\]\n\nSince $a^m \\cdot a^{-m}$ is the product of a number and its reciprocal, it equals 1:\n\n\\[ a^m \\cdot a^{-m} = 1 \\]\n\nThus, we have:\n\n\\[ a^0 = 1 \\]\n\nThis shows that for any non-zero number $a$, raising it to the power of zero results in 1."
    },
    {
      "id": "15",
      "statement": "Dividing a number by one gives the number itself.",
      "proof": "To prove that dividing a number by one gives the number itself, we can use the definition of division in terms of multiplication.\n\nLet $a$ be any real number. The operation of division by 1 can be expressed as:\n\n\\[\n\\frac{a}{1}\n\\]\n\nBy the definition of division, dividing by a number is equivalent to multiplying by its reciprocal. The reciprocal of 1 is 1 itself, since:\n\n\\[\n1 \\times 1 = 1\n\\]\n\nTherefore, we have:\n\n\\[\n\\frac{a}{1} = a \\times \\frac{1}{1} = a \\times 1\n\\]\n\nUsing the identity property of multiplication, which states that any number multiplied by 1 is the number itself, we have:\n\n\\[\na \\times 1 = a\n\\]\n\nThus, we conclude that:\n\n\\[\n\\frac{a}{1} = a\n\\]\n\nThis completes the proof that dividing a number by one gives the number itself."
    }
  ],
  "Geometry": [
    {
      "id": "1",
      "statement": "The sum of the interior angles of any triangle is 180 degrees.",
      "proof": "To prove that the sum of the interior angles of any triangle is 180 degrees, we can use a geometric approach involving parallel lines.\n\nConsider a triangle $\\triangle ABC$. We want to show that the sum of the angles $\\angle A + \\angle B + \\angle C = 180^\\circ$.\n\n1. Draw a line parallel to the side $BC$ of the triangle through the vertex $A$. Let this line intersect the extensions of sides $AB$ and $AC$ at points $D$ and $E$, respectively.\n\n2. Since $DE$ is parallel to $BC$, by the Alternate Interior Angles Theorem, we have:\n   - $\\angle DAB = \\angle ABC$ (alternate interior angles)\n   - $\\angle EAC = \\angle ACB$ (alternate interior angles)\n\n3. The angles $\\angle DAB$, $\\angle BAC$, and $\\angle CAE$ form a straight line at point $A$, so their sum is $180^\\circ$:\n   \\[\n   \\angle DAB + \\angle BAC + \\angle CAE = 180^\\circ\n   \\]\n\n4. Substituting the equal angles from step 2, we get:\n   \\[\n   \\angle ABC + \\angle BAC + \\angle ACB = 180^\\circ\n   \\]\n\nThus, the sum of the interior angles of triangle $\\triangle ABC$ is $180^\\circ$. This completes the proof."
    },
    {
      "id": "2",
      "statement": "In a right triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides.",
      "proof": "The statement you provided is known as the Pythagorean Theorem. Let's provide a proof for it.\n\n**Theorem (Pythagorean Theorem):** In a right triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides.\n\n**Proof:**\n\nConsider a right triangle $\\triangle ABC$ with the right angle at $C$. Let the lengths of the sides be $a$, $b$, and $c$, where $c$ is the hypotenuse.\n\nWe need to prove that:\n\n\\[ c^2 = a^2 + b^2. \\]\n\n**Proof using the area of squares:**\n\n1. Construct a square with side length $a + b$. This square has an area of $(a + b)^2$.\n\n2. Inside this square, place four copies of the right triangle $\\triangle ABC$. The hypotenuse $c$ of each triangle will form a smaller square in the center of the larger square.\n\n3. The area of the larger square is:\n\n   \\[\n   (a + b)^2 = a^2 + 2ab + b^2.\n   \\]\n\n4. The area of the four triangles is:\n\n   \\[\n   4 \\cdot \\left(\\frac{1}{2}ab\\right) = 2ab.\n   \\]\n\n5. The area of the smaller square (formed by the hypotenuses) is $c^2$.\n\n6. The area of the larger square can also be expressed as the sum of the area of the smaller square and the area of the four triangles:\n\n   \\[\n   c^2 + 2ab.\n   \\]\n\n7. Equating the two expressions for the area of the larger square, we have:\n\n   \\[\n   a^2 + 2ab + b^2 = c^2 + 2ab.\n   \\]\n\n8. Subtract $2ab$ from both sides:\n\n   \\[\n   a^2 + b^2 = c^2.\n   \\]\n\nThus, the Pythagorean Theorem is proven: the square of the hypotenuse is equal to the sum of the squares of the other two sides."
    },
    {
      "id": "3",
      "statement": "A straight angle measures 180 degrees.",
      "proof": "To prove that a straight angle measures 180 degrees, we can use the concept of supplementary angles and the properties of a circle.\n\nConsider a circle with center $O$. Let $A$, $B$, and $C$ be three points on the circumference of the circle such that $A$, $O$, and $C$ are collinear, and $B$ is any other point on the circle. The angle $\\angle AOC$ is a straight angle because $A$, $O$, and $C$ are collinear.\n\nNow, consider the arc $ABC$ of the circle. The measure of the arc $ABC$ is 180 degrees because it is a semicircle (half of the circle). According to the Inscribed Angle Theorem, the measure of an inscribed angle is half the measure of the arc it subtends. Therefore, the angle $\\angle ABC$, which is an inscribed angle subtending the arc $AC$, measures $\\frac{1}{2} \\times 180^\\circ = 90^\\circ$.\n\nSince $\\angle AOC$ is a straight angle and $\\angle ABC$ is a right angle, the sum of the angles around point $O$ is $360^\\circ$. Therefore, the measure of the straight angle $\\angle AOC$ is $180^\\circ$.\n\nThus, a straight angle measures 180 degrees."
    },
    {
      "id": "4",
      "statement": "A circle is a set of points equidistant from a single point called the center.",
      "proof": "To prove that a circle is a set of points equidistant from a single point called the center, we start with the definition of a circle in a Euclidean plane.\n\nLet $C$ be a point in the plane, which we will call the center of the circle, and let $r$ be a positive real number, which we will call the radius of the circle. The circle with center $C$ and radius $r$ is defined as the set of all points $P$ in the plane such that the distance from $P$ to $C$ is exactly $r$.\n\nMathematically, this can be expressed as:\n\\[\n\\{ P \\in \\mathbb{R}^2 \\mid d(P, C) = r \\}\n\\]\nwhere $d(P, C)$ denotes the Euclidean distance between the points $P$ and $C$. The Euclidean distance between two points $P = (x_1, y_1)$ and $C = (x_2, y_2)$ in the plane is given by:\n\\[\nd(P, C) = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n\\]\n\nThus, the set of points $P = (x, y)$ that satisfy the equation:\n\\[\n\\sqrt{(x - x_2)^2 + (y - y_2)^2} = r\n\\]\nis the circle with center $C = (x_2, y_2)$ and radius $r$. Squaring both sides of the equation to eliminate the square root, we obtain:\n\\[\n(x - x_2)^2 + (y - y_2)^2 = r^2\n\\]\n\nThis equation represents the locus of all points $(x, y)$ that are equidistant from the center $(x_2, y_2)$, with the distance being $r$. Therefore, by definition, a circle is indeed the set of all points that are equidistant from a single point, the center, with the common distance being the radius."
    },
    {
      "id": "5",
      "statement": "In an equilateral triangle, all sides are of equal length and all angles are 60 degrees.",
      "proof": "To prove that in an equilateral triangle all sides are of equal length and all angles are 60 degrees, we can use the properties of triangles and the concept of symmetry.\n\n**Proof:**\n\nLet $\\triangle ABC$ be an equilateral triangle, where $AB = BC = CA$.\n\n1. **Equal Sides:**\n\n   By definition, an equilateral triangle has all sides of equal length. Therefore, we have:\n   \\[\n   AB = BC = CA\n   \\]\n\n2. **Equal Angles:**\n\n   In any triangle, the sum of the interior angles is $180^\\circ$. Therefore, in $\\triangle ABC$, we have:\n   \\[\n   \\angle A + \\angle B + \\angle C = 180^\\circ\n   \\]\n\n   Since the triangle is equilateral, it is also equiangular, meaning all angles are equal. Let each angle be $\\theta$. Then:\n   \\[\n   \\angle A = \\angle B = \\angle C = \\theta\n   \\]\n\n   Substituting into the angle sum equation, we get:\n   \\[\n   \\theta + \\theta + \\theta = 180^\\circ\n   \\]\n   \\[\n   3\\theta = 180^\\circ\n   \\]\n   \\[\n   \\theta = \\frac{180^\\circ}{3} = 60^\\circ\n   \\]\n\nThus, in an equilateral triangle, all sides are of equal length, and all angles are $60^\\circ$. This completes the proof."
    },
    {
      "id": "6",
      "statement": "If a shape has four equal sides and four right angles, it is a square.",
      "proof": "To prove that a shape with four equal sides and four right angles is a square, we need to show that such a shape satisfies the definition of a square.\n\n**Definition of a Square:**\nA square is a quadrilateral with four equal sides and four right angles.\n\n**Proof:**\n\n1. **Given:**\n   - A quadrilateral with four equal sides.\n   - Four right angles.\n\n2. **Objective:**\n   - To prove that this quadrilateral is a square.\n\n3. **Analysis:**\n   - A quadrilateral is a polygon with four sides.\n   - If a quadrilateral has four right angles, each angle measures $90^\\circ$.\n   - The sum of the interior angles of any quadrilateral is $360^\\circ$. Therefore, having four right angles satisfies this condition: $4 \\times 90^\\circ = 360^\\circ$.\n\n4. **Properties of the Given Quadrilateral:**\n   - All sides are equal: Let the length of each side be $s$.\n   - All angles are right angles: Each angle is $90^\\circ$.\n\n5. **Conclusion:**\n   - The given quadrilateral satisfies both conditions of a square: four equal sides and four right angles.\n   - Therefore, by definition, the quadrilateral is a square.\n\nThus, a shape with four equal sides and four right angles is indeed a square."
    },
    {
      "id": "7",
      "statement": "The area of a rectangle is calculated by multiplying its length by its width.",
      "proof": "To prove that the area of a rectangle is calculated by multiplying its length by its width, we can use the definition of area and the properties of rectangles.\n\nConsider a rectangle with length $l$ and width $w$. By definition, the area of a rectangle is the measure of the region enclosed by the rectangle. We can determine this area by considering how many unit squares fit into the rectangle.\n\n1. **Unit Squares**: A unit square is a square with side length 1. The area of a unit square is 1 square unit.\n\n2. **Tiling the Rectangle**: Imagine tiling the rectangle with unit squares. Since the rectangle has a length $l$, we can fit $l$ unit squares along its length. Similarly, since the rectangle has a width $w$, we can fit $w$ unit squares along its width.\n\n3. **Total Number of Unit Squares**: The total number of unit squares that can fit into the rectangle is the product of the number of unit squares along the length and the number of unit squares along the width. Therefore, the total number of unit squares is $l \\times w$.\n\n4. **Area Calculation**: Since each unit square has an area of 1 square unit, the total area of the rectangle is the number of unit squares times the area of each unit square. Thus, the area of the rectangle is $l \\times w$ square units.\n\nTherefore, the area of a rectangle is calculated by multiplying its length by its width, which is $A = l \\times w$."
    },
    {
      "id": "8",
      "statement": "The circumference of a circle is given by 2\u03c0 times the radius of the circle.",
      "proof": "To prove that the circumference of a circle is given by $2\\pi$ times the radius of the circle, we start by considering the definition of the circumference and the properties of a circle.\n\nThe circumference of a circle is the distance around the circle. It is a fundamental property of the circle and is directly related to its radius.\n\n### Proof:\n\n1. **Definition of $\\pi$:**\n   The number $\\pi$ is defined as the ratio of the circumference of any circle to its diameter. That is, if $C$ is the circumference and $d$ is the diameter, then:\n   \\[\n   \\pi = \\frac{C}{d}\n   \\]\n\n2. **Relation between diameter and radius:**\n   The diameter $d$ of a circle is twice the radius $r$. Therefore, we have:\n   \\[\n   d = 2r\n   \\]\n\n3. **Substitute the diameter in the definition of $\\pi$:**\n   Using the relation $d = 2r$, we can substitute into the definition of $\\pi$:\n   \\[\n   \\pi = \\frac{C}{2r}\n   \\]\n\n4. **Solve for the circumference $C$:**\n   Rearranging the equation to solve for $C$, we get:\n   \\[\n   C = \\pi \\times 2r\n   \\]\n\n5. **Simplify the expression:**\n   This simplifies to:\n   \\[\n   C = 2\\pi r\n   \\]\n\nThus, the circumference of a circle is $2\\pi$ times the radius of the circle. This relationship holds for any circle, regardless of its size, due to the constant nature of $\\pi$."
    },
    {
      "id": "9",
      "statement": "The diagonals of a rectangle bisect each other.",
      "proof": "To prove that the diagonals of a rectangle bisect each other, let's consider a rectangle $ABCD$ with vertices $A$, $B$, $C$, and $D$. We need to show that the diagonals $AC$ and $BD$ intersect at a point $E$ such that $AE = EC$ and $BE = ED$.\n\n### Proof:\n\n1. **Coordinate Setup:**\n   - Place the rectangle in the coordinate plane with $A = (0, 0)$, $B = (a, 0)$, $C = (a, b)$, and $D = (0, b)$, where $a$ and $b$ are the lengths of the sides of the rectangle.\n\n2. **Equations of the Diagonals:**\n   - The diagonal $AC$ can be represented by the line segment from $A(0, 0)$ to $C(a, b)$.\n   - The diagonal $BD$ can be represented by the line segment from $B(a, 0)$ to $D(0, b)$.\n\n3. **Finding the Intersection Point $E$:**\n   - The midpoint of diagonal $AC$ is given by:\n     \\[\n     E_{AC} = \\left( \\frac{0 + a}{2}, \\frac{0 + b}{2} \\right) = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)\n     \\]\n   - The midpoint of diagonal $BD$ is given by:\n     \\[\n     E_{BD} = \\left( \\frac{a + 0}{2}, \\frac{0 + b}{2} \\right) = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)\n     \\]\n\n4. **Conclusion:**\n   - Since $E_{AC} = E_{BD} = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)$, the diagonals $AC$ and $BD$ intersect at the same point $E$.\n   - This point $E$ is the midpoint of both diagonals, which means that $AE = EC$ and $BE = ED$.\n\nTherefore, the diagonals of a rectangle bisect each other."
    },
    {
      "id": "10",
      "statement": "In a pair of parallel lines, alternate interior angles are equal.",
      "proof": "To prove that alternate interior angles are equal in a pair of parallel lines, consider two parallel lines $l_1$ and $l_2$ cut by a transversal line $t$. Let the points of intersection be $A$ on $l_1$ and $B$ on $l_2$. The transversal $t$ forms angles with the parallel lines at these points.\n\nLet's denote the angles formed at point $A$ as $\\angle 1$ and $\\angle 2$, and the angles formed at point $B$ as $\\angle 3$ and $\\angle 4$. We want to show that $\\angle 1 = \\angle 4$ and $\\angle 2 = \\angle 3$.\n\n**Proof:**\n\n1. **Corresponding Angles Postulate:** Since $l_1 \\parallel l_2$ and $t$ is a transversal, by the Corresponding Angles Postulate, the corresponding angles are equal. Therefore, $\\angle 1 = \\angle 3$ and $\\angle 2 = \\angle 4$.\n\n2. **Vertical Angles Theorem:** At each intersection point, the angles opposite each other are equal. Thus, $\\angle 1 = \\angle 2$ and $\\angle 3 = \\angle 4$.\n\n3. **Transitive Property of Equality:** From the above, we have:\n   - $\\angle 1 = \\angle 3$ (corresponding angles)\n   - $\\angle 2 = \\angle 4$ (corresponding angles)\n   - $\\angle 1 = \\angle 2$ (vertical angles)\n   - $\\angle 3 = \\angle 4$ (vertical angles)\n\n   Using the transitive property, we can conclude:\n   - $\\angle 1 = \\angle 4$ (since $\\angle 1 = \\angle 3$ and $\\angle 3 = \\angle 4$)\n   - $\\angle 2 = \\angle 3$ (since $\\angle 2 = \\angle 4$ and $\\angle 4 = \\angle 3$)\n\nThus, the alternate interior angles are equal, i.e., $\\angle 1 = \\angle 4$ and $\\angle 2 = \\angle 3$. This completes the proof."
    },
    {
      "id": "11",
      "statement": "The sum of the exterior angles of any polygon is 360 degrees.",
      "proof": "To prove that the sum of the exterior angles of any polygon is 360 degrees, we can use the concept of exterior angles and their relationship with interior angles.\n\nConsider a polygon with $n$ sides. An exterior angle is formed by extending one side of the polygon at a vertex and measuring the angle between this extension and the adjacent side. For each vertex of the polygon, there is a corresponding exterior angle.\n\nThe key observation is that each exterior angle and its corresponding interior angle form a linear pair, which means they are supplementary. Therefore, for each vertex, the sum of the interior angle $\\theta_i$ and the exterior angle $\\phi_i$ is:\n\n\\[\n\\theta_i + \\phi_i = 180^\\circ\n\\]\n\nSince the sum of the interior angles of an $n$-sided polygon is given by:\n\n\\[\n\\sum_{i=1}^{n} \\theta_i = (n-2) \\times 180^\\circ\n\\]\n\nwe can express the sum of the exterior angles as follows:\n\n\\[\n\\sum_{i=1}^{n} \\phi_i = \\sum_{i=1}^{n} (180^\\circ - \\theta_i)\n\\]\n\nSubstituting the sum of the interior angles, we have:\n\n\\[\n\\sum_{i=1}^{n} \\phi_i = \\sum_{i=1}^{n} 180^\\circ - \\sum_{i=1}^{n} \\theta_i = n \\times 180^\\circ - (n-2) \\times 180^\\circ\n\\]\n\nSimplifying the expression, we get:\n\n\\[\n\\sum_{i=1}^{n} \\phi_i = n \\times 180^\\circ - n \\times 180^\\circ + 2 \\times 180^\\circ = 360^\\circ\n\\]\n\nThus, the sum of the exterior angles of any polygon is always $360$ degrees, regardless of the number of sides."
    },
    {
      "id": "12",
      "statement": "A line segment joining two points on a curve that go through another point on the curve is called a chord.",
      "proof": "To prove that a line segment joining two points on a curve that passes through another point on the curve is a chord, we need to establish the definition of a chord in the context of geometry.\n\nA chord is defined as a line segment whose endpoints lie on a curve. In the context of a circle, a chord is a line segment with both endpoints on the circle. However, this definition can be generalized to any curve.\n\nLet's consider a curve $C$ and three points $A$, $B$, and $P$ on this curve. We are given that the line segment $AB$ passes through the point $P$. We need to show that $AB$ is a chord of the curve $C$.\n\n1. **Endpoints on the Curve**: By definition, a chord is a line segment with endpoints on the curve. Here, $A$ and $B$ are both points on the curve $C$. Therefore, the line segment $AB$ has its endpoints on the curve.\n\n2. **Passing Through Another Point**: The problem statement mentions that the line segment $AB$ passes through another point $P$ on the curve. This means that $P$ is a point on the line segment $AB$, but it does not affect the definition of $AB$ as a chord. The presence of $P$ on the segment $AB$ simply indicates that $P$ is collinear with $A$ and $B$ and lies between them.\n\nSince the line segment $AB$ has its endpoints $A$ and $B$ on the curve $C$, it satisfies the definition of a chord. The fact that it passes through another point $P$ on the curve does not change its status as a chord; it merely provides additional information about the geometry of the situation.\n\nThus, we conclude that a line segment joining two points on a curve that passes through another point on the curve is indeed a chord of the curve."
    },
    {
      "id": "13",
      "statement": "The distance between two points in a plane is always non-negative.",
      "proof": "To prove that the distance between two points in a plane is always non-negative, we use the distance formula derived from the Pythagorean theorem.\n\nConsider two points in the Cartesian plane: $A(x_1, y_1)$ and $B(x_2, y_2)$. The distance $d$ between these two points is given by the distance formula:\n\n$$ d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}. $$\n\nLet's analyze the expression under the square root:\n\n1. The terms $(x_2 - x_1)^2$ and $(y_2 - y_1)^2$ are squares of real numbers. The square of any real number is non-negative, i.e., $(x_2 - x_1)^2 \\geq 0$ and $(y_2 - y_1)^2 \\geq 0$.\n\n2. Therefore, the sum $(x_2 - x_1)^2 + (y_2 - y_1)^2$ is also non-negative, as it is the sum of two non-negative numbers.\n\n3. The square root function, $\\sqrt{x}$, is defined to be non-negative for all $x \\geq 0$. Thus, $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\geq 0$.\n\nTherefore, the distance $d$ between any two points $A(x_1, y_1)$ and $B(x_2, y_2)$ in a plane is always non-negative. The distance is zero if and only if the two points coincide, i.e., $x_1 = x_2$ and $y_1 = y_2$."
    },
    {
      "id": "14",
      "statement": "If two lines intersect, they do so at exactly one point.",
      "proof": "To prove that if two lines intersect, they do so at exactly one point, we will use the axioms and properties of Euclidean geometry.\n\n**Proof:**\n\nConsider two lines, say line $L_1$ and line $L_2$, in a Euclidean plane. By definition, a line is a straight one-dimensional figure having no thickness and extending infinitely in both directions. \n\n1. **Assumption of Intersection:** Assume that lines $L_1$ and $L_2$ intersect. By the definition of intersection, this means there exists at least one point, say point $P$, that lies on both $L_1$ and $L_2$.\n\n2. **Uniqueness of Intersection Point:**\n   - Suppose, for contradiction, that $L_1$ and $L_2$ intersect at more than one point. Let these points be $P$ and $Q$.\n   - If $P$ and $Q$ are distinct points on both $L_1$ and $L_2$, then by the definition of a line, there is exactly one line that passes through any two distinct points in a Euclidean plane.\n   - Therefore, if $P$ and $Q$ both lie on $L_1$ and $L_2$, then $L_1$ and $L_2$ must be the same line, which contradicts the assumption that they are two distinct lines.\n\n3. **Conclusion:**\n   - Since assuming more than one intersection point leads to a contradiction, it must be the case that if two distinct lines intersect, they do so at exactly one point.\n\nThus, we have shown that if two lines intersect, they intersect at exactly one point. This conclusion is consistent with the axioms of Euclidean geometry, particularly the axiom that states through any two distinct points, there is exactly one line."
    },
    {
      "id": "15",
      "statement": "The shortest distance between any two points is a straight line.",
      "proof": "To prove that the shortest distance between any two points is a straight line, we can use the concept of the Euclidean distance in a plane.\n\nLet the two points be $A$ and $B$ in a Euclidean plane. The straight line segment connecting $A$ and $B$ is denoted as $\\overline{AB}$.\n\n### Proof:\n\n1. **Definition of Distance:**\n   The Euclidean distance between two points $A(x_1, y_1)$ and $B(x_2, y_2)$ in a plane is given by the formula:\n   \\[\n   d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n   \\]\n\n2. **Consider a Path Other Than the Straight Line:**\n   Suppose there exists another path from $A$ to $B$ that is not a straight line. This path can be represented as a piecewise linear path or a curve.\n\n3. **Triangle Inequality:**\n   For any point $C$ on this alternative path, the triangle inequality states:\n   \\[\n   d(A, B) \\leq d(A, C) + d(C, B)\n   \\]\n   This inequality implies that the direct distance (straight line) between $A$ and $B$ is less than or equal to the sum of the distances from $A$ to $C$ and from $C$ to $B$.\n\n4. **Sum of Distances:**\n   If the path from $A$ to $B$ is not a straight line, it can be broken into segments, say $A = P_0, P_1, P_2, \\ldots, P_n = B$. The total distance along this path is:\n   \\[\n   d(A, P_1) + d(P_1, P_2) + \\cdots + d(P_{n-1}, B)\n   \\]\n   By repeatedly applying the triangle inequality, we find that:\n   \\[\n   d(A, B) \\leq d(A, P_1) + d(P_1, P_2) + \\cdots + d(P_{n-1}, B)\n   \\]\n\n5. **Conclusion:**\n   The equality holds if and only if the path is a straight line, i.e., when all intermediate points $P_i$ lie on the line segment $\\overline{AB}$. Therefore, any deviation from the straight line results in a longer path.\n\nThus, the shortest distance between any two points in a Euclidean plane is indeed the straight line connecting them."
    }
  ],
  "Calculus": [
    {
      "id": "1",
      "statement": "The derivative of a constant function is zero.",
      "proof": "To prove that the derivative of a constant function is zero, let's consider a constant function $f(x) = c$, where $c$ is a constant.\n\nThe derivative of a function $f(x)$ at a point $x$ is defined as the limit:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nFor the constant function $f(x) = c$, we have:\n\n\\[\nf(x+h) = c\n\\]\n\nThus, the difference $f(x+h) - f(x)$ becomes:\n\n\\[\nf(x+h) - f(x) = c - c = 0\n\\]\n\nSubstituting this into the definition of the derivative, we get:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{0}{h} = \\lim_{h \\to 0} 0 = 0\n\\]\n\nTherefore, the derivative of a constant function is zero."
    },
    {
      "id": "2",
      "statement": "The integral of a constant times a function is equal to the constant times the integral of the function.",
      "proof": "To prove this statement, we need to show that for a constant $c$ and a function $f(x)$, the following holds:\n\n\\[\n\\int c \\cdot f(x) \\, dx = c \\cdot \\int f(x) \\, dx\n\\]\n\n**Proof:**\n\nLet $F(x)$ be an antiderivative of $f(x)$, i.e., $F'(x) = f(x)$. By the linearity property of integrals, we have:\n\n\\[\n\\int c \\cdot f(x) \\, dx = c \\cdot \\int f(x) \\, dx\n\\]\n\nTo understand why this is true, consider the derivative of the right-hand side:\n\n1. The derivative of $F(x)$ is $f(x)$, i.e., $F'(x) = f(x)$.\n2. By the constant multiple rule in differentiation, the derivative of $c \\cdot F(x)$ is $c \\cdot F'(x) = c \\cdot f(x)$.\n\nThus, $c \\cdot F(x)$ is an antiderivative of $c \\cdot f(x)$, which implies:\n\n\\[\n\\int c \\cdot f(x) \\, dx = c \\cdot F(x) + C\n\\]\n\nwhere $C$ is the constant of integration. This shows that the integral of a constant times a function is indeed equal to the constant times the integral of the function, up to an additive constant. Therefore, the statement is proven."
    },
    {
      "id": "3",
      "statement": "The derivative of \\( x^n \\), where \\( n \\) is any real number, is \\( nx^{n-1} \\).",
      "proof": "To prove that the derivative of \\( x^n \\), where \\( n \\) is any real number, is \\( nx^{n-1} \\), we can use the definition of the derivative and the limit process. The derivative of a function \\( f(x) \\) at a point \\( x \\) is given by:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nFor \\( f(x) = x^n \\), we have:\n\n\\[\nf(x+h) = (x+h)^n\n\\]\n\nThus, the derivative is:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{(x+h)^n - x^n}{h}\n\\]\n\nUsing the binomial theorem, we can expand \\( (x+h)^n \\) as follows:\n\n\\[\n(x+h)^n = \\sum_{k=0}^{\\infty} \\binom{n}{k} x^{n-k} h^k\n\\]\n\nThis expansion is valid for any real number \\( n \\) and small \\( h \\). The first few terms of this expansion are:\n\n\\[\n(x+h)^n = x^n + nx^{n-1}h + \\frac{n(n-1)}{2}x^{n-2}h^2 + \\cdots\n\\]\n\nSubstituting this expansion into the limit, we have:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{x^n + nx^{n-1}h + \\frac{n(n-1)}{2}x^{n-2}h^2 + \\cdots - x^n}{h}\n\\]\n\nSimplifying, we get:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{nx^{n-1}h + \\frac{n(n-1)}{2}x^{n-2}h^2 + \\cdots}{h}\n\\]\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\left( nx^{n-1} + \\frac{n(n-1)}{2}x^{n-2}h + \\cdots \\right)\n\\]\n\nAs \\( h \\to 0 \\), all terms involving \\( h \\) vanish, leaving:\n\n\\[\nf'(x) = nx^{n-1}\n\\]\n\nThus, the derivative of \\( x^n \\) with respect to \\( x \\) is \\( nx^{n-1} \\)."
    },
    {
      "id": "4",
      "statement": "If a function is differentiable at a point, then it is also continuous at that point.",
      "proof": "To prove that if a function is differentiable at a point, then it is also continuous at that point, let's consider a function $f$ that is differentiable at a point $c$. By definition, $f$ is differentiable at $c$ if the following limit exists:\n\n\\[\nf'(c) = \\lim_{h \\to 0} \\frac{f(c+h) - f(c)}{h}\n\\]\n\nSince this limit exists, it implies that for any $\\epsilon > 0$, there exists a $\\delta > 0$ such that for all $h$ with $0 < |h| < \\delta$, we have:\n\n\\[\n\\left| \\frac{f(c+h) - f(c)}{h} - f'(c) \\right| < \\epsilon\n\\]\n\nTo show that $f$ is continuous at $c$, we need to show that:\n\n\\[\n\\lim_{x \\to c} f(x) = f(c)\n\\]\n\nConsider the expression for the difference $f(c+h) - f(c)$:\n\n\\[\nf(c+h) - f(c) = h \\cdot \\frac{f(c+h) - f(c)}{h}\n\\]\n\nTaking the limit as $h \\to 0$, we have:\n\n\\[\n\\lim_{h \\to 0} (f(c+h) - f(c)) = \\lim_{h \\to 0} \\left( h \\cdot \\frac{f(c+h) - f(c)}{h} \\right)\n\\]\n\nSince $\\lim_{h \\to 0} \\frac{f(c+h) - f(c)}{h} = f'(c)$, and $h \\to 0$, it follows that:\n\n\\[\n\\lim_{h \\to 0} (h \\cdot \\frac{f(c+h) - f(c)}{h}) = \\lim_{h \\to 0} h \\cdot f'(c) = 0\n\\]\n\nThus, we have:\n\n\\[\n\\lim_{h \\to 0} (f(c+h) - f(c)) = 0\n\\]\n\nThis implies:\n\n\\[\n\\lim_{h \\to 0} f(c+h) = f(c)\n\\]\n\nTherefore, $f$ is continuous at $c$. This completes the proof that if a function is differentiable at a point, then it is also continuous at that point."
    },
    {
      "id": "5",
      "statement": "The slope of the tangent line to a curve at a point is the derivative of the function at that point.",
      "proof": "To prove that the slope of the tangent line to a curve at a point is the derivative of the function at that point, we start with the definition of the derivative.\n\nLet $f(x)$ be a function, and let $a$ be a point in the domain of $f$. The derivative of $f$ at $a$, denoted by $f'(a)$, is defined as:\n\n\\[\nf'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}\n\\]\n\nThis limit, if it exists, represents the instantaneous rate of change of the function at the point $a$. Geometrically, this is interpreted as the slope of the tangent line to the curve $y = f(x)$ at the point $(a, f(a))$.\n\nTo understand why this is the slope of the tangent line, consider the secant line through the points $(a, f(a))$ and $(a+h, f(a+h))$. The slope of this secant line is given by:\n\n\\[\nm_{\\text{secant}} = \\frac{f(a+h) - f(a)}{h}\n\\]\n\nAs $h$ approaches 0, the point $(a+h, f(a+h))$ approaches $(a, f(a))$, and the secant line approaches the tangent line at $(a, f(a))$. The limit of the slopes of these secant lines as $h \\to 0$ is precisely the derivative $f'(a)$:\n\n\\[\nf'(a) = \\lim_{h \\to 0} m_{\\text{secant}} = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}\n\\]\n\nThus, the derivative $f'(a)$ is the slope of the tangent line to the curve at the point $(a, f(a))$. This completes the proof."
    },
    {
      "id": "6",
      "statement": "The integral of zero over any interval is zero.",
      "proof": "To prove that the integral of zero over any interval is zero, we consider the function $f(x) = 0$ for all $x$ in the real numbers. We want to show that the definite integral of $f(x)$ over any interval $[a, b]$ is zero.\n\nThe definite integral of a function $f(x)$ from $a$ to $b$ is given by:\n\n\\[\n\\int_a^b f(x) \\, dx\n\\]\n\nIn our case, since $f(x) = 0$ for all $x$, the integral becomes:\n\n\\[\n\\int_a^b 0 \\, dx\n\\]\n\nThe integral of a constant function $c$ over an interval $[a, b]$ is given by:\n\n\\[\n\\int_a^b c \\, dx = c(b - a)\n\\]\n\nApplying this to our function $f(x) = 0$, we have:\n\n\\[\n\\int_a^b 0 \\, dx = 0 \\cdot (b - a) = 0\n\\]\n\nThus, the integral of zero over any interval $[a, b]$ is indeed zero."
    },
    {
      "id": "7",
      "statement": "The area under the curve \\( y = f(x) \\) from \\( a \\) to \\( a \\) (i.e., over a zero-length interval) is zero.",
      "proof": "To prove that the area under the curve \\( y = f(x) \\) from \\( a \\) to \\( a \\) is zero, we consider the definite integral of \\( f(x) \\) over the interval \\([a, a]\\).\n\nThe definite integral of a function \\( f(x) \\) from \\( a \\) to \\( b \\) is given by:\n\n\\[\n\\int_a^b f(x) \\, dx\n\\]\n\nIn this case, since the interval is from \\( a \\) to \\( a \\), the integral becomes:\n\n\\[\n\\int_a^a f(x) \\, dx\n\\]\n\nBy the properties of definite integrals, specifically the property that states the integral of a function over an interval of zero length is zero, we have:\n\n\\[\n\\int_a^a f(x) \\, dx = 0\n\\]\n\nThis is because the integral essentially sums the \"area\" under the curve over the interval \\([a, a]\\), which has no width. Therefore, the area under the curve from \\( a \\) to \\( a \\) is zero."
    },
    {
      "id": "8",
      "statement": "The chain rule allows you to differentiate the composition of two functions.",
      "proof": "To prove the chain rule, let's consider two functions $f$ and $g$, where $f: \\mathbb{R} \\to \\mathbb{R}$ and $g: \\mathbb{R} \\to \\mathbb{R}$. We want to differentiate the composition $h(x) = f(g(x))$ with respect to $x$.\n\nThe chain rule states that if $f$ is differentiable at $g(x)$ and $g$ is differentiable at $x$, then $h$ is differentiable at $x$, and the derivative is given by:\n\n\\[\nh'(x) = f'(g(x)) \\cdot g'(x).\n\\]\n\n**Proof:**\n\nAssume $f$ is differentiable at $g(x)$ and $g$ is differentiable at $x$. By the definition of differentiability, we have:\n\n1. $f(g(x + \\Delta x)) = f(g(x)) + f'(g(x)) \\cdot (g(x + \\Delta x) - g(x)) + \\epsilon_1$, where $\\epsilon_1 \\to 0$ as $g(x + \\Delta x) - g(x) \\to 0$.\n\n2. $g(x + \\Delta x) = g(x) + g'(x) \\cdot \\Delta x + \\epsilon_2$, where $\\epsilon_2 \\to 0$ as $\\Delta x \\to 0$.\n\nNow, consider the difference quotient for $h(x)$:\n\n\\[\n\\frac{h(x + \\Delta x) - h(x)}{\\Delta x} = \\frac{f(g(x + \\Delta x)) - f(g(x))}{\\Delta x}.\n\\]\n\nSubstitute the expression for $f(g(x + \\Delta x))$:\n\n\\[\n= \\frac{f(g(x)) + f'(g(x)) \\cdot (g(x + \\Delta x) - g(x)) + \\epsilon_1 - f(g(x))}{\\Delta x}.\n\\]\n\nSimplify:\n\n\\[\n= \\frac{f'(g(x)) \\cdot (g(x + \\Delta x) - g(x)) + \\epsilon_1}{\\Delta x}.\n\\]\n\nSubstitute the expression for $g(x + \\Delta x) - g(x)$:\n\n\\[\n= \\frac{f'(g(x)) \\cdot (g'(x) \\cdot \\Delta x + \\epsilon_2) + \\epsilon_1}{\\Delta x}.\n\\]\n\nDistribute $f'(g(x))$:\n\n\\[\n= \\frac{f'(g(x)) \\cdot g'(x) \\cdot \\Delta x + f'(g(x)) \\cdot \\epsilon_2 + \\epsilon_1}{\\Delta x}.\n\\]\n\nSeparate the terms:\n\n\\[\n= f'(g(x)) \\cdot g'(x) + \\frac{f'(g(x)) \\cdot \\epsilon_2}{\\Delta x} + \\frac{\\epsilon_1}{\\Delta x}.\n\\]\n\nAs $\\Delta x \\to 0$, both $\\epsilon_1 \\to 0$ and $\\epsilon_2 \\to 0$, so:\n\n\\[\n\\lim_{\\Delta x \\to 0} \\frac{f'(g(x)) \\cdot \\epsilon_2}{\\Delta x} = 0, \\quad \\text{and} \\quad \\lim_{\\Delta x \\to 0} \\frac{\\epsilon_1}{\\Delta x} = 0.\n\\]\n\nThus, the limit of the difference quotient is:\n\n\\[\nh'(x) = f'(g(x)) \\cdot g'(x).\n\\]\n\nThis completes the proof of the chain rule."
    },
    {
      "id": "9",
      "statement": "The Fundamental Theorem of Calculus links the concept of differentiation with that of integration.",
      "proof": "The Fundamental Theorem of Calculus (FTC) is a central theorem in calculus that establishes the relationship between differentiation and integration. It consists of two parts:\n\n**Part 1: The First Fundamental Theorem of Calculus**\n\nLet $f$ be a continuous real-valued function defined on a closed interval $[a, b]$. Define a function $F$ by\n\n$$ F(x) = \\int_a^x f(t) \\, dt $$\n\nfor all $x$ in $[a, b]$. Then $F$ is continuous on $[a, b]$, differentiable on the open interval $(a, b)$, and\n\n$$ F'(x) = f(x) $$\n\nfor all $x$ in $(a, b)$.\n\n**Proof of Part 1:**\n\nTo prove this, we need to show that $F'(x) = f(x)$ for all $x \\in (a, b)$. By the definition of the derivative, we have:\n\n$$ F'(x) = \\lim_{h \\to 0} \\frac{F(x+h) - F(x)}{h}. $$\n\nSubstituting the definition of $F(x)$, we get:\n\n$$ F(x+h) - F(x) = \\int_a^{x+h} f(t) \\, dt - \\int_a^x f(t) \\, dt = \\int_x^{x+h} f(t) \\, dt. $$\n\nThus,\n\n$$ F'(x) = \\lim_{h \\to 0} \\frac{1}{h} \\int_x^{x+h} f(t) \\, dt. $$\n\nSince $f$ is continuous at $x$, for any $\\epsilon > 0$, there exists a $\\delta > 0$ such that for all $t$ in $(x, x+\\delta)$, we have $|f(t) - f(x)| < \\epsilon$. Therefore,\n\n$$ \\left| \\frac{1}{h} \\int_x^{x+h} f(t) \\, dt - f(x) \\right| = \\left| \\frac{1}{h} \\int_x^{x+h} (f(t) - f(x)) \\, dt \\right| \\leq \\frac{1}{h} \\int_x^{x+h} |f(t) - f(x)| \\, dt < \\epsilon $$\n\nfor sufficiently small $h$. Hence, \n\n$$ \\lim_{h \\to 0} \\frac{1}{h} \\int_x^{x+h} f(t) \\, dt = f(x), $$\n\nwhich implies $F'(x) = f(x)$.\n\n**Part 2: The Second Fundamental Theorem of Calculus**\n\nLet $f$ be a real-valued function that is integrable on $[a, b]$. If $F$ is any antiderivative of $f$ on $[a, b]$, that is, $F'(x) = f(x)$ for all $x$ in $[a, b]$, then\n\n$$ \\int_a^b f(x) \\, dx = F(b) - F(a). $$\n\n**Proof of Part 2:**\n\nTo prove this, we use the fact that $F'(x) = f(x)$ implies that the derivative of $F$ is $f$. By the definition of the definite integral as the limit of Riemann sums, we have:\n\n$$ \\int_a^b f(x) \\, dx = \\lim_{n \\to \\infty} \\sum_{i=1}^n f(x_i^*) \\Delta x_i, $$\n\nwhere $\\Delta x_i = \\frac{b-a}{n}$ and $x_i^*$ is a sample point in the $i$-th subinterval. By the Mean Value Theorem for Integrals, there exists a point $c_i$ in each subinterval $[x_{i-1}, x_i]$ such that\n\n$$ f(c_i) = \\frac{F(x_i) - F(x_{i-1})}{x_i - x_{i-1}}. $$\n\nThus,\n\n$$ \\sum_{i=1}^n f(c_i) \\Delta x_i = \\sum_{i=1}^n (F(x_i) - F(x_{i-1})) = F(b) - F(a). $$\n\nTaking the limit as $n \\to \\infty$, we obtain\n\n$$ \\int_a^b f(x) \\, dx = F(b) - F(a). $$\n\nThis completes the proof of the Second Fundamental Theorem of Calculus. \n\nTogether, these two parts of the Fundamental Theorem of Calculus establish a profound connection between differentiation and integration, showing that they are essentially inverse processes."
    },
    {
      "id": "10",
      "statement": "The limit of a constant sequence is just that constant.",
      "proof": "To prove that the limit of a constant sequence is the constant itself, let's consider a constant sequence $\\{a_n\\}$ where $a_n = c$ for all $n \\in \\mathbb{N}$, and $c$ is a constant real number.\n\nWe want to show that:\n\n\\[\n\\lim_{n \\to \\infty} a_n = c\n\\]\n\nBy the definition of the limit of a sequence, we need to show that for every $\\epsilon > 0$, there exists a natural number $N$ such that for all $n \\geq N$, the inequality $|a_n - c| < \\epsilon$ holds.\n\nSince $a_n = c$ for all $n$, we have:\n\n\\[\n|a_n - c| = |c - c| = 0\n\\]\n\nClearly, $0 < \\epsilon$ for any $\\epsilon > 0$. Therefore, the inequality $|a_n - c| < \\epsilon$ is satisfied for all $n \\in \\mathbb{N}$, regardless of the choice of $N$. \n\nThus, we can choose $N = 1$, or any natural number, and the condition is satisfied for all $n \\geq N$. \n\nTherefore, by the definition of the limit of a sequence, we conclude that:\n\n\\[\n\\lim_{n \\to \\infty} a_n = c\n\\]\n\nThis completes the proof that the limit of a constant sequence is the constant itself."
    },
    {
      "id": "11",
      "statement": "The derivative of the function \\( f(x) = e^x \\) is \\( e^x \\).",
      "proof": "To prove that the derivative of the function \\( f(x) = e^x \\) is \\( e^x \\), we will use the definition of the derivative:\n\nThe derivative of a function \\( f(x) \\) at a point \\( x \\) is given by the limit:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nFor the function \\( f(x) = e^x \\), we have:\n\n\\[\nf(x+h) = e^{x+h} = e^x \\cdot e^h\n\\]\n\nSubstituting into the definition of the derivative, we get:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{e^x \\cdot e^h - e^x}{h}\n\\]\n\nFactor out \\( e^x \\) from the numerator:\n\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{e^x (e^h - 1)}{h}\n\\]\n\nSince \\( e^x \\) is constant with respect to \\( h \\), we can take it outside the limit:\n\n\\[\nf'(x) = e^x \\cdot \\lim_{h \\to 0} \\frac{e^h - 1}{h}\n\\]\n\nIt is a known limit that:\n\n\\[\n\\lim_{h \\to 0} \\frac{e^h - 1}{h} = 1\n\\]\n\nTherefore, we have:\n\n\\[\nf'(x) = e^x \\cdot 1 = e^x\n\\]\n\nThus, the derivative of \\( f(x) = e^x \\) is indeed \\( e^x \\)."
    },
    {
      "id": "12",
      "statement": "The integral of \\( fx) = 1/x \\) over the interval \\([-1, 1]\\) is undefined because the function has a vertical asymptote at \\( x = 0 \\).",
      "proof": "To show that the integral of \\( f(x) = \\frac{1}{x} \\) over the interval \\([-1, 1]\\) is undefined, we need to consider the behavior of the function at the point \\( x = 0 \\), where it has a vertical asymptote.\n\nThe integral of \\( f(x) = \\frac{1}{x} \\) over \\([-1, 1]\\) can be expressed as the sum of two improper integrals:\n\n\\[\n\\int_{-1}^{1} \\frac{1}{x} \\, dx = \\int_{-1}^{0} \\frac{1}{x} \\, dx + \\int_{0}^{1} \\frac{1}{x} \\, dx\n\\]\n\nBoth of these integrals are improper because the function \\(\\frac{1}{x}\\) is not defined at \\(x = 0\\). We need to evaluate each integral separately as a limit.\n\n1. **Evaluate \\(\\int_{-1}^{0} \\frac{1}{x} \\, dx\\):**\n\n   \\[\n   \\int_{-1}^{0} \\frac{1}{x} \\, dx = \\lim_{\\epsilon \\to 0^-} \\int_{-1}^{\\epsilon} \\frac{1}{x} \\, dx\n   \\]\n\n   The antiderivative of \\(\\frac{1}{x}\\) is \\(\\ln |x|\\), so:\n\n   \\[\n   \\int_{-1}^{\\epsilon} \\frac{1}{x} \\, dx = \\left[ \\ln |x| \\right]_{-1}^{\\epsilon} = \\ln |\\epsilon| - \\ln 1 = \\ln |\\epsilon|\n   \\]\n\n   As \\(\\epsilon \\to 0^-\\), \\(\\ln |\\epsilon|\\) approaches \\(-\\infty\\).\n\n2. **Evaluate \\(\\int_{0}^{1} \\frac{1}{x} \\, dx\\):**\n\n   \\[\n   \\int_{0}^{1} \\frac{1}{x} \\, dx = \\lim_{\\delta \\to 0^+} \\int_{\\delta}^{1} \\frac{1}{x} \\, dx\n   \\]\n\n   Again, using the antiderivative:\n\n   \\[\n   \\int_{\\delta}^{1} \\frac{1}{x} \\, dx = \\left[ \\ln |x| \\right]_{\\delta}^{1} = \\ln 1 - \\ln |\\delta| = -\\ln |\\delta|\n   \\]\n\n   As \\(\\delta \\to 0^+\\), \\(-\\ln |\\delta|\\) approaches \\(+\\infty\\).\n\nSince one part of the integral approaches \\(-\\infty\\) and the other part approaches \\(+\\infty\\), the overall integral:\n\n\\[\n\\int_{-1}^{1} \\frac{1}{x} \\, dx = \\lim_{\\epsilon \\to 0^-} \\ln |\\epsilon| + \\lim_{\\delta \\to 0^+} (-\\ln |\\delta|)\n\\]\n\nis undefined because it involves the sum of \\(-\\infty\\) and \\(+\\infty\\), which is not a finite number. Therefore, the integral of \\( f(x) = \\frac{1}{x} \\) over the interval \\([-1, 1]\\) is undefined due to the vertical asymptote at \\( x = 0 \\)."
    },
    {
      "id": "13",
      "statement": "The derivative of a sum of functions is the sum of the derivatives of those functions.",
      "proof": "To prove that the derivative of a sum of functions is the sum of the derivatives of those functions, we will use the definition of the derivative.\n\nLet $f(x)$ and $g(x)$ be two functions that are differentiable at a point $x$. We want to show that:\n\n\\[\n\\frac{d}{dx}[f(x) + g(x)] = \\frac{d}{dx}[f(x)] + \\frac{d}{dx}[g(x)]\n\\]\n\nBy the definition of the derivative, the derivative of the sum $f(x) + g(x)$ at a point $x$ is given by:\n\n\\[\n\\frac{d}{dx}[f(x) + g(x)] = \\lim_{h \\to 0} \\frac{(f(x+h) + g(x+h)) - (f(x) + g(x))}{h}\n\\]\n\nWe can separate the terms in the numerator:\n\n\\[\n= \\lim_{h \\to 0} \\left( \\frac{f(x+h) - f(x)}{h} + \\frac{g(x+h) - g(x)}{h} \\right)\n\\]\n\nSince the limit of a sum is the sum of the limits (provided the limits exist), we have:\n\n\\[\n= \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} + \\lim_{h \\to 0} \\frac{g(x+h) - g(x)}{h}\n\\]\n\nThese are precisely the definitions of the derivatives of $f(x)$ and $g(x)$ at $x$:\n\n\\[\n= f'(x) + g'(x)\n\\]\n\nThus, we have shown that:\n\n\\[\n\\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)\n\\]\n\nThis completes the proof that the derivative of a sum of functions is the sum of the derivatives of those functions."
    },
    {
      "id": "14",
      "statement": "If a function is increasing on an interval, then its derivative is non-negative on that interval.",
      "proof": "The statement \"If a function is increasing on an interval, then its derivative is non-negative on that interval\" is not entirely correct as stated. The correct statement should be: \"If a function is differentiable and increasing on an interval, then its derivative is non-negative on that interval.\" Let's prove this statement.\n\n**Proof:**\n\nLet $f$ be a differentiable function on an interval $I$. We assume that $f$ is increasing on $I$. This means that for any two points $x_1, x_2 \\in I$ with $x_1 < x_2$, we have:\n\n$$ f(x_1) \\leq f(x_2). $$\n\nWe need to show that $f'(x) \\geq 0$ for all $x \\in I$.\n\nBy the definition of the derivative, for any point $x \\in I$, the derivative $f'(x)$ is given by:\n\n$$ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}. $$\n\nTo show that $f'(x) \\geq 0$, consider the following:\n\n1. **For $h > 0$:**\n\n   Since $f$ is increasing, $f(x+h) \\geq f(x)$. Therefore, the difference quotient is non-negative:\n\n   $$ \\frac{f(x+h) - f(x)}{h} \\geq 0. $$\n\n2. **For $h < 0$:**\n\n   Again, since $f$ is increasing, $f(x) \\geq f(x+h)$. Therefore, the difference quotient is also non-negative:\n\n   $$ \\frac{f(x+h) - f(x)}{h} \\leq 0. $$\n\n   However, since $h < 0$, dividing by a negative number reverses the inequality, so:\n\n   $$ \\frac{f(x+h) - f(x)}{h} \\geq 0. $$\n\nIn both cases, as $h \\to 0$, the limit of the difference quotient is non-negative. Therefore, we conclude that:\n\n$$ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} \\geq 0. $$\n\nThus, if $f$ is differentiable and increasing on an interval $I$, then $f'(x) \\geq 0$ for all $x \\in I$."
    },
    {
      "id": "15",
      "statement": "The limit of the sum of two functions is the sum of their limits, provided both limits exist.",
      "proof": "To prove that the limit of the sum of two functions is the sum of their limits, provided both limits exist, we will use the definition of limits.\n\nLet $f(x)$ and $g(x)$ be two functions, and suppose that $\\lim_{x \\to a} f(x) = L$ and $\\lim_{x \\to a} g(x) = M$. We want to show that:\n\n\\[\n\\lim_{x \\to a} (f(x) + g(x)) = L + M.\n\\]\n\nBy the definition of a limit, for every $\\epsilon > 0$, there exists a $\\delta_1 > 0$ such that if $0 < |x - a| < \\delta_1$, then $|f(x) - L| < \\frac{\\epsilon}{2}$. Similarly, there exists a $\\delta_2 > 0$ such that if $0 < |x - a| < \\delta_2$, then $|g(x) - M| < \\frac{\\epsilon}{2}$.\n\nLet $\\delta = \\min(\\delta_1, \\delta_2)$. Then, if $0 < |x - a| < \\delta$, both $|f(x) - L| < \\frac{\\epsilon}{2}$ and $|g(x) - M| < \\frac{\\epsilon}{2}$ hold.\n\nNow consider the expression $|(f(x) + g(x)) - (L + M)|$:\n\n\\[\n|(f(x) + g(x)) - (L + M)| = |(f(x) - L) + (g(x) - M)|.\n\\]\n\nBy the triangle inequality, we have:\n\n\\[\n|(f(x) - L) + (g(x) - M)| \\leq |f(x) - L| + |g(x) - M|.\n\\]\n\nThus,\n\n\\[\n|(f(x) + g(x)) - (L + M)| \\leq |f(x) - L| + |g(x) - M| < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon.\n\\]\n\nTherefore, for every $\\epsilon > 0$, there exists a $\\delta > 0$ such that if $0 < |x - a| < \\delta$, then $|(f(x) + g(x)) - (L + M)| < \\epsilon$. This proves that:\n\n\\[\n\\lim_{x \\to a} (f(x) + g(x)) = L + M.\n\\]\n\nThus, the limit of the sum of two functions is indeed the sum of their limits, provided both limits exist."
    }
  ],
  "Number Theory": [
    {
      "id": "1",
      "statement": "The sum of two even numbers is even.",
      "proof": "To prove that the sum of two even numbers is even, let's consider two arbitrary even numbers. By definition, an even number can be expressed as $2k$ for some integer $k$. \n\nLet the two even numbers be $2a$ and $2b$, where $a$ and $b$ are integers. The sum of these two even numbers is:\n\n\\[\n2a + 2b\n\\]\n\nWe can factor out the common factor of 2:\n\n\\[\n2a + 2b = 2(a + b)\n\\]\n\nSince $a + b$ is an integer (because the sum of two integers is an integer), we can express the sum $2(a + b)$ as $2c$, where $c = a + b$ is an integer. \n\nThus, the sum $2(a + b)$ is of the form $2c$, which is the definition of an even number. Therefore, the sum of two even numbers is even."
    },
    {
      "id": "2",
      "statement": "An integer is divisible by 5 if its last digit is 0 or 5.",
      "proof": "To prove that an integer is divisible by 5 if and only if its last digit is 0 or 5, we can use the properties of modular arithmetic.\n\n**Proof:**\n\nLet the integer be denoted by $n$. We can express $n$ in terms of its decimal representation as follows:\n\n\\[ n = 10k + d \\]\n\nwhere $k$ is an integer and $d$ is the last digit of $n$, with $d \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$.\n\nWe want to determine when $n$ is divisible by 5. This is equivalent to saying:\n\n\\[ n \\equiv 0 \\pmod{5} \\]\n\nSubstituting the expression for $n$, we have:\n\n\\[ 10k + d \\equiv 0 \\pmod{5} \\]\n\nSince $10 \\equiv 0 \\pmod{5}$ (because 10 is divisible by 5), it follows that:\n\n\\[ 10k \\equiv 0 \\pmod{5} \\]\n\nThus, the congruence simplifies to:\n\n\\[ d \\equiv 0 \\pmod{5} \\]\n\nThis means that $d$ must be a multiple of 5. Given that $d$ is a single digit, the only possibilities for $d$ are 0 or 5.\n\n**Conclusion:**\n\nTherefore, an integer $n$ is divisible by 5 if and only if its last digit $d$ is 0 or 5. This completes the proof."
    },
    {
      "id": "3",
      "statement": "The sum of the digits of a number is a multiple of 9 if and only if the number is divisible by 9.",
      "proof": "To prove this statement, we need to show two implications:\n\n1. If a number is divisible by 9, then the sum of its digits is divisible by 9.\n2. If the sum of the digits of a number is divisible by 9, then the number itself is divisible by 9.\n\nLet's consider a number $N$ with digits $a_k, a_{k-1}, \\ldots, a_1, a_0$. We can express $N$ in terms of its digits as:\n\n\\[ N = a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10 + a_0. \\]\n\n### Proof of Implication 1:\n\nAssume $N$ is divisible by 9. We need to show that the sum of its digits, $S = a_k + a_{k-1} + \\cdots + a_1 + a_0$, is also divisible by 9.\n\nSince $10 \\equiv 1 \\pmod{9}$, it follows that $10^i \\equiv 1^i \\equiv 1 \\pmod{9}$ for any non-negative integer $i$. Therefore, we have:\n\n\\[ N \\equiv a_k \\cdot 1 + a_{k-1} \\cdot 1 + \\cdots + a_1 \\cdot 1 + a_0 \\cdot 1 \\equiv a_k + a_{k-1} + \\cdots + a_1 + a_0 \\equiv S \\pmod{9}. \\]\n\nSince $N$ is divisible by 9, $N \\equiv 0 \\pmod{9}$. Thus, $S \\equiv 0 \\pmod{9}$, meaning $S$ is divisible by 9.\n\n### Proof of Implication 2:\n\nAssume the sum of the digits $S = a_k + a_{k-1} + \\cdots + a_1 + a_0$ is divisible by 9. We need to show that $N$ is divisible by 9.\n\nFrom the congruence relation derived earlier:\n\n\\[ N \\equiv S \\pmod{9}. \\]\n\nSince $S \\equiv 0 \\pmod{9}$, it follows that $N \\equiv 0 \\pmod{9}$. Therefore, $N$ is divisible by 9.\n\n### Conclusion:\n\nBoth implications have been proven, so we conclude that a number is divisible by 9 if and only if the sum of its digits is divisible by 9."
    },
    {
      "id": "4",
      "statement": "Any integer greater than 1 that is not a prime is called a composite number.",
      "proof": "To prove that any integer greater than 1 that is not a prime is a composite number, we need to establish the definitions and then show the logical implications.\n\n**Definitions:**\n\n1. A prime number is an integer greater than 1 that has no positive divisors other than 1 and itself.\n2. A composite number is an integer greater than 1 that is not a prime, meaning it has divisors other than 1 and itself.\n\n**Proof:**\n\nLet $n$ be an integer greater than 1. We want to show that if $n$ is not a prime, then $n$ is a composite number.\n\n1. Assume $n$ is not a prime. By definition, this means that $n$ has divisors other than 1 and itself.\n2. Therefore, there exist integers $a$ and $b$ such that $1 < a < n$, $1 < b < n$, and $a \\cdot b = n$.\n3. Since $a$ and $b$ are both greater than 1 and less than $n$, $n$ can be expressed as a product of two smaller positive integers.\n4. This satisfies the definition of a composite number, which is an integer greater than 1 that can be expressed as a product of two smaller positive integers.\n\nThus, any integer greater than 1 that is not a prime is indeed a composite number."
    },
    {
      "id": "5",
      "statement": "The greatest common divisor (GCD) of a number with itself is the number.",
      "proof": "To prove that the greatest common divisor (GCD) of a number with itself is the number, let's consider a natural number $n$. We want to show that $\\gcd(n, n) = n$.\n\n**Proof:**\n\n1. **Definition of GCD:**\n   The greatest common divisor of two integers $a$ and $b$, denoted $\\gcd(a, b)$, is the largest positive integer $d$ that divides both $a$ and $b$.\n\n2. **Applying the Definition:**\n   In our case, we have $a = n$ and $b = n$. Therefore, we need to find the largest positive integer $d$ such that $d \\mid n$ and $d \\mid n$.\n\n3. **Divisibility:**\n   Clearly, $n \\mid n$ because any number divides itself. Therefore, $n$ is a common divisor of $n$ and $n$.\n\n4. **Maximality:**\n   We need to show that $n$ is the largest such divisor. Suppose there exists a positive integer $d > n$ such that $d \\mid n$ and $d \\mid n$. However, this is impossible because no integer greater than $n$ can divide $n$.\n\n5. **Conclusion:**\n   Since $n$ is a divisor of itself and no integer greater than $n$ can divide $n$, it follows that $n$ is the greatest common divisor of $n$ and $n$.\n\nThus, we conclude that $\\gcd(n, n) = n$."
    },
    {
      "id": "6",
      "statement": "An integer is divisible by 2 if its last digit is even.",
      "proof": "To prove that an integer is divisible by 2 if and only if its last digit is even, we can use the properties of the decimal number system.\n\nLet $n$ be an integer expressed in its decimal form as $n = a_k a_{k-1} \\ldots a_1 a_0$, where $a_i$ are the digits of $n$ and $a_0$ is the last digit (the units digit). This can be written as:\n\n\\[ n = a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10 + a_0. \\]\n\nNotice that each power of 10, except for $10^0 = 1$, is divisible by 2. Specifically, $10^1 = 10$, $10^2 = 100$, and so on, are all divisible by 2. Therefore, the divisibility of $n$ by 2 depends solely on the last term, $a_0$, because all other terms are already divisible by 2.\n\nThus, $n$ is divisible by 2 if and only if $a_0$ is divisible by 2. The digits that are divisible by 2 are 0, 2, 4, 6, and 8, which are precisely the even digits.\n\nTherefore, an integer is divisible by 2 if and only if its last digit is even."
    },
    {
      "id": "7",
      "statement": "The least common multiple (LCM) of two co-prime numbers is the product of the numbers.",
      "proof": "To prove that the least common multiple (LCM) of two co-prime numbers is the product of the numbers, let's consider two co-prime integers $a$ and $b$. By definition, two numbers are co-prime if their greatest common divisor (GCD) is 1, i.e., $\\gcd(a, b) = 1$.\n\nThe relationship between the greatest common divisor and the least common multiple of two numbers is given by the formula:\n\n\\[\n\\text{lcm}(a, b) \\cdot \\gcd(a, b) = a \\cdot b\n\\]\n\nSince $a$ and $b$ are co-prime, we have $\\gcd(a, b) = 1$. Substituting this into the formula, we get:\n\n\\[\n\\text{lcm}(a, b) \\cdot 1 = a \\cdot b\n\\]\n\nThis simplifies to:\n\n\\[\n\\text{lcm}(a, b) = a \\cdot b\n\\]\n\nThus, the least common multiple of two co-prime numbers $a$ and $b$ is indeed the product of the numbers, $a \\cdot b$. This completes the proof."
    },
    {
      "id": "8",
      "statement": "A perfect square is an integer that can be expressed as the square of another integer.",
      "proof": "To prove that a perfect square is an integer that can be expressed as the square of another integer, we start with the definition of a perfect square.\n\nA perfect square is a number that can be written in the form $n^2$, where $n$ is an integer. We need to show that if a number $m$ is a perfect square, then $m$ can be expressed as $n^2$ for some integer $n$.\n\n**Proof:**\n\n1. **Definition of a Perfect Square:**\n   By definition, a perfect square is a number that can be expressed as the square of an integer. This means there exists some integer $n$ such that $m = n^2$.\n\n2. **Existence of an Integer $n$:**\n   If $m$ is a perfect square, then by definition, there exists an integer $n$ such that $m = n^2$. This directly satisfies the condition that $m$ can be expressed as the square of an integer.\n\n3. **Integer Nature of $m$:**\n   Since $n$ is an integer, and the square of an integer is also an integer (because the product of two integers is an integer), $m = n^2$ is an integer.\n\nTherefore, a perfect square is indeed an integer that can be expressed as the square of another integer. This completes the proof."
    },
    {
      "id": "9",
      "statement": "If a number is divisible by both 3 and 4, it is also divisible by 12.",
      "proof": "To prove that if a number is divisible by both 3 and 4, it is also divisible by 12, we can use the concept of the least common multiple (LCM).\n\nLet $n$ be a number that is divisible by both 3 and 4. This means that there exist integers $k$ and $m$ such that:\n\n\\[ n = 3k \\]\n\\[ n = 4m \\]\n\nSince $n$ is divisible by both 3 and 4, it must also be divisible by the least common multiple of 3 and 4. The LCM of two numbers is the smallest positive integer that is divisible by both numbers.\n\nTo find the LCM of 3 and 4, we consider their prime factorizations:\n\n- The prime factorization of 3 is $3^1$.\n- The prime factorization of 4 is $2^2$.\n\nThe LCM is found by taking the highest power of each prime that appears in these factorizations:\n\n- The highest power of 2 is $2^2$.\n- The highest power of 3 is $3^1$.\n\nThus, the LCM of 3 and 4 is:\n\n\\[ \\text{LCM}(3, 4) = 2^2 \\times 3^1 = 4 \\times 3 = 12 \\]\n\nTherefore, if $n$ is divisible by both 3 and 4, it must be divisible by 12. This means there exists an integer $p$ such that:\n\n\\[ n = 12p \\]\n\nThus, we have shown that if a number is divisible by both 3 and 4, it is also divisible by 12."
    },
    {
      "id": "10",
      "statement": "Two numbers are co-prime if their greatest common divisor is 1.",
      "proof": "To prove that two numbers are co-prime if and only if their greatest common divisor (GCD) is 1, we need to show two implications:\n\n1. If two numbers are co-prime, then their GCD is 1.\n2. If the GCD of two numbers is 1, then they are co-prime.\n\n**Proof:**\n\nLet $a$ and $b$ be two integers.\n\n**(1) If $a$ and $b$ are co-prime, then $\\gcd(a, b) = 1$:**\n\nBy definition, two numbers are co-prime if they have no common positive integer divisors other than 1. This means that the only positive integer that divides both $a$ and $b$ is 1. Therefore, the greatest common divisor of $a$ and $b$ must be 1. Hence, $\\gcd(a, b) = 1$.\n\n**(2) If $\\gcd(a, b) = 1$, then $a$ and $b$ are co-prime:**\n\nAssume $\\gcd(a, b) = 1$. By definition of the greatest common divisor, this means that the largest positive integer that divides both $a$ and $b$ is 1. Therefore, there are no other common divisors of $a$ and $b$ except for 1. This implies that $a$ and $b$ are co-prime, as they do not share any other common factors.\n\nThus, we have shown both implications, and therefore, two numbers are co-prime if and only if their greatest common divisor is 1."
    },
    {
      "id": "11",
      "statement": "If the sum of two numbers is odd, one of them is even and the other is odd.",
      "proof": "To prove this statement, let's consider two integers $a$ and $b$ such that their sum $a + b$ is odd. We need to show that one of these integers is even and the other is odd.\n\nRecall the definitions:\n- An integer is even if it can be expressed in the form $2k$ for some integer $k$.\n- An integer is odd if it can be expressed in the form $2k + 1$ for some integer $k$.\n\nNow, consider the possible parity (evenness or oddness) of $a$ and $b$:\n\n1. **Case 1**: Both $a$ and $b$ are even.\n   - If $a = 2m$ and $b = 2n$ for some integers $m$ and $n$, then their sum is $a + b = 2m + 2n = 2(m + n)$, which is even. This contradicts the assumption that $a + b$ is odd.\n\n2. **Case 2**: Both $a$ and $b$ are odd.\n   - If $a = 2m + 1$ and $b = 2n + 1$ for some integers $m$ and $n$, then their sum is $a + b = (2m + 1) + (2n + 1) = 2m + 2n + 2 = 2(m + n + 1)$, which is also even. This again contradicts the assumption that $a + b$ is odd.\n\n3. **Case 3**: One of $a$ or $b$ is even, and the other is odd.\n   - Without loss of generality, assume $a$ is even and $b$ is odd. Then $a = 2m$ and $b = 2n + 1$ for some integers $m$ and $n$. Their sum is $a + b = 2m + (2n + 1) = 2(m + n) + 1$, which is odd, consistent with the assumption.\n\nSince Cases 1 and 2 lead to contradictions, the only possibility is Case 3, where one number is even and the other is odd. Therefore, if the sum of two numbers is odd, one of them must be even and the other must be odd. This completes the proof."
    },
    {
      "id": "12",
      "statement": "The absolute value of an integer is always non-negative.",
      "proof": "To prove that the absolute value of an integer is always non-negative, we start by recalling the definition of the absolute value function for integers.\n\nFor any integer $n$, the absolute value of $n$, denoted by $|n|$, is defined as:\n\n\\[\n|n| = \n\\begin{cases} \nn, & \\text{if } n \\geq 0, \\\\\n-n, & \\text{if } n < 0.\n\\end{cases}\n\\]\n\nWe need to show that $|n| \\geq 0$ for any integer $n$.\n\n1. **Case 1: $n \\geq 0$**\n\n   If $n$ is non-negative, then by definition, $|n| = n$. Since $n \\geq 0$, it follows directly that $|n| = n \\geq 0$.\n\n2. **Case 2: $n < 0$**\n\n   If $n$ is negative, then by definition, $|n| = -n$. Since $n < 0$, the negation of $n$ (i.e., $-n$) is positive. Therefore, $|n| = -n > 0$, which implies $|n| \\geq 0$.\n\nIn both cases, we have shown that $|n| \\geq 0$. Therefore, the absolute value of an integer is always non-negative."
    },
    {
      "id": "13",
      "statement": "Every integer is a rational number.",
      "proof": "To prove that every integer is a rational number, we need to show that for any integer $a$, there exists a rational number representation of $a$. \n\nRecall that a rational number is defined as a number that can be expressed in the form $\\frac{p}{q}$, where $p$ and $q$ are integers, and $q \\neq 0$.\n\nLet $a$ be any integer. We can express $a$ as a rational number by writing it in the form:\n\n\\[\n\\frac{a}{1}\n\\]\n\nHere, $p = a$ and $q = 1$. Since $a$ is an integer and $1$ is a non-zero integer, $\\frac{a}{1}$ is a valid rational number representation of $a$.\n\nTherefore, every integer $a$ can be expressed as the rational number $\\frac{a}{1}$, proving that every integer is indeed a rational number."
    },
    {
      "id": "14",
      "statement": "The number 1 is neither a prime number nor a composite number.",
      "proof": "To prove that the number 1 is neither a prime number nor a composite number, we need to recall the definitions of prime and composite numbers.\n\n1. **Definition of a Prime Number**: A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. In other words, a prime number has exactly two distinct positive divisors: 1 and the number itself.\n\n2. **Definition of a Composite Number**: A composite number is a natural number greater than 1 that has more than two distinct positive divisors. In other words, a composite number can be divided evenly by numbers other than 1 and itself.\n\nNow, let's analyze the number 1:\n\n- The number 1 has exactly one positive divisor, which is 1 itself. It does not have two distinct positive divisors, so it cannot be classified as a prime number.\n- Since a composite number must have more than two distinct positive divisors and must be greater than 1, the number 1 cannot be classified as a composite number either.\n\nTherefore, the number 1 is neither a prime number nor a composite number. It is often referred to as a unit in number theory."
    },
    {
      "id": "15",
      "statement": "A number that leaves a remainder of 1 when divided by 2 is an odd number.",
      "proof": "To prove that a number that leaves a remainder of 1 when divided by 2 is an odd number, we start by considering the definition of odd and even numbers.\n\nA number $n$ is said to be even if it can be expressed in the form $n = 2k$ for some integer $k$. Conversely, a number is odd if it can be expressed in the form $n = 2k + 1$ for some integer $k$.\n\nNow, consider a number $n$ that leaves a remainder of 1 when divided by 2. By the division algorithm, we can express $n$ as:\n\n\\[ n = 2q + r \\]\n\nwhere $q$ is the quotient, and $r$ is the remainder such that $0 \\leq r < 2$. Given that the remainder is 1, we have:\n\n\\[ n = 2q + 1 \\]\n\nThis expression matches the form of an odd number, $n = 2k + 1$, where $k = q$ is an integer.\n\nTherefore, a number that leaves a remainder of 1 when divided by 2 is indeed an odd number."
    },
    {
      "id": "16",
      "statement": "Any integer greater than 1 is either a prime number or can be expressed as a product of prime numbers.",
      "proof": "This statement is a fundamental result in number theory known as the Fundamental Theorem of Arithmetic. It states that every integer greater than 1 is either a prime number or can be expressed as a product of prime numbers, and this factorization is unique up to the order of the factors.\n\n**Proof:**\n\n1. **Base Case:**\n   Consider the smallest integer greater than 1, which is 2. The number 2 is a prime number, so the statement holds for 2.\n\n2. **Inductive Step:**\n   Assume that for all integers $k$ such that $2 \\leq k \\leq n$, the statement holds: each $k$ is either a prime or can be expressed as a product of prime numbers.\n\n   We need to show that the statement holds for $n+1$.\n\n   - If $n+1$ is a prime number, then the statement is trivially true for $n+1$.\n\n   - If $n+1$ is not a prime number, then $n+1$ is composite. By definition of a composite number, there exist integers $a$ and $b$ such that $1 < a < n+1$, $1 < b < n+1$, and $n+1 = a \\cdot b$.\n\n     By the inductive hypothesis, both $a$ and $b$ are either prime or can be expressed as a product of prime numbers. Therefore, $n+1 = a \\cdot b$ can be expressed as a product of prime numbers.\n\n3. **Conclusion:**\n   By the principle of mathematical induction, the statement holds for all integers greater than 1. Thus, any integer greater than 1 is either a prime number or can be expressed as a product of prime numbers.\n\nThis completes the proof of the Fundamental Theorem of Arithmetic."
    },
    {
      "id": "17",
      "statement": "The sum of the digits of any number divisible by 9 is also divisible by 9.",
      "proof": "To prove that the sum of the digits of any number divisible by 9 is also divisible by 9, we can use the concept of congruences in modular arithmetic.\n\nLet $N$ be a number with digits $a_k, a_{k-1}, \\ldots, a_1, a_0$. We can express $N$ in terms of its digits as:\n\n\\[ N = a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10^1 + a_0 \\cdot 10^0. \\]\n\nWe want to show that if $N$ is divisible by 9, then the sum of its digits $S = a_k + a_{k-1} + \\cdots + a_1 + a_0$ is also divisible by 9.\n\nFirst, note that $10 \\equiv 1 \\pmod{9}$. This is because $10 - 1 = 9$, which is divisible by 9. Therefore, for any integer $m$, we have $10^m \\equiv 1^m \\equiv 1 \\pmod{9}$.\n\nUsing this property, we can rewrite $N$ modulo 9 as:\n\n\\[\nN \\equiv a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10^1 + a_0 \\cdot 10^0 \\pmod{9}.\n\\]\n\nSince $10^m \\equiv 1 \\pmod{9}$ for all $m$, we have:\n\n\\[\nN \\equiv a_k \\cdot 1 + a_{k-1} \\cdot 1 + \\cdots + a_1 \\cdot 1 + a_0 \\cdot 1 \\pmod{9}.\n\\]\n\nThis simplifies to:\n\n\\[\nN \\equiv a_k + a_{k-1} + \\cdots + a_1 + a_0 \\equiv S \\pmod{9}.\n\\]\n\nThus, if $N$ is divisible by 9, then $N \\equiv 0 \\pmod{9}$. Therefore, $S \\equiv 0 \\pmod{9}$, which means that the sum of the digits $S$ is also divisible by 9.\n\nThis completes the proof."
    },
    {
      "id": "18",
      "statement": "If a number ends in a 0, it is divisible by 5.",
      "proof": "To prove that if a number ends in a 0, it is divisible by 5, let's consider a number $N$ that ends in 0. We can express $N$ in the form:\n\n\\[ N = 10k \\]\n\nwhere $k$ is an integer. This is because a number ending in 0 can be written as a multiple of 10.\n\nNow, we can factor 10 as:\n\n\\[ 10 = 2 \\times 5 \\]\n\nThus, we have:\n\n\\[ N = 10k = (2 \\times 5)k = 2k \\times 5 \\]\n\nThis shows that $N$ is a multiple of 5, since it can be expressed as $5$ times another integer ($2k$).\n\nTherefore, if a number ends in 0, it is divisible by 5."
    },
    {
      "id": "19",
      "statement": "The difference between two even numbers is even.",
      "proof": "To prove that the difference between two even numbers is even, let's consider two even numbers. By definition, an even number can be expressed in the form $2k$, where $k$ is an integer.\n\nLet the two even numbers be $2m$ and $2n$, where $m$ and $n$ are integers. We need to show that the difference $2m - 2n$ is even.\n\nCalculate the difference:\n\n\\[\n2m - 2n = 2(m - n)\n\\]\n\nSince $m - n$ is an integer (because the difference of two integers is an integer), we can express $2(m - n)$ as $2k$ where $k = m - n$. This shows that $2(m - n)$ is divisible by 2, and hence it is even.\n\nTherefore, the difference between two even numbers is even."
    },
    {
      "id": "20",
      "statement": "The number 2 is the smallest and only even prime number.",
      "proof": "To prove that 2 is the smallest and only even prime number, we need to establish two things:\n\n1. **2 is a prime number:**\n\n   A prime number is defined as a natural number greater than 1 that has no positive divisors other than 1 and itself. The number 2 is greater than 1, and its only divisors are 1 and 2. Therefore, 2 is a prime number.\n\n2. **2 is the only even prime number:**\n\n   An even number is any integer that is divisible by 2. Suppose there exists another even prime number $p$ such that $p \\neq 2$. Since $p$ is even, it can be expressed as $p = 2k$ for some integer $k$. \n\n   If $p$ is greater than 2, then $k$ must be greater than 1, which implies that $p$ has divisors other than 1 and itself (specifically, 1, 2, $k$, and $p$). This contradicts the definition of a prime number, which must have exactly two distinct positive divisors: 1 and itself.\n\n   Therefore, no even number greater than 2 can be prime. Hence, 2 is the only even prime number.\n\nIn conclusion, 2 is the smallest and only even prime number."
    },
    {
      "id": "21",
      "statement": "Multiplying any integer by zero results in zero.",
      "proof": "To prove that multiplying any integer by zero results in zero, we can use the properties of integers and the axioms of arithmetic.\n\nLet $a$ be any integer. We want to show that $a \\cdot 0 = 0$.\n\nWe start by using the distributive property of multiplication over addition, which states that for any integers $a$, $b$, and $c$, we have:\n\n\\[ a \\cdot (b + c) = a \\cdot b + a \\cdot c. \\]\n\nNow, consider the expression $a \\cdot (0 + 0)$. According to the distributive property, we have:\n\n\\[ a \\cdot (0 + 0) = a \\cdot 0 + a \\cdot 0. \\]\n\nHowever, we also know that $0 + 0 = 0$, so:\n\n\\[ a \\cdot 0 = a \\cdot (0 + 0) = a \\cdot 0 + a \\cdot 0. \\]\n\nSubtract $a \\cdot 0$ from both sides of the equation:\n\n\\[ a \\cdot 0 - a \\cdot 0 = (a \\cdot 0 + a \\cdot 0) - a \\cdot 0. \\]\n\nThis simplifies to:\n\n\\[ 0 = a \\cdot 0. \\]\n\nThus, we have shown that multiplying any integer $a$ by zero results in zero:\n\n\\[ a \\cdot 0 = 0. \\]\n\nThis completes the proof."
    },
    {
      "id": "22",
      "statement": "An integer is divisible by 3 if the sum of its digits is divisible by 3.",
      "proof": "To prove that an integer is divisible by 3 if and only if the sum of its digits is divisible by 3, we can use the concept of congruences in modular arithmetic.\n\nLet the integer be represented in its decimal form as $N = a_k a_{k-1} \\ldots a_1 a_0$, where $a_i$ are the digits of the number. This means that:\n\n\\[ N = a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10^1 + a_0 \\cdot 10^0. \\]\n\nWe need to show that $N \\equiv 0 \\pmod{3}$ if and only if $a_k + a_{k-1} + \\cdots + a_1 + a_0 \\equiv 0 \\pmod{3}$.\n\nFirst, note that $10 \\equiv 1 \\pmod{3}$ because $10 - 1 = 9$ is divisible by 3. Therefore, for any power of 10, we have:\n\n\\[ 10^i \\equiv 1^i \\equiv 1 \\pmod{3}. \\]\n\nUsing this congruence, we can rewrite $N$ modulo 3 as:\n\n\\[\nN \\equiv a_k \\cdot 10^k + a_{k-1} \\cdot 10^{k-1} + \\cdots + a_1 \\cdot 10^1 + a_0 \\cdot 10^0 \\pmod{3}.\n\\]\n\nSubstituting $10^i \\equiv 1 \\pmod{3}$, we get:\n\n\\[\nN \\equiv a_k \\cdot 1 + a_{k-1} \\cdot 1 + \\cdots + a_1 \\cdot 1 + a_0 \\cdot 1 \\pmod{3}.\n\\]\n\nThis simplifies to:\n\n\\[\nN \\equiv a_k + a_{k-1} + \\cdots + a_1 + a_0 \\pmod{3}.\n\\]\n\nThus, $N \\equiv 0 \\pmod{3}$ if and only if $a_k + a_{k-1} + \\cdots + a_1 + a_0 \\equiv 0 \\pmod{3}$.\n\nTherefore, an integer is divisible by 3 if and only if the sum of its digits is divisible by 3."
    },
    {
      "id": "23",
      "statement": "If an integer is divisible by both 3 and 4, it is divisible by 12.",
      "proof": "To prove this statement, we need to show that if an integer $n$ is divisible by both 3 and 4, then it is also divisible by 12.\n\nLet $n$ be an integer such that $n$ is divisible by both 3 and 4. This means there exist integers $k$ and $m$ such that:\n\n\\[ n = 3k \\]\n\\[ n = 4m \\]\n\nSince $n$ is divisible by both 3 and 4, it must be a common multiple of 3 and 4. The least common multiple (LCM) of 3 and 4 is 12. Therefore, $n$ must be divisible by 12.\n\nTo see why, consider the prime factorizations of 3 and 4:\n\n- The prime factorization of 3 is $3^1$.\n- The prime factorization of 4 is $2^2$.\n\nThe LCM of 3 and 4 is obtained by taking the highest power of each prime that appears in the factorizations:\n\n- The highest power of 2 is $2^2$.\n- The highest power of 3 is $3^1$.\n\nThus, the LCM of 3 and 4 is $2^2 \\times 3^1 = 4 \\times 3 = 12$.\n\nSince $n$ is a multiple of both 3 and 4, it must be a multiple of their LCM, which is 12. Therefore, there exists an integer $t$ such that:\n\n\\[ n = 12t \\]\n\nThis shows that $n$ is divisible by 12, completing the proof."
    },
    {
      "id": "24",
      "statement": "The least common multiple of two numbers is always greater than or equal to the greater of the two numbers.",
      "proof": "To prove that the least common multiple (LCM) of two numbers is always greater than or equal to the greater of the two numbers, let's consider two positive integers $a$ and $b$ without loss of generality, where $a \\leq b$.\n\nThe least common multiple of $a$ and $b$, denoted as $\\text{lcm}(a, b)$, is defined as the smallest positive integer that is divisible by both $a$ and $b$.\n\nSince $\\text{lcm}(a, b)$ is a multiple of $b$, we can write $\\text{lcm}(a, b) = kb$ for some positive integer $k$. By the definition of LCM, $kb$ must be divisible by $a$ as well. Therefore, $kb \\geq b$ because $k \\geq 1$ (since $k$ is a positive integer).\n\nNow, since $a \\leq b$, it follows that $b \\geq a$. Therefore, $\\text{lcm}(a, b) = kb \\geq b \\geq a$.\n\nThus, we have shown that $\\text{lcm}(a, b) \\geq b$, and since $b \\geq a$, it follows that $\\text{lcm}(a, b) \\geq \\max(a, b)$.\n\nTherefore, the least common multiple of two numbers is always greater than or equal to the greater of the two numbers."
    },
    {
      "id": "25",
      "statement": "If a number n is a perfect square, then the square root of n is an integer.",
      "proof": "To prove that if a number $n$ is a perfect square, then the square root of $n$ is an integer, we start with the definition of a perfect square.\n\nA number $n$ is a perfect square if there exists an integer $m$ such that $n = m^2$. We need to show that the square root of $n$, denoted as $\\sqrt{n}$, is an integer.\n\nSince $n = m^2$, we have:\n\n\\[\n\\sqrt{n} = \\sqrt{m^2}\n\\]\n\nBy the properties of square roots, we know that:\n\n\\[\n\\sqrt{m^2} = m\n\\]\n\nwhere $m$ is an integer by assumption. Therefore, $\\sqrt{n} = m$, which is an integer.\n\nThus, if $n$ is a perfect square, then $\\sqrt{n}$ is indeed an integer. This completes the proof."
    },
    {
      "id": "26",
      "statement": "Adding or subtracting 0 from a number does not change the value of that number.",
      "proof": "To prove that adding or subtracting 0 from a number does not change the value of that number, we can use the properties of addition and subtraction in the set of real numbers.\n\n**Proof:**\n\nLet $a$ be any real number.\n\n1. **Addition of 0:**\n\n   By the definition of the additive identity in the set of real numbers, we have:\n   \\[\n   a + 0 = a\n   \\]\n   This property states that adding 0 to any number $a$ results in the number $a$ itself. Therefore, adding 0 to a number does not change its value.\n\n2. **Subtraction of 0:**\n\n   Subtraction can be defined in terms of addition as adding the additive inverse. The additive inverse of 0 is 0 itself. Therefore, we have:\n   \\[\n   a - 0 = a + (-0) = a + 0 = a\n   \\]\n   This shows that subtracting 0 from any number $a$ results in the number $a$ itself. Therefore, subtracting 0 from a number does not change its value.\n\nIn conclusion, both adding and subtracting 0 from a number $a$ results in the number $a$, confirming that the value of the number remains unchanged."
    },
    {
      "id": "27",
      "statement": "The sum of the first n natural numbers is given by the formula n(n + 1)/2.",
      "proof": "To prove that the sum of the first $n$ natural numbers is given by the formula $\\frac{n(n + 1)}{2}$, we can use mathematical induction.\n\n**Base Case:**\n\nFor $n = 1$, the sum of the first $n$ natural numbers is $1$. According to the formula, we have:\n\n\\[\n\\frac{1(1 + 1)}{2} = \\frac{2}{2} = 1\n\\]\n\nThus, the base case holds.\n\n**Inductive Step:**\n\nAssume that the formula holds for some arbitrary positive integer $k$, i.e., assume:\n\n\\[\n1 + 2 + 3 + \\cdots + k = \\frac{k(k + 1)}{2}\n\\]\n\nWe need to show that the formula also holds for $k + 1$, i.e.,\n\n\\[\n1 + 2 + 3 + \\cdots + k + (k + 1) = \\frac{(k + 1)(k + 2)}{2}\n\\]\n\nStarting from the inductive hypothesis, add $k + 1$ to both sides:\n\n\\[\n1 + 2 + 3 + \\cdots + k + (k + 1) = \\frac{k(k + 1)}{2} + (k + 1)\n\\]\n\nFactor out $(k + 1)$ on the right-hand side:\n\n\\[\n= \\frac{k(k + 1)}{2} + \\frac{2(k + 1)}{2} = \\frac{k(k + 1) + 2(k + 1)}{2}\n\\]\n\n\\[\n= \\frac{(k + 1)(k + 2)}{2}\n\\]\n\nThus, the formula holds for $k + 1$.\n\nBy the principle of mathematical induction, the formula $\\frac{n(n + 1)}{2}$ is true for all natural numbers $n$."
    },
    {
      "id": "28",
      "statement": "Any prime number greater than 2 is odd.",
      "proof": "To prove that any prime number greater than 2 is odd, we start by recalling the definition of a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\n\nNow, consider the prime number 2. It is the only even prime number because it is divisible by 1 and 2, and no other even number can be prime since any even number greater than 2 can be divided by 2, making it composite.\n\nLet's assume, for the sake of contradiction, that there exists an even prime number $p$ greater than 2. By definition, an even number can be expressed as $p = 2k$ for some integer $k \\geq 1$. Since $p$ is greater than 2, $k$ must be greater than 1, which implies $p$ is divisible by 2 and $k$, making $p$ composite. This contradicts the assumption that $p$ is prime.\n\nTherefore, any prime number greater than 2 cannot be even and must be odd. Hence, any prime number greater than 2 is odd."
    },
    {
      "id": "29",
      "statement": "The greatest common divisor of two co-prime numbers is 1.",
      "proof": "To prove that the greatest common divisor (GCD) of two co-prime numbers is 1, we start by recalling the definition of co-prime numbers. Two integers $a$ and $b$ are said to be co-prime if their only positive common divisor is 1.\n\nLet's denote the greatest common divisor of $a$ and $b$ as $\\gcd(a, b)$. By definition, $\\gcd(a, b)$ is the largest positive integer that divides both $a$ and $b$.\n\nSince $a$ and $b$ are co-prime, the only positive integer that divides both $a$ and $b$ is 1. Therefore, the greatest common divisor of $a$ and $b$ must be 1.\n\nTo formalize this:\n\n1. Assume $a$ and $b$ are co-prime, meaning the only positive integer that divides both $a$ and $b$ is 1.\n2. By definition, $\\gcd(a, b)$ is the largest positive integer that divides both $a$ and $b$.\n3. Since 1 is the only positive integer that divides both $a$ and $b$, it follows that $\\gcd(a, b) = 1$.\n\nThus, we have shown that if $a$ and $b$ are co-prime, then $\\gcd(a, b) = 1$."
    },
    {
      "id": "30",
      "statement": "The sum of the interior angles of a triangle is always 180 degrees.",
      "proof": "To prove that the sum of the interior angles of a triangle is always 180 degrees, we can use a geometric approach based on the properties of parallel lines and alternate interior angles.\n\nConsider a triangle $\\triangle ABC$. We want to show that the sum of the angles $\\angle A + \\angle B + \\angle C = 180^\\circ$.\n\n1. Draw a line parallel to the side $BC$ of the triangle through the vertex $A$. Let this line be $DE$, where $D$ and $E$ are points on the line such that $DE \\parallel BC$.\n\n2. Since $DE \\parallel BC$, by the Alternate Interior Angles Theorem, the angle $\\angle DAB$ is equal to $\\angle ABC$, and the angle $\\angle EAC$ is equal to $\\angle ACB$.\n\n3. The angles $\\angle DAB$, $\\angle BAC$, and $\\angle EAC$ form a straight line at point $A$, so their sum is $180^\\circ$. That is, \n   \\[\n   \\angle DAB + \\angle BAC + \\angle EAC = 180^\\circ.\n   \\]\n\n4. Substituting the equal angles from step 2, we have:\n   \\[\n   \\angle ABC + \\angle BAC + \\angle ACB = 180^\\circ.\n   \\]\n\nThus, the sum of the interior angles of triangle $\\triangle ABC$ is $180^\\circ$. This completes the proof."
    }
  ],
  "Statistics": [
    {
      "id": "1",
      "statement": "The mean of a set of identical numbers is the number itself.",
      "proof": "To prove that the mean of a set of identical numbers is the number itself, let's consider a set of $n$ identical numbers, each equal to $a$. The set can be represented as $\\{a, a, a, \\ldots, a\\}$, where there are $n$ occurrences of the number $a$.\n\nThe mean (or average) of a set of numbers is calculated by dividing the sum of all the numbers in the set by the number of elements in the set. Therefore, the mean $\\mu$ of this set is given by:\n\n\\[\n\\mu = \\frac{\\text{sum of all elements}}{\\text{number of elements}} = \\frac{a + a + a + \\cdots + a}{n}\n\\]\n\nSince there are $n$ terms, each equal to $a$, the sum of all elements is:\n\n\\[\na + a + a + \\cdots + a = n \\cdot a\n\\]\n\nSubstituting this into the formula for the mean, we have:\n\n\\[\n\\mu = \\frac{n \\cdot a}{n}\n\\]\n\nSimplifying the expression, we get:\n\n\\[\n\\mu = a\n\\]\n\nThus, the mean of a set of identical numbers is the number itself."
    },
    {
      "id": "2",
      "statement": "The median of a list of numbers is the middle number when the numbers are arranged in order. If there are an even number of numbers, the median is the average of the two middle numbers.",
      "proof": "To prove the statement about the median, we need to consider two cases: when the list of numbers has an odd number of elements and when it has an even number of elements.\n\n**Case 1: Odd number of elements**\n\nSuppose we have a list of $n$ numbers, where $n$ is odd. Let's denote these numbers as $a_1, a_2, \\ldots, a_n$. First, we arrange these numbers in non-decreasing order: $a_1 \\leq a_2 \\leq \\cdots \\leq a_n$. Since $n$ is odd, we can write $n = 2k + 1$ for some integer $k$. The middle number in this ordered list is the $(k+1)$-th number, which is $a_{k+1}$. Therefore, the median of the list is $a_{k+1}$.\n\n**Case 2: Even number of elements**\n\nNow, suppose we have a list of $n$ numbers, where $n$ is even. Again, denote these numbers as $a_1, a_2, \\ldots, a_n$ and arrange them in non-decreasing order: $a_1 \\leq a_2 \\leq \\cdots \\leq a_n$. Since $n$ is even, we can write $n = 2k$ for some integer $k$. The two middle numbers in this ordered list are the $k$-th and $(k+1)$-th numbers, which are $a_k$ and $a_{k+1}$. According to the definition of the median for an even number of elements, the median is the average of these two middle numbers. Therefore, the median is given by:\n\n\\[\n\\text{Median} = \\frac{a_k + a_{k+1}}{2}\n\\]\n\nIn both cases, the definition of the median is satisfied: for an odd number of elements, it is the middle number, and for an even number of elements, it is the average of the two middle numbers. This completes the proof."
    },
    {
      "id": "3",
      "statement": "The mode of a dataset is the number that appears most frequently.",
      "proof": "To prove that the mode of a dataset is the number that appears most frequently, we start by considering the definition of the mode.\n\nLet $X = \\{x_1, x_2, \\ldots, x_n\\}$ be a dataset consisting of $n$ elements. The mode of this dataset is defined as the value that appears with the highest frequency in the dataset.\n\nTo formalize this, let $f(x)$ denote the frequency of the element $x$ in the dataset $X$. That is, $f(x)$ is the number of times $x$ appears in $X$. The mode, denoted as $\\text{mode}(X)$, is the element $m \\in X$ such that:\n\n\\[ f(m) \\geq f(x) \\quad \\text{for all} \\quad x \\in X. \\]\n\nThis means that the frequency of $m$ is greater than or equal to the frequency of any other element in the dataset. Therefore, $m$ is the element that appears most frequently in the dataset.\n\nTo illustrate this with an example, consider the dataset $X = \\{3, 1, 4, 4, 5, 6, 4, 2\\}$. The frequencies of the elements are:\n\n- $f(1) = 1$\n- $f(2) = 1$\n- $f(3) = 1$\n- $f(4) = 3$\n- $f(5) = 1$\n- $f(6) = 1$\n\nThe element $4$ has the highest frequency, $f(4) = 3$, which is greater than the frequency of any other element in the dataset. Therefore, the mode of this dataset is $4$.\n\nThus, by definition, the mode of a dataset is indeed the number that appears most frequently."
    },
    {
      "id": "4",
      "statement": "The range of a dataset is the difference between the highest and lowest values.",
      "proof": "To prove that the range of a dataset is the difference between the highest and lowest values, let's consider a dataset $D = \\{x_1, x_2, \\ldots, x_n\\}$, where $x_1, x_2, \\ldots, x_n$ are real numbers.\n\n1. **Definition of Range**: The range of a dataset is defined as the difference between the maximum and minimum values in the dataset.\n\n2. **Identify Maximum and Minimum**: Let $x_{\\max}$ be the maximum value in the dataset $D$, and $x_{\\min}$ be the minimum value in the dataset $D$. By definition:\n   \\[\n   x_{\\max} = \\max(x_1, x_2, \\ldots, x_n)\n   \\]\n   \\[\n   x_{\\min} = \\min(x_1, x_2, \\ldots, x_n)\n   \\]\n\n3. **Calculate the Range**: The range $R$ of the dataset is given by:\n   \\[\n   R = x_{\\max} - x_{\\min}\n   \\]\n\n4. **Verification**: By the properties of maximum and minimum, for any $x_i \\in D$, we have:\n   \\[\n   x_{\\min} \\leq x_i \\leq x_{\\max}\n   \\]\n   Therefore, the difference $x_{\\max} - x_{\\min}$ represents the largest possible difference between any two values in the dataset, which is the definition of the range.\n\nThus, the range of a dataset is indeed the difference between the highest and lowest values."
    },
    {
      "id": "5",
      "statement": "A dataset with all identical values has a standard deviation of zero.",
      "proof": "To prove that a dataset with all identical values has a standard deviation of zero, let's consider a dataset consisting of $n$ identical values, say $x_1 = x_2 = \\cdots = x_n = c$, where $c$ is a constant.\n\nThe standard deviation of a dataset is defined as the square root of the variance. The variance is the average of the squared differences from the mean. Let's calculate the mean and variance for this dataset:\n\n1. **Mean ($\\mu$):**\n\n   The mean $\\mu$ of the dataset is given by:\n\n   \\[\n   \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} (c + c + \\cdots + c) = \\frac{nc}{n} = c\n   \\]\n\n2. **Variance ($\\sigma^2$):**\n\n   The variance $\\sigma^2$ is given by:\n\n   \\[\n   \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\n   \\]\n\n   Substituting the values, we have:\n\n   \\[\n   \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (c - c)^2 = \\frac{1}{n} \\sum_{i=1}^{n} 0^2 = \\frac{1}{n} \\cdot 0 = 0\n   \\]\n\n3. **Standard Deviation ($\\sigma$):**\n\n   The standard deviation $\\sigma$ is the square root of the variance:\n\n   \\[\n   \\sigma = \\sqrt{\\sigma^2} = \\sqrt{0} = 0\n   \\]\n\nThus, the standard deviation of a dataset with all identical values is zero."
    },
    {
      "id": "6",
      "statement": "The sum of probabilities in any probability distribution is equal to 1.",
      "proof": "To prove that the sum of probabilities in any probability distribution is equal to 1, we need to consider the definition of a probability distribution.\n\nA probability distribution is a function that assigns probabilities to each possible outcome of a random experiment. Let $S$ be the sample space of the experiment, which is the set of all possible outcomes. A probability distribution assigns a probability $P(x)$ to each outcome $x \\in S$ such that:\n\n1. $0 \\leq P(x) \\leq 1$ for all $x \\in S$ (probabilities are non-negative and do not exceed 1).\n2. The sum of the probabilities of all possible outcomes is equal to 1.\n\nWe need to show that:\n\n\\[\n\\sum_{x \\in S} P(x) = 1\n\\]\n\n**Proof:**\n\nConsider a random experiment with a finite sample space $S = \\{x_1, x_2, \\ldots, x_n\\}$. The probability distribution assigns a probability $P(x_i)$ to each outcome $x_i$ such that $0 \\leq P(x_i) \\leq 1$ for all $i = 1, 2, \\ldots, n$.\n\nBy the axioms of probability, the total probability of all possible outcomes must be 1. Therefore, we have:\n\n\\[\n\\sum_{i=1}^{n} P(x_i) = 1\n\\]\n\nThis is a direct consequence of the definition of a probability distribution. Each $P(x_i)$ represents the probability of the outcome $x_i$, and the sum of these probabilities over all possible outcomes must account for the entire sample space, which is certain to occur, hence the sum is 1.\n\nFor a continuous probability distribution, the sample space $S$ is not finite, but the principle is the same. The probability distribution is described by a probability density function $f(x)$, and the total probability is given by the integral over the entire sample space:\n\n\\[\n\\int_{S} f(x) \\, dx = 1\n\\]\n\nThis integral represents the total probability over the continuous sample space, and it must equal 1 by the definition of a probability density function.\n\nThus, in both discrete and continuous cases, the sum (or integral) of probabilities in any probability distribution is equal to 1."
    },
    {
      "id": "7",
      "statement": "The probability of an event occurring is always between 0 and 1, inclusive.",
      "proof": "To prove that the probability of an event occurring is always between 0 and 1, inclusive, we rely on the axioms of probability.\n\nLet $S$ be a sample space, which is the set of all possible outcomes of a random experiment, and let $E$ be an event, which is a subset of $S$. The probability of the event $E$, denoted by $P(E)$, is defined according to the following axioms:\n\n1. **Non-negativity**: For any event $E$, the probability $P(E) \\geq 0$.\n\n2. **Normalization**: The probability of the sample space $S$ is 1, i.e., $P(S) = 1$.\n\n3. **Additivity**: For any two mutually exclusive events $E_1$ and $E_2$, the probability of their union is the sum of their probabilities: $P(E_1 \\cup E_2) = P(E_1) + P(E_2)$.\n\nNow, let's prove that $0 \\leq P(E) \\leq 1$ for any event $E$:\n\n- **Lower Bound (0)**: By the non-negativity axiom, $P(E) \\geq 0$ for any event $E$. This establishes the lower bound.\n\n- **Upper Bound (1)**: Consider the event $E$ and its complement $E^c = S \\setminus E$. Since $E$ and $E^c$ are mutually exclusive and their union is the entire sample space $S$, we have:\n\n  \\[\n  P(E \\cup E^c) = P(S) = 1\n  \\]\n\n  By the additivity axiom, we have:\n\n  \\[\n  P(E \\cup E^c) = P(E) + P(E^c)\n  \\]\n\n  Therefore:\n\n  \\[\n  1 = P(E) + P(E^c)\n  \\]\n\n  Since $P(E^c) \\geq 0$ (by the non-negativity axiom), it follows that:\n\n  \\[\n  P(E) \\leq 1\n  \\]\n\nCombining both bounds, we conclude that for any event $E$, the probability $P(E)$ satisfies:\n\n\\[\n0 \\leq P(E) \\leq 1\n\\]\n\nThis completes the proof that the probability of an event occurring is always between 0 and 1, inclusive."
    },
    {
      "id": "8",
      "statement": "If a set of data points are all equal, the variance of the dataset is zero.",
      "proof": "To prove that if a set of data points are all equal, the variance of the dataset is zero, let's consider a dataset consisting of $n$ data points, all of which are equal to some constant value $c$. \n\nThe dataset can be represented as $\\{x_1, x_2, \\ldots, x_n\\}$ where $x_i = c$ for all $i = 1, 2, \\ldots, n$.\n\nThe variance of a dataset is defined as the average of the squared differences between each data point and the mean of the dataset. Mathematically, the variance $\\sigma^2$ is given by:\n\n\\[\n\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\nwhere $\\bar{x}$ is the mean of the dataset. \n\nFirst, we calculate the mean $\\bar{x}$ of the dataset:\n\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} \\sum_{i=1}^{n} c = \\frac{nc}{n} = c\n\\]\n\nSince all data points are equal to $c$, the mean $\\bar{x}$ is also $c$.\n\nNext, we calculate the variance:\n\n\\[\n\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{1}{n} \\sum_{i=1}^{n} (c - c)^2 = \\frac{1}{n} \\sum_{i=1}^{n} 0^2 = \\frac{1}{n} \\cdot 0 = 0\n\\]\n\nThus, the variance of the dataset is zero. This proves that if all data points in a dataset are equal, the variance of the dataset is zero."
    },
    {
      "id": "9",
      "statement": "The probability of an impossible event is 0.",
      "proof": "To prove that the probability of an impossible event is 0, we start by considering the axioms of probability. Let $S$ be the sample space of a probability experiment, and let $E$ be an event in $S$. The probability of an event $E$, denoted by $P(E)$, is defined by the following axioms:\n\n1. **Non-negativity**: For any event $E$, $P(E) \\geq 0$.\n2. **Normalization**: $P(S) = 1$, where $S$ is the sample space.\n3. **Additivity**: For any two mutually exclusive events $E_1$ and $E_2$, $P(E_1 \\cup E_2) = P(E_1) + P(E_2)$.\n\nAn impossible event is an event that cannot occur, which means it is represented by the empty set $\\emptyset$. According to the additivity axiom, for any event $E$, we have:\n\n\\[ P(E \\cup \\emptyset) = P(E) + P(\\emptyset). \\]\n\nSince $E \\cup \\emptyset = E$, it follows that:\n\n\\[ P(E) = P(E) + P(\\emptyset). \\]\n\nSubtracting $P(E)$ from both sides, we get:\n\n\\[ 0 = P(\\emptyset). \\]\n\nThus, the probability of an impossible event is 0."
    },
    {
      "id": "10",
      "statement": "If you flip a fair coin, the probability of landing heads on a single flip is 0.5.",
      "proof": "To prove that the probability of landing heads on a single flip of a fair coin is 0.5, we start by considering the definition of a fair coin. A fair coin is one that has two equally likely outcomes: heads (H) and tails (T).\n\nThe sample space for a single flip of a fair coin is:\n\\[ S = \\{ H, T \\} \\]\n\nSince the coin is fair, each outcome in the sample space is equally likely. Therefore, the probability of each outcome is given by the formula for the probability of an event $E$:\n\n\\[ P(E) = \\frac{\\text{Number of favorable outcomes for } E}{\\text{Total number of possible outcomes}} \\]\n\nFor the event of landing heads, the number of favorable outcomes is 1 (since there is only one head in the sample space), and the total number of possible outcomes is 2 (heads or tails). Thus, the probability of landing heads is:\n\n\\[ P(\\text{Heads}) = \\frac{1}{2} \\]\n\nTherefore, the probability of landing heads on a single flip of a fair coin is 0.5."
    },
    {
      "id": "11",
      "statement": "The mean of a symmetric distribution is equal to its median.",
      "proof": "To prove that the mean of a symmetric distribution is equal to its median, let's consider a continuous random variable $X$ with a symmetric probability density function (pdf) $f(x)$ about a point $a$. This means that for all $x$, $f(a - x) = f(a + x)$. \n\n### Mean of the Distribution\n\nThe mean (expected value) of the distribution is given by:\n\n\\[\n\\mu = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\n\\]\n\nFor a symmetric distribution about $a$, we can change variables to simplify the integral. Let $u = x - a$, so $x = u + a$ and $dx = du$. The limits of integration remain the same because they are infinite. The integral becomes:\n\n\\[\n\\mu = \\int_{-\\infty}^{\\infty} (u + a) f(u + a) \\, du\n\\]\n\nSince the distribution is symmetric about $a$, we have $f(u + a) = f(a - u)$. Therefore, the integral can be split as:\n\n\\[\n\\mu = \\int_{-\\infty}^{\\infty} u f(u + a) \\, du + \\int_{-\\infty}^{\\infty} a f(u + a) \\, du\n\\]\n\nThe first integral is zero because the function $u f(u + a)$ is odd about $u = 0$:\n\n\\[\n\\int_{-\\infty}^{\\infty} u f(u + a) \\, du = 0\n\\]\n\nThe second integral simplifies to:\n\n\\[\n\\int_{-\\infty}^{\\infty} a f(u + a) \\, du = a \\int_{-\\infty}^{\\infty} f(u + a) \\, du = a\n\\]\n\nsince $\\int_{-\\infty}^{\\infty} f(u + a) \\, du = 1$ (the total probability is 1).\n\nThus, the mean $\\mu = a$.\n\n### Median of the Distribution\n\nThe median $m$ of the distribution is the value such that:\n\n\\[\n\\int_{-\\infty}^{m} f(x) \\, dx = \\frac{1}{2}\n\\]\n\nFor a symmetric distribution about $a$, the median is also $a$ because the area under the curve from $-\\infty$ to $a$ is equal to the area from $a$ to $\\infty$. This is due to the symmetry property $f(a - x) = f(a + x)$, which ensures that the distribution is balanced around $a$.\n\n### Conclusion\n\nSince both the mean and the median are equal to $a$, we conclude that for a symmetric distribution, the mean is equal to the median."
    },
    {
      "id": "12",
      "statement": "An outlier is an observation that lies an abnormal distance from other values in a dataset.",
      "proof": "To mathematically define an outlier, we often use the concept of the interquartile range (IQR) in statistics. The IQR is the range between the first quartile (Q1) and the third quartile (Q3) of a dataset. It measures the spread of the middle 50% of the data.\n\nAn observation is typically considered an outlier if it lies below $Q1 - 1.5 \\times \\text{IQR}$ or above $Q3 + 1.5 \\times \\text{IQR}$. Let's provide a formal proof of this criterion:\n\n1. **Calculate the Quartiles:**\n   - Arrange the data in ascending order.\n   - The first quartile, $Q1$, is the median of the first half of the data.\n   - The third quartile, $Q3$, is the median of the second half of the data.\n\n2. **Calculate the Interquartile Range (IQR):**\n   \\[\n   \\text{IQR} = Q3 - Q1\n   \\]\n\n3. **Determine the Outlier Boundaries:**\n   - The lower boundary for outliers is $Q1 - 1.5 \\times \\text{IQR}$.\n   - The upper boundary for outliers is $Q3 + 1.5 \\times \\text{IQR}$.\n\n4. **Identify Outliers:**\n   - Any data point less than $Q1 - 1.5 \\times \\text{IQR}$ or greater than $Q3 + 1.5 \\times \\text{IQR}$ is considered an outlier.\n\nThe choice of $1.5 \\times \\text{IQR}$ as a multiplier is somewhat arbitrary but is widely used because it provides a balance between identifying extreme values and not being overly sensitive to small deviations. This method is robust and works well for a wide range of data distributions, especially when the data is not normally distributed."
    },
    {
      "id": "13",
      "statement": "The correlation coefficient between two variables ranges from -1 to 1.",
      "proof": "To prove that the correlation coefficient between two variables ranges from -1 to 1, we start by considering the definition of the Pearson correlation coefficient, $\\rho_{X,Y}$, for two random variables $X$ and $Y$:\n\n\\[\n\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\n\nwhere $\\text{Cov}(X, Y)$ is the covariance of $X$ and $Y$, and $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively.\n\nThe covariance is defined as:\n\n\\[\n\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]\n\\]\n\nwhere $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$ are the means of $X$ and $Y$.\n\nTo show that $-1 \\leq \\rho_{X,Y} \\leq 1$, we use the Cauchy-Schwarz inequality, which states that for any random variables $A$ and $B$:\n\n\\[\n\\mathbb{E}[A^2] \\mathbb{E}[B^2] \\geq (\\mathbb{E}[AB])^2\n\\]\n\nApplying this to $A = X - \\mu_X$ and $B = Y - \\mu_Y$, we have:\n\n\\[\n\\mathbb{E}[(X - \\mu_X)^2] \\mathbb{E}[(Y - \\mu_Y)^2] \\geq (\\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)])^2\n\\]\n\nThis simplifies to:\n\n\\[\n\\sigma_X^2 \\sigma_Y^2 \\geq (\\text{Cov}(X, Y))^2\n\\]\n\nTaking the square root of both sides, we obtain:\n\n\\[\n\\sigma_X \\sigma_Y \\geq |\\text{Cov}(X, Y)|\n\\]\n\nDividing both sides by $\\sigma_X \\sigma_Y$ (assuming $\\sigma_X, \\sigma_Y > 0$), we get:\n\n\\[\n1 \\geq \\left|\\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\right| = |\\rho_{X,Y}|\n\\]\n\nThus, we have shown that:\n\n\\[\n-1 \\leq \\rho_{X,Y} \\leq 1\n\\]\n\nThis completes the proof that the correlation coefficient between two variables ranges from -1 to 1."
    },
    {
      "id": "14",
      "statement": "If two events are independent, the probability of both occurring is the product of their individual probabilities.",
      "proof": "To prove that if two events are independent, the probability of both occurring is the product of their individual probabilities, we start with the definition of independent events.\n\nLet $A$ and $B$ be two events in a probability space. The events $A$ and $B$ are said to be independent if the occurrence of one does not affect the probability of the occurrence of the other. Mathematically, this is expressed as:\n\n\\[\nP(A \\cap B) = P(A) \\cdot P(B \\mid A)\n\\]\n\nFor independent events, the probability of $B$ occurring given that $A$ has occurred is the same as the probability of $B$ occurring without any condition. Therefore, we have:\n\n\\[\nP(B \\mid A) = P(B)\n\\]\n\nSubstituting this into the equation for the intersection of $A$ and $B$, we get:\n\n\\[\nP(A \\cap B) = P(A) \\cdot P(B)\n\\]\n\nThis shows that if $A$ and $B$ are independent, the probability of both $A$ and $B$ occurring is the product of their individual probabilities. Thus, the statement is proven."
    },
    {
      "id": "15",
      "statement": "A histogram is a graphical representation of the distribution of numerical data.",
      "proof": "A histogram is a type of bar chart that represents the frequency distribution of a dataset. It is used to visualize the distribution of numerical data by dividing the data into intervals, known as bins, and displaying the frequency of data points within each bin.\n\nTo construct a histogram, follow these steps:\n\n1. **Collect and Organize Data**: Gather the numerical data you wish to analyze.\n\n2. **Determine the Range**: Find the minimum and maximum values in the dataset to determine the range.\n\n3. **Choose the Number of Bins**: Decide how many bins to use. The number of bins can affect the appearance of the histogram. A common rule of thumb is to use the square root of the number of data points, but this can vary depending on the dataset and the level of detail desired.\n\n4. **Calculate Bin Width**: Divide the range by the number of bins to determine the width of each bin. Ensure that the bins are of equal width.\n\n5. **Create Bins**: Define the intervals (bins) based on the calculated bin width.\n\n6. **Count Frequencies**: Count the number of data points that fall into each bin.\n\n7. **Draw the Histogram**: On the horizontal axis, place the bins. On the vertical axis, place the frequencies. Draw bars for each bin with heights corresponding to the frequencies.\n\nA histogram provides a visual summary of the data's distribution, showing patterns such as skewness, modality, and the presence of outliers. It is particularly useful for identifying the shape of the data distribution, such as whether it is normal, skewed, or has multiple peaks."
    }
  ],
  "Trigonometry": [
    {
      "id": "1",
      "statement": "The sine of 0 degrees is 0.",
      "proof": "To prove that the sine of 0 degrees is 0, we can use the unit circle definition of the sine function.\n\nThe unit circle is a circle with a radius of 1 centered at the origin of the coordinate plane. The angle $\\theta$ is measured from the positive x-axis, and the coordinates of a point on the unit circle corresponding to the angle $\\theta$ are given by $(\\cos \\theta, \\sin \\theta)$.\n\nFor $\\theta = 0$ degrees, the point on the unit circle is $(\\cos 0^\\circ, \\sin 0^\\circ)$. At 0 degrees, the point on the unit circle is at $(1, 0)$ because the angle is along the positive x-axis.\n\nTherefore, $\\sin 0^\\circ$ is the y-coordinate of this point, which is 0. Thus, we have:\n\n\\[\n\\sin 0^\\circ = 0\n\\]\n\nThis completes the proof that the sine of 0 degrees is 0."
    },
    {
      "id": "2",
      "statement": "The cosine of 90 degrees is 0.",
      "proof": "To prove that the cosine of 90 degrees is 0, we can use the unit circle definition of trigonometric functions.\n\nThe unit circle is a circle with a radius of 1 centered at the origin of the coordinate plane. The angle in standard position is measured from the positive x-axis, and the coordinates of a point on the unit circle corresponding to an angle $\\theta$ are given by $(\\cos \\theta, \\sin \\theta)$.\n\nFor an angle of 90 degrees (or $\\frac{\\pi}{2}$ radians), the point on the unit circle is at the top of the circle. The coordinates of this point are $(0, 1)$ because the radius of the circle is 1 and the point is directly above the origin on the y-axis.\n\nThus, for $\\theta = 90^\\circ$, the cosine of the angle, which is the x-coordinate of the point on the unit circle, is:\n\n\\[\n\\cos 90^\\circ = 0\n\\]\n\nTherefore, the cosine of 90 degrees is indeed 0."
    },
    {
      "id": "3",
      "statement": "The tangent of 45 degrees is 1.",
      "proof": "To prove that the tangent of 45 degrees is 1, we can use the definition of the tangent function in terms of a right triangle and the properties of an isosceles right triangle.\n\nConsider a right triangle where one of the angles is 45 degrees. In an isosceles right triangle, the two non-right angles are equal, and since the sum of angles in a triangle is 180 degrees, each of the non-right angles must be 45 degrees.\n\nLet the two legs of this isosceles right triangle be of length $a$. The hypotenuse, by the Pythagorean theorem, will have a length of $\\sqrt{a^2 + a^2} = \\sqrt{2a^2} = a\\sqrt{2}$.\n\nThe tangent of an angle in a right triangle is defined as the ratio of the length of the opposite side to the length of the adjacent side. For a 45-degree angle in our isosceles right triangle, the opposite and adjacent sides are the same, both having length $a$.\n\nThus, the tangent of 45 degrees is:\n\n\\[\n\\tan(45^\\circ) = \\frac{\\text{opposite}}{\\text{adjacent}} = \\frac{a}{a} = 1\n\\]\n\nTherefore, the tangent of 45 degrees is indeed 1."
    },
    {
      "id": "4",
      "statement": "The sine of an angle is equal to the cosine of its complement.",
      "proof": "To prove that the sine of an angle is equal to the cosine of its complement, we start by considering an angle $\\theta$ and its complement, which is $90^\\circ - \\theta$.\n\nThe statement we want to prove is:\n\n\\[\n\\sin(\\theta) = \\cos(90^\\circ - \\theta)\n\\]\n\nWe will use the co-function identity from trigonometry, which states that the sine of an angle is equal to the cosine of its complementary angle. Specifically, the identity is:\n\n\\[\n\\sin(\\theta) = \\cos(90^\\circ - \\theta)\n\\]\n\nTo understand why this identity holds, consider a right triangle. In a right triangle, the sum of the angles is $180^\\circ$. If one angle is $\\theta$, the other non-right angle must be $90^\\circ - \\theta$ because the right angle is $90^\\circ$.\n\nIn this right triangle:\n\n- The sine of angle $\\theta$ is defined as the ratio of the length of the side opposite $\\theta$ to the hypotenuse.\n- The cosine of angle $90^\\circ - \\theta$ is defined as the ratio of the length of the side adjacent to $90^\\circ - \\theta$ to the hypotenuse.\n\nHowever, in a right triangle, the side opposite $\\theta$ is the same as the side adjacent to $90^\\circ - \\theta$. Therefore, the two ratios are equal, which gives us:\n\n\\[\n\\sin(\\theta) = \\cos(90^\\circ - \\theta)\n\\]\n\nThis completes the proof that the sine of an angle is equal to the cosine of its complement."
    },
    {
      "id": "5",
      "statement": "The sum of the angles in any triangle is 180 degrees.",
      "proof": "To prove that the sum of the angles in any triangle is 180 degrees, we can use a geometric approach involving parallel lines.\n\nConsider a triangle $\\triangle ABC$. We want to show that the sum of the interior angles $\\angle A + \\angle B + \\angle C = 180^\\circ$.\n\n1. Draw a line parallel to the side $BC$ of the triangle through the vertex $A$. Let this line intersect the extensions of sides $AB$ and $AC$ at points $D$ and $E$, respectively.\n\n2. Since $DE$ is parallel to $BC$, by the Alternate Interior Angles Theorem, we have:\n   - $\\angle DAB = \\angle ABC$ (alternate interior angles)\n   - $\\angle EAC = \\angle ACB$ (alternate interior angles)\n\n3. The angles $\\angle DAB$, $\\angle BAC$, and $\\angle EAC$ form a straight line at point $A$, so their sum is $180^\\circ$:\n   \\[\n   \\angle DAB + \\angle BAC + \\angle EAC = 180^\\circ\n   \\]\n\n4. Substituting the equal angles from step 2, we have:\n   \\[\n   \\angle ABC + \\angle BAC + \\angle ACB = 180^\\circ\n   \\]\n\nThus, the sum of the angles in triangle $\\triangle ABC$ is $180^\\circ$. This completes the proof."
    },
    {
      "id": "6",
      "statement": "The tangent of an angle is the sine of the angle divided by the cosine of the angle.",
      "proof": "To prove that the tangent of an angle is the sine of the angle divided by the cosine of the angle, we start with the definitions of the trigonometric functions sine, cosine, and tangent in the context of a right triangle.\n\nConsider a right triangle with an angle $\\theta$. The definitions of the sine, cosine, and tangent functions for this angle are as follows:\n\n- The sine of the angle $\\theta$ is defined as the ratio of the length of the opposite side to the length of the hypotenuse:\n  \\[\n  \\sin(\\theta) = \\frac{\\text{opposite}}{\\text{hypotenuse}}\n  \\]\n\n- The cosine of the angle $\\theta$ is defined as the ratio of the length of the adjacent side to the length of the hypotenuse:\n  \\[\n  \\cos(\\theta) = \\frac{\\text{adjacent}}{\\text{hypotenuse}}\n  \\]\n\n- The tangent of the angle $\\theta$ is defined as the ratio of the length of the opposite side to the length of the adjacent side:\n  \\[\n  \\tan(\\theta) = \\frac{\\text{opposite}}{\\text{adjacent}}\n  \\]\n\nWe want to show that:\n\\[\n\\tan(\\theta) = \\frac{\\sin(\\theta)}{\\cos(\\theta)}\n\\]\n\nSubstituting the definitions of sine and cosine into the right-hand side of the equation, we have:\n\\[\n\\frac{\\sin(\\theta)}{\\cos(\\theta)} = \\frac{\\frac{\\text{opposite}}{\\text{hypotenuse}}}{\\frac{\\text{adjacent}}{\\text{hypotenuse}}}\n\\]\n\nTo simplify this expression, we multiply the numerator and the denominator by the hypotenuse:\n\\[\n\\frac{\\frac{\\text{opposite}}{\\text{hypotenuse}}}{\\frac{\\text{adjacent}}{\\text{hypotenuse}}} = \\frac{\\text{opposite} \\cdot \\text{hypotenuse}}{\\text{hypotenuse} \\cdot \\text{adjacent}} = \\frac{\\text{opposite}}{\\text{adjacent}}\n\\]\n\nThis is exactly the definition of $\\tan(\\theta)$:\n\\[\n\\tan(\\theta) = \\frac{\\text{opposite}}{\\text{adjacent}}\n\\]\n\nThus, we have shown that:\n\\[\n\\tan(\\theta) = \\frac{\\sin(\\theta)}{\\cos(\\theta)}\n\\]\n\nThis completes the proof."
    },
    {
      "id": "7",
      "statement": "The sine of 30 degrees is 1/2.",
      "proof": "To prove that the sine of 30 degrees is $\\frac{1}{2}$, we can use the properties of a 30-60-90 right triangle.\n\nConsider an equilateral triangle with each side of length 2. Since all angles in an equilateral triangle are 60 degrees, if we draw an altitude from one vertex to the opposite side, it will bisect the angle and the opposite side, creating two 30-60-90 right triangles.\n\nIn a 30-60-90 triangle, the sides are in the ratio $1 : \\sqrt{3} : 2$. The side opposite the 30-degree angle is the shortest, the side opposite the 60-degree angle is $\\sqrt{3}$ times the shortest side, and the hypotenuse is twice the shortest side.\n\nIn our equilateral triangle, the altitude divides the triangle into two right triangles, each with a hypotenuse of 2 (the original side of the equilateral triangle), a base of 1 (half of the original side), and an altitude of $\\sqrt{3}$ (by the Pythagorean theorem).\n\nNow, in one of these right triangles, the sine of the 30-degree angle is the ratio of the length of the side opposite the angle to the hypotenuse. Therefore, we have:\n\n\\[\n\\sin(30^\\circ) = \\frac{\\text{opposite side}}{\\text{hypotenuse}} = \\frac{1}{2}\n\\]\n\nThus, the sine of 30 degrees is indeed $\\frac{1}{2}$."
    },
    {
      "id": "8",
      "statement": "The cosine of 60 degrees is 1/2.",
      "proof": "To prove that the cosine of 60 degrees is $\\frac{1}{2}$, we can use the properties of an equilateral triangle.\n\nConsider an equilateral triangle $ABC$ with each side of length $2$. Since the triangle is equilateral, each angle is $60^\\circ$. \n\nNow, draw an altitude from vertex $A$ to the midpoint $D$ of side $BC$. This altitude will bisect the angle at $A$ and the side $BC$, creating two right triangles, $\\triangle ABD$ and $\\triangle ACD$. \n\nSince $D$ is the midpoint of $BC$, $BD = DC = 1$. The altitude $AD$ is perpendicular to $BC$, so $\\angle ADB = 90^\\circ$. \n\nIn the right triangle $\\triangle ABD$, we have:\n- $AB = 2$ (the side of the equilateral triangle),\n- $BD = 1$ (half of the base $BC$),\n- $\\angle BAD = 60^\\circ$.\n\nUsing the definition of cosine in a right triangle, we have:\n\\[\n\\cos(60^\\circ) = \\frac{\\text{adjacent side}}{\\text{hypotenuse}} = \\frac{BD}{AB} = \\frac{1}{2}.\n\\]\n\nThus, the cosine of 60 degrees is $\\frac{1}{2}$."
    },
    {
      "id": "9",
      "statement": "For any angle, the sine squared of the angle plus the cosine squared of the angle equals 1.",
      "proof": "The statement to prove is the Pythagorean identity: \n\n\\[\n\\sin^2 \\theta + \\cos^2 \\theta = 1\n\\]\n\nfor any angle $\\theta$.\n\n**Proof:**\n\nConsider a right triangle with an angle $\\theta$. Let the hypotenuse of the triangle be of length $1$. The side opposite the angle $\\theta$ has length $\\sin \\theta$, and the side adjacent to the angle $\\theta$ has length $\\cos \\theta$. \n\nBy the Pythagorean theorem, the sum of the squares of the lengths of the two legs of a right triangle is equal to the square of the length of the hypotenuse. Therefore, we have:\n\n\\[\n(\\sin \\theta)^2 + (\\cos \\theta)^2 = 1^2\n\\]\n\nwhich simplifies to:\n\n\\[\n\\sin^2 \\theta + \\cos^2 \\theta = 1\n\\]\n\nThis identity holds for any angle $\\theta$, as it is derived from the fundamental properties of right triangles and the definitions of sine and cosine on the unit circle. Thus, the Pythagorean identity is proven."
    },
    {
      "id": "10",
      "statement": "The secant of an angle is the reciprocal of the cosine of the angle.",
      "proof": "To prove that the secant of an angle is the reciprocal of the cosine of the angle, we start with the definitions of the trigonometric functions involved.\n\nThe cosine of an angle $\\theta$ in a right triangle is defined as the ratio of the length of the adjacent side to the hypotenuse. Mathematically, this is expressed as:\n\n\\[\n\\cos \\theta = \\frac{\\text{adjacent}}{\\text{hypotenuse}}\n\\]\n\nThe secant of an angle $\\theta$ is defined as the reciprocal of the cosine of the angle. Therefore, it is given by:\n\n\\[\n\\sec \\theta = \\frac{1}{\\cos \\theta}\n\\]\n\nSubstituting the expression for $\\cos \\theta$ into the formula for $\\sec \\theta$, we have:\n\n\\[\n\\sec \\theta = \\frac{1}{\\frac{\\text{adjacent}}{\\text{hypotenuse}}} = \\frac{\\text{hypotenuse}}{\\text{adjacent}}\n\\]\n\nThis shows that the secant of an angle is indeed the reciprocal of the cosine of the angle, as it is the ratio of the hypotenuse to the adjacent side, which is the inverse of the cosine ratio. Thus, the statement is proven."
    },
    {
      "id": "11",
      "statement": "Co-terminal angles have the same values for sine, cosine, and tangent functions.",
      "proof": "To prove that co-terminal angles have the same values for sine, cosine, and tangent functions, let's consider an angle $\\theta$ and its co-terminal angle $\\theta + 2\\pi k$, where $k$ is any integer. Co-terminal angles differ by a full rotation of $2\\pi$ radians (or $360^\\circ$), which means they end at the same position on the unit circle.\n\n1. **Sine Function:**\n\n   The sine of an angle $\\theta$ is defined as the $y$-coordinate of the point on the unit circle corresponding to that angle. For the co-terminal angle $\\theta + 2\\pi k$, the position on the unit circle is the same as for $\\theta$, because adding $2\\pi k$ corresponds to completing $k$ full rotations around the circle. Therefore, the $y$-coordinate remains unchanged:\n\n   \\[\n   \\sin(\\theta + 2\\pi k) = \\sin(\\theta)\n   \\]\n\n2. **Cosine Function:**\n\n   Similarly, the cosine of an angle $\\theta$ is defined as the $x$-coordinate of the point on the unit circle corresponding to that angle. For the co-terminal angle $\\theta + 2\\pi k$, the $x$-coordinate is also unchanged:\n\n   \\[\n   \\cos(\\theta + 2\\pi k) = \\cos(\\theta)\n   \\]\n\n3. **Tangent Function:**\n\n   The tangent of an angle $\\theta$ is defined as the ratio of the sine to the cosine of that angle:\n\n   \\[\n   \\tan(\\theta) = \\frac{\\sin(\\theta)}{\\cos(\\theta)}\n   \\]\n\n   Since both the sine and cosine values are unchanged for co-terminal angles, the tangent value is also unchanged:\n\n   \\[\n   \\tan(\\theta + 2\\pi k) = \\frac{\\sin(\\theta + 2\\pi k)}{\\cos(\\theta + 2\\pi k)} = \\frac{\\sin(\\theta)}{\\cos(\\theta)} = \\tan(\\theta)\n   \\]\n\nThus, we have shown that co-terminal angles have the same values for the sine, cosine, and tangent functions."
    },
    {
      "id": "12",
      "statement": "The sine function has a period of 360 degrees.",
      "proof": "To prove that the sine function has a period of 360 degrees, we need to show that for any angle $\\theta$, the sine function satisfies the periodicity condition:\n\n\\[\n\\sin(\\theta + 360^\\circ) = \\sin(\\theta).\n\\]\n\nThe sine function is defined as the y-coordinate of a point on the unit circle corresponding to an angle $\\theta$ measured from the positive x-axis. \n\n1. **Unit Circle Definition**: Consider a point $P$ on the unit circle with coordinates $(\\cos(\\theta), \\sin(\\theta))$. When we increase the angle by $360^\\circ$, the point $P$ completes one full rotation around the circle and returns to its original position. Therefore, the coordinates of the point after a $360^\\circ$ rotation are the same as the original coordinates, i.e.,\n\n   \\[\n   (\\cos(\\theta + 360^\\circ), \\sin(\\theta + 360^\\circ)) = (\\cos(\\theta), \\sin(\\theta)).\n   \\]\n\n2. **Conclusion**: From the above, it follows that:\n\n   \\[\n   \\sin(\\theta + 360^\\circ) = \\sin(\\theta).\n   \\]\n\nThis shows that the sine function repeats its values every $360^\\circ$, confirming that the period of the sine function is indeed $360^\\circ$."
    },
    {
      "id": "13",
      "statement": "The cosecant of 90 degrees is 1.",
      "proof": "To prove that the cosecant of 90 degrees is 1, we start by recalling the definition of the cosecant function. The cosecant of an angle $\\theta$ is defined as the reciprocal of the sine of that angle:\n\n\\[\n\\csc(\\theta) = \\frac{1}{\\sin(\\theta)}\n\\]\n\nWe need to evaluate $\\csc(90^\\circ)$. First, we find $\\sin(90^\\circ)$.\n\nThe sine of 90 degrees is a well-known trigonometric value:\n\n\\[\n\\sin(90^\\circ) = 1\n\\]\n\nSubstituting this into the definition of cosecant, we have:\n\n\\[\n\\csc(90^\\circ) = \\frac{1}{\\sin(90^\\circ)} = \\frac{1}{1} = 1\n\\]\n\nTherefore, the cosecant of 90 degrees is indeed 1."
    },
    {
      "id": "14",
      "statement": "The cosine of 0 degrees is 1.",
      "proof": "To prove that the cosine of 0 degrees is 1, we can use the unit circle definition of the trigonometric functions.\n\nThe unit circle is a circle with a radius of 1 centered at the origin of the coordinate plane. The angle in standard position is measured from the positive x-axis, and the coordinates of a point on the unit circle corresponding to an angle $\\theta$ are given by $(\\cos \\theta, \\sin \\theta)$.\n\nFor an angle of 0 degrees, the point on the unit circle is at $(1, 0)$. This is because the angle of 0 degrees corresponds to the positive x-axis, where the x-coordinate is 1 (the radius of the unit circle) and the y-coordinate is 0.\n\nTherefore, the cosine of 0 degrees, which is the x-coordinate of the point on the unit circle, is:\n\n\\[\n\\cos(0^\\circ) = 1\n\\]\n\nThis completes the proof that the cosine of 0 degrees is 1."
    },
    {
      "id": "15",
      "statement": "The tangent function has vertical asymptotes where the cosine is zero.",
      "proof": "To prove that the tangent function has vertical asymptotes where the cosine is zero, we start by considering the definition of the tangent function in terms of sine and cosine:\n\n\\[\n\\tan(x) = \\frac{\\sin(x)}{\\cos(x)}\n\\]\n\nA vertical asymptote occurs in a function when the function approaches infinity as the input approaches a certain value. For the tangent function, this happens when the denominator, $\\cos(x)$, approaches zero, while the numerator, $\\sin(x)$, remains finite.\n\nThe cosine function, $\\cos(x)$, is zero at the points:\n\n\\[\nx = \\frac{\\pi}{2} + k\\pi, \\quad \\text{where } k \\text{ is an integer}\n\\]\n\nAt these points, the tangent function is undefined because division by zero is not possible. As $x$ approaches $\\frac{\\pi}{2} + k\\pi$ from the left or the right, the value of $\\tan(x)$ tends to $\\pm \\infty$, indicating a vertical asymptote.\n\nTo see this more clearly, consider the behavior of $\\tan(x)$ as $x$ approaches $\\frac{\\pi}{2}$ from the left ($x \\to \\frac{\\pi}{2}^-$) and from the right ($x \\to \\frac{\\pi}{2}^+$):\n\n- As $x \\to \\frac{\\pi}{2}^-$, $\\cos(x) \\to 0^+$, and $\\sin(x)$ approaches 1. Thus, $\\tan(x) = \\frac{\\sin(x)}{\\cos(x)} \\to +\\infty$.\n- As $x \\to \\frac{\\pi}{2}^+$, $\\cos(x) \\to 0^-$, and $\\sin(x)$ approaches 1. Thus, $\\tan(x) = \\frac{\\sin(x)}{\\cos(x)} \\to -\\infty$.\n\nThis behavior is similar for all points $x = \\frac{\\pi}{2} + k\\pi$. Therefore, the tangent function has vertical asymptotes at these points, where the cosine function is zero."
    }
  ],
  "Probability": [
    {
      "id": "1",
      "statement": "If a coin is flipped, there are two possible outcomes: heads or tails.",
      "proof": "To prove that if a coin is flipped, there are two possible outcomes, heads or tails, we can use the basic principles of probability and the definition of a fair coin.\n\nA fair coin is defined as a coin that has two distinct sides, one labeled \"heads\" and the other labeled \"tails.\" When the coin is flipped, it will land on one of these two sides. \n\nLet's denote the sample space $S$ of this experiment, which is the set of all possible outcomes, as follows:\n\n\\[ S = \\{ \\text{Heads}, \\text{Tails} \\} \\]\n\nThe sample space $S$ contains exactly two elements, corresponding to the two possible outcomes of the coin flip.\n\nBy the definition of probability, the probability of an event is the number of favorable outcomes divided by the total number of possible outcomes. Since there are two possible outcomes in the sample space, the probability of each outcome (assuming the coin is fair) is:\n\n\\[ P(\\text{Heads}) = \\frac{1}{2} \\]\n\\[ P(\\text{Tails}) = \\frac{1}{2} \\]\n\nThus, when a coin is flipped, there are indeed two possible outcomes: heads or tails. This is consistent with the definition of a fair coin and the basic principles of probability."
    },
    {
      "id": "2",
      "statement": "The probability of rolling a 3 on a standard six-sided die is 1/6.",
      "proof": "To prove that the probability of rolling a 3 on a standard six-sided die is $\\frac{1}{6}$, we start by considering the properties of a standard six-sided die.\n\nA standard six-sided die has six faces, each numbered from 1 to 6. When the die is rolled, each face has an equal chance of landing face up. Therefore, the die is a fair die, and each outcome is equally likely.\n\nThe sample space $S$ for a single roll of the die is:\n\n\\[ S = \\{1, 2, 3, 4, 5, 6\\} \\]\n\nThe total number of possible outcomes is the size of the sample space, which is 6.\n\nWe are interested in the event $E$ of rolling a 3. The event $E$ is:\n\n\\[ E = \\{3\\} \\]\n\nThe number of favorable outcomes for this event is 1, since there is only one face with the number 3.\n\nThe probability $P(E)$ of the event $E$ occurring is given by the ratio of the number of favorable outcomes to the total number of possible outcomes:\n\n\\[ P(E) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}} = \\frac{1}{6} \\]\n\nThus, the probability of rolling a 3 on a standard six-sided die is $\\frac{1}{6}$."
    },
    {
      "id": "3",
      "statement": "The probability that an event occurs and the probability that the event does not occur always sum to 1.",
      "proof": "To prove that the probability of an event occurring and the probability of the event not occurring always sum to 1, we start with the basic axioms of probability.\n\nLet $E$ be an event in a sample space $S$. The probability of the event $E$ occurring is denoted by $P(E)$, and the probability of the event $E$ not occurring is denoted by $P(E^c)$, where $E^c$ is the complement of $E$ in the sample space $S$.\n\nAccording to the axioms of probability:\n\n1. The probability of the entire sample space is 1: $P(S) = 1$.\n2. The probability of the complement of an event is given by: $P(E^c) = P(S) - P(E)$.\n\nSince $E$ and $E^c$ are complementary events, they are mutually exclusive and exhaustive, meaning that $E \\cup E^c = S$ and $E \\cap E^c = \\emptyset$. Therefore, we have:\n\n\\[\nP(E \\cup E^c) = P(S) = 1\n\\]\n\nBy the additivity axiom for mutually exclusive events, we have:\n\n\\[\nP(E \\cup E^c) = P(E) + P(E^c)\n\\]\n\nSubstituting $P(S) = 1$ into the equation, we get:\n\n\\[\nP(E) + P(E^c) = 1\n\\]\n\nThus, the probability that an event occurs and the probability that the event does not occur always sum to 1."
    },
    {
      "id": "4",
      "statement": "When rolling a six-sided die, the probability of rolling an even number (2, 4, or 6) is 1/2.",
      "proof": "To prove that the probability of rolling an even number on a six-sided die is $\\frac{1}{2}$, we start by identifying the sample space and the favorable outcomes.\n\n1. **Sample Space**: A standard six-sided die has six faces, numbered 1 through 6. Therefore, the sample space $S$ is:\n   \\[\n   S = \\{1, 2, 3, 4, 5, 6\\}\n   \\]\n   The total number of possible outcomes is $|S| = 6$.\n\n2. **Favorable Outcomes**: The even numbers on a six-sided die are 2, 4, and 6. Thus, the set of favorable outcomes $E$ is:\n   \\[\n   E = \\{2, 4, 6\\}\n   \\]\n   The number of favorable outcomes is $|E| = 3$.\n\n3. **Probability Calculation**: The probability of an event is given by the ratio of the number of favorable outcomes to the total number of possible outcomes. Therefore, the probability $P(E)$ of rolling an even number is:\n   \\[\n   P(E) = \\frac{|E|}{|S|} = \\frac{3}{6} = \\frac{1}{2}\n   \\]\n\nThus, the probability of rolling an even number on a six-sided die is indeed $\\frac{1}{2}$."
    },
    {
      "id": "5",
      "statement": "If a card is drawn from a standard deck, the probability of the card being a heart is 1/4.",
      "proof": "To prove that the probability of drawing a heart from a standard deck of cards is $\\frac{1}{4}$, we start by considering the composition of a standard deck.\n\nA standard deck of cards consists of 52 cards, which are divided into 4 suits: hearts, diamonds, clubs, and spades. Each suit contains 13 cards.\n\nThe probability of an event is given by the ratio of the number of favorable outcomes to the total number of possible outcomes. In this case, the favorable outcomes are the number of heart cards, and the total possible outcomes are the total number of cards in the deck.\n\n1. **Number of favorable outcomes (hearts):** There are 13 hearts in a deck.\n2. **Total number of possible outcomes (total cards):** There are 52 cards in total.\n\nThe probability $P$ of drawing a heart is therefore:\n\n\\[\nP(\\text{heart}) = \\frac{\\text{Number of hearts}}{\\text{Total number of cards}} = \\frac{13}{52}\n\\]\n\nSimplifying the fraction $\\frac{13}{52}$:\n\n\\[\n\\frac{13}{52} = \\frac{13 \\div 13}{52 \\div 13} = \\frac{1}{4}\n\\]\n\nThus, the probability of drawing a heart from a standard deck of cards is $\\frac{1}{4}$."
    },
    {
      "id": "6",
      "statement": "For two independent events A and B, the probability that both A and B happen is the product of their individual probabilities.",
      "proof": "To prove that for two independent events $A$ and $B$, the probability that both $A$ and $B$ happen is the product of their individual probabilities, we start with the definition of independence.\n\nTwo events $A$ and $B$ are said to be independent if the occurrence of one does not affect the probability of the occurrence of the other. Mathematically, this is expressed as:\n\n\\[\nP(A \\cap B) = P(A) \\cdot P(B \\mid A)\n\\]\n\nFor independent events, the probability of $B$ given $A$ is simply the probability of $B$, i.e., $P(B \\mid A) = P(B)$. Therefore, the equation becomes:\n\n\\[\nP(A \\cap B) = P(A) \\cdot P(B)\n\\]\n\nThis shows that for independent events $A$ and $B$, the probability that both events occur is the product of their individual probabilities. Thus, the statement is proven."
    },
    {
      "id": "7",
      "statement": "The probability of selecting a red marble from a bag that contains only red marbles is 1.",
      "proof": "To prove that the probability of selecting a red marble from a bag that contains only red marbles is 1, we can use the basic definition of probability.\n\nLet the total number of marbles in the bag be $n$, and since all marbles are red, the number of red marbles is also $n$.\n\nThe probability $P$ of an event is given by the ratio of the number of favorable outcomes to the total number of possible outcomes. In this case, the favorable outcome is selecting a red marble, and the total number of possible outcomes is selecting any marble from the bag.\n\nThus, the probability $P$ of selecting a red marble is:\n\n\\[\nP(\\text{red marble}) = \\frac{\\text{Number of red marbles}}{\\text{Total number of marbles}} = \\frac{n}{n} = 1\n\\]\n\nTherefore, the probability of selecting a red marble from a bag that contains only red marbles is 1."
    },
    {
      "id": "8",
      "statement": "In a single draw from a standard 52-card deck, the probability of drawing a King is 4/52 or 1/13.",
      "proof": "To prove that the probability of drawing a King from a standard 52-card deck is $\\frac{4}{52}$ or $\\frac{1}{13}$, we proceed as follows:\n\nA standard deck of cards consists of 52 cards, which are divided into 4 suits: hearts, diamonds, clubs, and spades. Each suit contains 13 cards, and among these 13 cards, there is exactly one King.\n\nTherefore, there are 4 Kings in the entire deck (one King from each suit).\n\nThe probability of drawing a King from the deck is given by the ratio of the number of favorable outcomes (drawing a King) to the total number of possible outcomes (drawing any card from the deck). Mathematically, this is expressed as:\n\n\\[\nP(\\text{King}) = \\frac{\\text{Number of Kings}}{\\text{Total number of cards}} = \\frac{4}{52}\n\\]\n\nWe can simplify this fraction by dividing both the numerator and the denominator by their greatest common divisor, which is 4:\n\n\\[\n\\frac{4}{52} = \\frac{4 \\div 4}{52 \\div 4} = \\frac{1}{13}\n\\]\n\nThus, the probability of drawing a King from a standard 52-card deck is indeed $\\frac{1}{13}$."
    },
    {
      "id": "9",
      "statement": "When you roll two six-sided dice, the probability that their total sum is 7 is more than the probability that it is 2.",
      "proof": "To prove this statement, we need to calculate the probabilities of the two events: the sum of the numbers on two six-sided dice being 7, and the sum being 2.\n\n**Step 1: Calculate the probability of the sum being 7.**\n\nWhen rolling two six-sided dice, each die has 6 faces, so there are a total of $6 \\times 6 = 36$ possible outcomes.\n\nThe pairs of numbers that sum to 7 are:\n- (1, 6)\n- (2, 5)\n- (3, 4)\n- (4, 3)\n- (5, 2)\n- (6, 1)\n\nThere are 6 such pairs.\n\nThus, the probability that the sum is 7 is:\n\\[\nP(\\text{sum is 7}) = \\frac{6}{36} = \\frac{1}{6}\n\\]\n\n**Step 2: Calculate the probability of the sum being 2.**\n\nThe only pair of numbers that sums to 2 is:\n- (1, 1)\n\nThere is only 1 such pair.\n\nThus, the probability that the sum is 2 is:\n\\[\nP(\\text{sum is 2}) = \\frac{1}{36}\n\\]\n\n**Step 3: Compare the probabilities.**\n\nWe have:\n\\[\nP(\\text{sum is 7}) = \\frac{1}{6} = \\frac{6}{36}\n\\]\n\\[\nP(\\text{sum is 2}) = \\frac{1}{36}\n\\]\n\nClearly, $\\frac{6}{36} > \\frac{1}{36}$.\n\nTherefore, the probability that the sum of the numbers on two six-sided dice is 7 is indeed more than the probability that it is 2. This completes the proof."
    },
    {
      "id": "10",
      "statement": "If a single six-sided die is rolled, the probability of not rolling a 1 is 5/6.",
      "proof": "To prove that the probability of not rolling a 1 on a single six-sided die is $\\frac{5}{6}$, we start by considering the sample space of the die roll.\n\nA standard six-sided die has six faces, numbered 1 through 6. The sample space $S$ of a single die roll is:\n\n\\[ S = \\{1, 2, 3, 4, 5, 6\\} \\]\n\nThe event $A$ of rolling a 1 is:\n\n\\[ A = \\{1\\} \\]\n\nThe probability of event $A$, denoted as $P(A)$, is the ratio of the number of favorable outcomes to the total number of possible outcomes. Since there is only one way to roll a 1, we have:\n\n\\[ P(A) = \\frac{\\text{Number of favorable outcomes for } A}{\\text{Total number of outcomes}} = \\frac{1}{6} \\]\n\nThe event of not rolling a 1 is the complement of event $A$, denoted as $A^c$. The probability of the complement event $A^c$, $P(A^c)$, is given by:\n\n\\[ P(A^c) = 1 - P(A) \\]\n\nSubstituting the value of $P(A)$, we get:\n\n\\[ P(A^c) = 1 - \\frac{1}{6} = \\frac{6}{6} - \\frac{1}{6} = \\frac{5}{6} \\]\n\nThus, the probability of not rolling a 1 on a single six-sided die is $\\frac{5}{6}$."
    },
    {
      "id": "11",
      "statement": "The probability of choosing an even number from the numbers 1 to 10 is 1/2.",
      "proof": "To prove that the probability of choosing an even number from the numbers 1 to 10 is $\\frac{1}{2}$, we first identify the total number of outcomes and the number of favorable outcomes.\n\n1. **Total number of outcomes**: The numbers from 1 to 10 are $\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$. There are 10 numbers in total.\n\n2. **Favorable outcomes**: The even numbers within this set are $\\{2, 4, 6, 8, 10\\}$. There are 5 even numbers.\n\nThe probability of choosing an even number is given by the ratio of the number of favorable outcomes to the total number of outcomes. Therefore, the probability $P$ is:\n\n\\[\nP(\\text{even number}) = \\frac{\\text{Number of even numbers}}{\\text{Total number of numbers}} = \\frac{5}{10} = \\frac{1}{2}\n\\]\n\nThus, the probability of choosing an even number from the numbers 1 to 10 is indeed $\\frac{1}{2}$."
    },
    {
      "id": "12",
      "statement": "In a fair coin toss, the probability of getting heads is the same as the probability of getting tails.",
      "proof": "To prove that in a fair coin toss, the probability of getting heads is the same as the probability of getting tails, we start by defining the sample space and the events.\n\nA fair coin has two possible outcomes when tossed: heads (H) and tails (T). The sample space $S$ for a single coin toss is:\n\n\\[ S = \\{ H, T \\} \\]\n\nSince the coin is fair, each outcome is equally likely. Therefore, the probability of each outcome is given by the formula for the probability of an event $E$:\n\n\\[ P(E) = \\frac{\\text{Number of favorable outcomes for } E}{\\text{Total number of possible outcomes}} \\]\n\nFor the event of getting heads, $E_H = \\{ H \\}$, there is 1 favorable outcome (getting heads), and the total number of possible outcomes is 2 (heads or tails). Thus, the probability of getting heads is:\n\n\\[ P(H) = \\frac{1}{2} \\]\n\nSimilarly, for the event of getting tails, $E_T = \\{ T \\}$, there is also 1 favorable outcome (getting tails), and the total number of possible outcomes is 2. Thus, the probability of getting tails is:\n\n\\[ P(T) = \\frac{1}{2} \\]\n\nSince $P(H) = \\frac{1}{2}$ and $P(T) = \\frac{1}{2}$, we conclude that the probability of getting heads is the same as the probability of getting tails in a fair coin toss."
    },
    {
      "id": "13",
      "statement": "The probability of drawing a black card from a well-shuffled standard deck of cards is 1/2.",
      "proof": "To prove that the probability of drawing a black card from a well-shuffled standard deck of cards is $\\frac{1}{2}$, we start by considering the composition of a standard deck of cards.\n\nA standard deck of cards consists of 52 cards, which are divided into four suits: spades, hearts, diamonds, and clubs. Each suit contains 13 cards. The suits of spades and clubs are black, while the suits of hearts and diamonds are red.\n\nThus, the number of black cards in the deck is the sum of the number of spades and the number of clubs:\n\n\\[\n13 \\, (\\text{spades}) + 13 \\, (\\text{clubs}) = 26 \\, \\text{black cards}\n\\]\n\nThe probability of drawing a black card from the deck is the ratio of the number of black cards to the total number of cards in the deck:\n\n\\[\n\\text{Probability of drawing a black card} = \\frac{\\text{Number of black cards}}{\\text{Total number of cards}} = \\frac{26}{52}\n\\]\n\nSimplifying the fraction:\n\n\\[\n\\frac{26}{52} = \\frac{1}{2}\n\\]\n\nTherefore, the probability of drawing a black card from a well-shuffled standard deck of cards is indeed $\\frac{1}{2}$."
    },
    {
      "id": "14",
      "statement": "The probability that a randomly selected day of the week is a weekend day (Saturday or Sunday) is 2/7.",
      "proof": "To prove that the probability of randomly selecting a weekend day (Saturday or Sunday) from a week is $\\frac{2}{7}$, we start by considering the total number of days in a week and the number of weekend days.\n\n1. **Total Number of Days in a Week**: There are 7 days in a week: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday.\n\n2. **Number of Weekend Days**: The weekend days are Saturday and Sunday. Therefore, there are 2 weekend days.\n\n3. **Probability Calculation**: The probability of an event is given by the ratio of the number of favorable outcomes to the total number of possible outcomes. In this case, the favorable outcomes are the weekend days, and the possible outcomes are all the days of the week.\n\n   \\[\n   \\text{Probability of selecting a weekend day} = \\frac{\\text{Number of weekend days}}{\\text{Total number of days in a week}} = \\frac{2}{7}\n   \\]\n\nThus, the probability that a randomly selected day of the week is a weekend day is indeed $\\frac{2}{7}$."
    },
    {
      "id": "15",
      "statement": "If you flip a coin and roll a die, the probability of getting a head and a roll of 4 is 1/12.",
      "proof": "To prove that the probability of getting a head when flipping a coin and rolling a 4 on a die is $\\frac{1}{12}$, we need to consider the individual probabilities of each event and then use the multiplication rule for independent events.\n\n1. **Probability of flipping a head:**\n\n   A fair coin has two possible outcomes: heads (H) or tails (T). The probability of getting a head is:\n   \\[\n   P(\\text{Head}) = \\frac{1}{2}\n   \\]\n\n2. **Probability of rolling a 4 on a die:**\n\n   A fair six-sided die has six possible outcomes: 1, 2, 3, 4, 5, and 6. The probability of rolling a 4 is:\n   \\[\n   P(\\text{Roll a 4}) = \\frac{1}{6}\n   \\]\n\n3. **Combined probability of both events:**\n\n   Since the coin flip and the die roll are independent events, the probability of both events occurring is the product of their individual probabilities:\n   \\[\n   P(\\text{Head and Roll a 4}) = P(\\text{Head}) \\times P(\\text{Roll a 4}) = \\frac{1}{2} \\times \\frac{1}{6} = \\frac{1}{12}\n   \\]\n\nThus, the probability of flipping a head and rolling a 4 is indeed $\\frac{1}{12}$."
    }
  ],
  "Combinatorics": [
    {
      "id": "1",
      "statement": "The number of ways to arrange a set of objects where some are identical is fewer than if all objects were distinct.",
      "proof": "To prove this statement, let's consider a set of $n$ objects where some of the objects are identical. Suppose we have $n_1$ objects of type 1, $n_2$ objects of type 2, ..., and $n_k$ objects of type $k$, such that $n_1 + n_2 + \\cdots + n_k = n$. \n\nThe number of distinct permutations of these objects is given by the multinomial coefficient:\n\n\\[\n\\frac{n!}{n_1! \\, n_2! \\, \\cdots \\, n_k!}\n\\]\n\nNow, consider the case where all $n$ objects are distinct. The number of permutations of $n$ distinct objects is simply $n!$.\n\nTo show that the number of ways to arrange the objects with some identical is fewer than if all were distinct, we need to show:\n\n\\[\n\\frac{n!}{n_1! \\, n_2! \\, \\cdots \\, n_k!} < n!\n\\]\n\nThis inequality holds because each $n_i! \\geq 1$, and at least one $n_i > 1$ (since some objects are identical), which implies $n_i! > 1$. Therefore, the product $n_1! \\, n_2! \\, \\cdots \\, n_k! > 1$, leading to:\n\n\\[\n\\frac{n!}{n_1! \\, n_2! \\, \\cdots \\, n_k!} < n!\n\\]\n\nThus, the number of ways to arrange a set of objects where some are identical is indeed fewer than if all objects were distinct."
    },
    {
      "id": "2",
      "statement": "The number of subsets of a set with \\(n\\) elements is \\(2^n\\).",
      "proof": "To prove that the number of subsets of a set with \\( n \\) elements is \\( 2^n \\), we can use the concept of binary choices for each element in the set.\n\nConsider a set \\( S \\) with \\( n \\) elements, say \\( S = \\{ a_1, a_2, \\ldots, a_n \\} \\).\n\nFor each element \\( a_i \\) in the set \\( S \\), there are two choices: either include \\( a_i \\) in a subset or do not include \\( a_i \\) in a subset. \n\nSince there are \\( n \\) elements in the set, and each element has 2 independent choices (include or not include), the total number of different subsets is the product of the number of choices for each element. This can be expressed as:\n\n\\[\n2 \\times 2 \\times \\cdots \\times 2 = 2^n\n\\]\n\nwhere there are \\( n \\) factors of 2, one for each element in the set.\n\nThus, the number of subsets of a set with \\( n \\) elements is \\( 2^n \\). This includes all possible subsets, ranging from the empty set to the set itself."
    },
    {
      "id": "3",
      "statement": "If you have a set of 3 items, there are exactly 6 permutations of these items.",
      "proof": "To prove that there are exactly 6 permutations of a set of 3 items, we can use the concept of factorial, which is a fundamental principle in combinatorics for counting permutations.\n\nConsider a set $S = \\{a, b, c\\}$ with 3 distinct items. The number of permutations of a set of $n$ distinct items is given by $n!$, which is the product of all positive integers up to $n$.\n\nFor our set $S$, we have $n = 3$. Therefore, the number of permutations is:\n\n\\[\n3! = 3 \\times 2 \\times 1 = 6\n\\]\n\nTo verify this, we can list all possible permutations of the set $\\{a, b, c\\}$:\n\n1. $(a, b, c)$\n2. $(a, c, b)$\n3. $(b, a, c)$\n4. $(b, c, a)$\n5. $(c, a, b)$\n6. $(c, b, a)$\n\nAs we can see, there are exactly 6 permutations, confirming that $3! = 6$. Thus, the statement that there are exactly 6 permutations of a set of 3 items is correct."
    },
    {
      "id": "4",
      "statement": "The total number of outcomes when flipping two coins is 4.",
      "proof": "To determine the total number of outcomes when flipping two coins, we can use the basic principle of counting. Each coin has two possible outcomes: heads (H) or tails (T).\n\nLet's consider the process of flipping two coins:\n\n1. **First Coin**: There are 2 possible outcomes (H or T).\n2. **Second Coin**: Similarly, there are 2 possible outcomes (H or T).\n\nSince the outcome of each coin flip is independent of the other, we can use the multiplication principle to find the total number of outcomes. The total number of outcomes is the product of the number of outcomes for each coin:\n\n\\[\n2 \\times 2 = 4\n\\]\n\nWe can also list all possible outcomes to verify this:\n\n- First coin is heads, second coin is heads: (H, H)\n- First coin is heads, second coin is tails: (H, T)\n- First coin is tails, second coin is heads: (T, H)\n- First coin is tails, second coin is tails: (T, T)\n\nThus, there are indeed 4 possible outcomes when flipping two coins."
    },
    {
      "id": "5",
      "statement": "Choosing 2 people from a group of 5 can be done in 10 different ways.",
      "proof": "To prove that choosing 2 people from a group of 5 can be done in 10 different ways, we use the concept of combinations. The number of ways to choose $r$ objects from a set of $n$ objects is given by the binomial coefficient:\n\n\\[\n\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n\\]\n\nIn this case, we want to choose 2 people from a group of 5, so $n = 5$ and $r = 2$. Plugging these values into the formula, we get:\n\n\\[\n\\binom{5}{2} = \\frac{5!}{2!(5-2)!} = \\frac{5!}{2! \\cdot 3!}\n\\]\n\nCalculating the factorials, we have:\n\n- $5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120$\n- $2! = 2 \\times 1 = 2$\n- $3! = 3 \\times 2 \\times 1 = 6$\n\nSubstituting these into the formula gives:\n\n\\[\n\\binom{5}{2} = \\frac{120}{2 \\times 6} = \\frac{120}{12} = 10\n\\]\n\nTherefore, there are 10 different ways to choose 2 people from a group of 5."
    },
    {
      "id": "6",
      "statement": "If a group has 0 objects, there is only 1 way to choose a subset, which is to choose none at all.",
      "proof": "To prove this statement, we need to show that if a set has 0 elements, there is exactly 1 subset of this set, which is the empty set itself.\n\nLet $S$ be a set with 0 elements, i.e., $S = \\emptyset$. The power set of $S$, denoted by $\\mathcal{P}(S)$, is the set of all subsets of $S$. \n\nBy definition, the power set of any set $S$ with $n$ elements has $2^n$ subsets. In this case, since $S$ has 0 elements, we have $n = 0$. Therefore, the number of subsets of $S$ is $2^0 = 1$.\n\nThe only subset of the empty set $\\emptyset$ is the empty set itself. Thus, $\\mathcal{P}(\\emptyset) = \\{\\emptyset\\}$.\n\nTherefore, if a group has 0 objects, there is exactly 1 way to choose a subset, which is to choose none at all."
    },
    {
      "id": "7",
      "statement": "The number of ways to choose and arrange 3 letters out of the alphabet is larger than the number of ways to just choose 3 letters.",
      "proof": "To prove this statement, we need to compare the number of ways to choose and arrange 3 letters from the alphabet with the number of ways to just choose 3 letters.\n\n1. **Choosing and Arranging 3 Letters:**\n\n   The number of ways to choose and arrange 3 letters from the alphabet is given by the permutation formula. The alphabet has 26 letters, so the number of permutations of 3 letters is:\n\n   \\[\n   P(26, 3) = \\frac{26!}{(26-3)!} = \\frac{26!}{23!} = 26 \\times 25 \\times 24\n   \\]\n\n   Calculating this gives:\n\n   \\[\n   26 \\times 25 \\times 24 = 15600\n   \\]\n\n2. **Choosing 3 Letters:**\n\n   The number of ways to choose 3 letters from the alphabet is given by the combination formula. The number of combinations of 3 letters is:\n\n   \\[\n   C(26, 3) = \\binom{26}{3} = \\frac{26!}{3!(26-3)!} = \\frac{26 \\times 25 \\times 24}{3 \\times 2 \\times 1}\n   \\]\n\n   Calculating this gives:\n\n   \\[\n   \\frac{26 \\times 25 \\times 24}{6} = 2600\n   \\]\n\n3. **Comparison:**\n\n   We have found that the number of ways to choose and arrange 3 letters is 15600, while the number of ways to just choose 3 letters is 2600. Clearly, 15600 is greater than 2600.\n\nThus, the number of ways to choose and arrange 3 letters out of the alphabet is indeed larger than the number of ways to just choose 3 letters."
    },
    {
      "id": "8",
      "statement": "For any number \\(n\\), the number of combinations of \\(n\\) items taken 1 at a time is equal to \\(n\\).",
      "proof": "To prove that the number of combinations of \\(n\\) items taken 1 at a time is equal to \\(n\\), we need to evaluate the binomial coefficient \\(\\binom{n}{1}\\).\n\nThe binomial coefficient \\(\\binom{n}{k}\\) is defined as:\n\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\nwhere \\(n!\\) (n factorial) is the product of all positive integers up to \\(n\\).\n\nFor the case where \\(k = 1\\), the formula becomes:\n\n\\[\n\\binom{n}{1} = \\frac{n!}{1!(n-1)!}\n\\]\n\nSimplifying this expression, we have:\n\n\\[\n\\binom{n}{1} = \\frac{n \\cdot (n-1)!}{1 \\cdot (n-1)!}\n\\]\n\nThe \\((n-1)!\\) terms in the numerator and the denominator cancel out, leaving:\n\n\\[\n\\binom{n}{1} = \\frac{n}{1} = n\n\\]\n\nThus, the number of combinations of \\(n\\) items taken 1 at a time is indeed \\(n\\). This makes intuitive sense because choosing 1 item from \\(n\\) items can be done in exactly \\(n\\) different ways, corresponding to each individual item being chosen."
    },
    {
      "id": "9",
      "statement": "The number of ways to arrange 0 objects is 1 (the empty arrangement).",
      "proof": "To prove that the number of ways to arrange 0 objects is 1, we can use the concept of permutations.\n\nThe number of permutations of $n$ distinct objects is given by $n!$ (n factorial), which is the product of all positive integers up to $n$. Specifically, $n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 1$.\n\nFor $n = 0$, the factorial is defined as:\n\n\\[ 0! = 1 \\]\n\nThis definition is consistent with the combinatorial interpretation. When there are 0 objects, there is exactly one way to arrange them: the empty arrangement. This is analogous to the concept of the empty set in set theory, which has exactly one subset, namely itself.\n\nThus, the number of ways to arrange 0 objects is indeed 1, as given by the definition of $0!$."
    },
    {
      "id": "10",
      "statement": "The number of ways to arrange 4 distinct books on a shelf is \\(4!\\) which equals 24.",
      "proof": "To determine the number of ways to arrange 4 distinct books on a shelf, we need to calculate the number of permutations of 4 distinct objects. \n\nThe formula for the number of permutations of \\( n \\) distinct objects is given by \\( n! \\) (read as \"n factorial\"), which is the product of all positive integers up to \\( n \\).\n\nFor 4 distinct books, we have:\n\n\\[\n4! = 4 \\times 3 \\times 2 \\times 1\n\\]\n\nCalculating this step-by-step:\n\n- First, multiply 4 by 3: \n  \\[\n  4 \\times 3 = 12\n  \\]\n\n- Next, multiply the result by 2:\n  \\[\n  12 \\times 2 = 24\n  \\]\n\n- Finally, multiply by 1 (which does not change the value):\n  \\[\n  24 \\times 1 = 24\n  \\]\n\nThus, the number of ways to arrange 4 distinct books on a shelf is \\( 4! = 24 \\)."
    },
    {
      "id": "11",
      "statement": "A 10-sided die has 10 possible outcomes for each roll.",
      "proof": "To prove that a 10-sided die has 10 possible outcomes for each roll, we can use the basic principles of combinatorics and probability.\n\nA die is a small cube or polyhedron used in games of chance, and each face of the die represents a possible outcome. In the case of a 10-sided die, it is typically a polyhedron known as a pentagonal trapezohedron, which has 10 faces.\n\n**Proof:**\n\n1. **Definition of a 10-sided die:**\n   A 10-sided die is a fair die with 10 distinct faces, each labeled with a unique number from 1 to 10. This means that each face corresponds to one of the numbers in the set $\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$.\n\n2. **Possible outcomes:**\n   When you roll a 10-sided die, the outcome is determined by which face lands facing upwards. Since there are 10 distinct faces, there are 10 possible outcomes, each corresponding to one of the numbers on the faces.\n\n3. **Uniform probability:**\n   Assuming the die is fair, each face has an equal probability of landing face up. Therefore, the probability of any specific outcome (e.g., rolling a 3) is $\\frac{1}{10}$.\n\n4. **Conclusion:**\n   Since there are 10 distinct faces and each face represents a unique outcome, a 10-sided die indeed has 10 possible outcomes for each roll.\n\nThus, by the definition of a 10-sided die and the uniform distribution of outcomes, we conclude that a 10-sided die has 10 possible outcomes for each roll."
    },
    {
      "id": "12",
      "statement": "A deck of 52 playing cards allows for 52 distinct single card draws.",
      "proof": "To prove that a deck of 52 playing cards allows for 52 distinct single card draws, we can use basic principles of combinatorics.\n\nA standard deck of playing cards consists of 52 distinct cards. Each card is unique, characterized by its suit (clubs, diamonds, hearts, spades) and its rank (Ace, 2, 3, ..., 10, Jack, Queen, King). \n\nWhen drawing a single card from the deck, each card represents a unique outcome. Therefore, the number of distinct outcomes when drawing one card is equal to the number of distinct cards in the deck.\n\nTo determine the number of distinct single card draws, we simply count the number of cards in the deck. Since there are 52 cards, there are 52 distinct single card draws possible.\n\nThus, the number of distinct single card draws from a deck of 52 playing cards is 52. This can be expressed mathematically as:\n\n\\[\n\\text{Number of distinct single card draws} = \\binom{52}{1} = 52\n\\]\n\nThis confirms that there are 52 distinct single card draws possible from a standard deck of 52 playing cards."
    },
    {
      "id": "13",
      "statement": "Choosing no items from a set of 6 items can be done in exactly 1 way.",
      "proof": "To prove that choosing no items from a set of 6 items can be done in exactly 1 way, we can use the concept of binomial coefficients.\n\nThe number of ways to choose $k$ items from a set of $n$ items is given by the binomial coefficient $\\binom{n}{k}$. In this case, we want to choose 0 items from a set of 6 items, so we are interested in $\\binom{6}{0}$.\n\nThe binomial coefficient is defined as:\n\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\nSubstituting $n = 6$ and $k = 0$, we have:\n\n\\[\n\\binom{6}{0} = \\frac{6!}{0!(6-0)!} = \\frac{6!}{0! \\cdot 6!}\n\\]\n\nSince $0! = 1$, this simplifies to:\n\n\\[\n\\binom{6}{0} = \\frac{6!}{1 \\cdot 6!} = \\frac{6!}{6!} = 1\n\\]\n\nTherefore, there is exactly 1 way to choose no items from a set of 6 items. This result is consistent with the combinatorial interpretation that the empty subset is the only subset containing no elements."
    },
    {
      "id": "14",
      "statement": "A committee of 3 people can be chosen from a pool of 7 people in 35 different ways.",
      "proof": "To prove that a committee of 3 people can be chosen from a pool of 7 people in 35 different ways, we use the concept of combinations. The number of ways to choose $r$ objects from a set of $n$ objects is given by the binomial coefficient:\n\n\\[\n\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n\\]\n\nIn this case, we have $n = 7$ and $r = 3$. Therefore, the number of ways to choose 3 people from 7 is:\n\n\\[\n\\binom{7}{3} = \\frac{7!}{3!(7-3)!} = \\frac{7!}{3! \\cdot 4!}\n\\]\n\nCalculating the factorials, we have:\n\n- $7! = 7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1 = 5040$\n- $3! = 3 \\times 2 \\times 1 = 6$\n- $4! = 4 \\times 3 \\times 2 \\times 1 = 24$\n\nSubstituting these values into the formula, we get:\n\n\\[\n\\binom{7}{3} = \\frac{5040}{6 \\times 24} = \\frac{5040}{144} = 35\n\\]\n\nThus, a committee of 3 people can indeed be chosen from a pool of 7 people in 35 different ways."
    },
    {
      "id": "15",
      "statement": "The sum of the number of permutations of \\(n\\) objects taken \\(k\\) at a time for all \\(k\\) from 0 to \\(n\\) gives \\(n!\\) multiplied by 2.",
      "proof": "To prove this statement, we need to show that:\n\n\\[\n\\sum_{k=0}^{n} P(n, k) = 2 \\cdot n!\n\\]\n\nwhere \\( P(n, k) \\) is the number of permutations of \\( n \\) objects taken \\( k \\) at a time, given by:\n\n\\[\nP(n, k) = \\frac{n!}{(n-k)!}\n\\]\n\nThe left-hand side of the equation becomes:\n\n\\[\n\\sum_{k=0}^{n} \\frac{n!}{(n-k)!}\n\\]\n\nWe can interpret this sum combinatorially. Consider a set of \\( n \\) distinct objects. For each subset of these objects, we can permute the elements of the subset in a number of ways equal to the factorial of the size of the subset. The sum of permutations over all possible subset sizes is equivalent to considering all possible selections and arrangements of the \\( n \\) objects.\n\nTo see why this sum equals \\( 2 \\cdot n! \\), consider the following reasoning:\n\n1. Each object can either be included in a permutation or not. Thus, for each of the \\( n \\) objects, there are 2 choices: include it or exclude it. This gives \\( 2^n \\) total combinations of inclusion/exclusion.\n\n2. However, this count includes the empty set, which contributes 1 to the total count but does not contribute any permutations. Therefore, the number of non-empty subsets is \\( 2^n - 1 \\).\n\n3. For each non-empty subset, the number of permutations is the factorial of the size of the subset. The sum of permutations over all non-empty subsets is:\n\n\\[\n\\sum_{k=1}^{n} \\frac{n!}{(n-k)!} = n! \\left( \\sum_{k=1}^{n} \\frac{1}{(n-k)!} \\right)\n\\]\n\n4. The sum of permutations over all subsets, including the empty set, is:\n\n\\[\n\\sum_{k=0}^{n} \\frac{n!}{(n-k)!} = n! \\left( \\sum_{k=0}^{n} \\frac{1}{(n-k)!} \\right)\n\\]\n\n5. The sum \\( \\sum_{k=0}^{n} \\frac{1}{(n-k)!} \\) is equal to \\( 2 \\) because it represents the expansion of \\( (1+1)^n = 2^n \\) in terms of permutations, which includes all possible arrangements of the \\( n \\) objects.\n\nThus, the sum of the number of permutations of \\( n \\) objects taken \\( k \\) at a time for all \\( k \\) from 0 to \\( n \\) is:\n\n\\[\n\\sum_{k=0}^{n} P(n, k) = 2 \\cdot n!\n\\]\n\nThis completes the proof."
    }
  ],
  "Logic": [
    {
      "id": "1",
      "statement": "If \"All cats are mammals\" and \"Tom is a cat,\" then Tom is a mammal.",
      "proof": "To prove the statement \"If 'All cats are mammals' and 'Tom is a cat,' then Tom is a mammal,\" we can use a simple logical argument based on the rules of syllogism.\n\n1. **Premise 1**: All cats are mammals.\n2. **Premise 2**: Tom is a cat.\n\nWe want to conclude that Tom is a mammal.\n\nThe argument can be structured as follows:\n\n- From Premise 1, we know that the set of all cats is a subset of the set of all mammals. This can be expressed in logical terms as: \n  \\[\n  \\forall x \\, (C(x) \\rightarrow M(x))\n  \\]\n  where $C(x)$ denotes \"x is a cat\" and $M(x)$ denotes \"x is a mammal.\"\n\n- From Premise 2, we have a specific instance: Tom is a cat. This can be expressed as:\n  \\[\n  C(\\text{Tom})\n  \\]\n\n- By applying the rule of universal instantiation to Premise 1, we can substitute Tom for $x$ in the universal statement, yielding:\n  \\[\n  C(\\text{Tom}) \\rightarrow M(\\text{Tom})\n  \\]\n\n- Since we have $C(\\text{Tom})$ from Premise 2, we can apply modus ponens, which states that if $P \\rightarrow Q$ and $P$ are both true, then $Q$ must be true. Therefore, we conclude:\n  \\[\n  M(\\text{Tom})\n  \\]\n\nThus, Tom is a mammal. This completes the proof using basic principles of logic."
    },
    {
      "id": "2",
      "statement": "A statement and its negation cannot both be true at the same time.",
      "proof": "The statement we want to prove is a fundamental principle of classical logic known as the Law of Non-Contradiction. It asserts that a statement and its negation cannot both be true simultaneously. Let's denote a statement by $P$. The negation of $P$ is denoted by $\\neg P$. The Law of Non-Contradiction can be formally expressed as:\n\n\\[ \\neg (P \\land \\neg P) \\]\n\nTo prove this, we can use a truth table to show that $P \\land \\neg P$ is always false, regardless of the truth value of $P$.\n\n| $P$ | $\\neg P$ | $P \\land \\neg P$ |\n|-----|---------|------------------|\n|  T  |    F    |        F         |\n|  F  |    T    |        F         |\n\nFrom the truth table, we observe that the conjunction $P \\land \\neg P$ is false in all possible cases. Therefore, its negation $\\neg (P \\land \\neg P)$ is true in all cases, confirming that a statement and its negation cannot both be true at the same time.\n\nThis proof relies on the assumption of classical logic, where the Law of Excluded Middle and the Law of Non-Contradiction hold. In other logical systems, such as paraconsistent logic, these laws may not apply in the same way. However, within the framework of classical logic, the proof is valid and complete."
    },
    {
      "id": "3",
      "statement": "If \"It is raining,\" implies \"The ground is wet,\" and it is raining, then the ground is wet.",
      "proof": "To prove this statement, we can use the rules of propositional logic, specifically modus ponens.\n\nLet $P$ be the proposition \"It is raining,\" and let $Q$ be the proposition \"The ground is wet.\" We are given the implication $P \\rightarrow Q$, which reads as \"If it is raining, then the ground is wet.\"\n\nWe are also given that $P$ is true, i.e., \"It is raining.\"\n\nAccording to the rule of modus ponens, if $P \\rightarrow Q$ is true and $P$ is true, then $Q$ must also be true.\n\n1. $P \\rightarrow Q$ (If it is raining, then the ground is wet) - Given\n2. $P$ (It is raining) - Given\n\nBy modus ponens, from 1 and 2, we can conclude:\n\n3. $Q$ (The ground is wet)\n\nThus, if \"It is raining,\" implies \"The ground is wet,\" and it is raining, then the ground is indeed wet."
    },
    {
      "id": "4",
      "statement": "If it is true that \"All dogs bark\" and \"Buddy is a dog,\" then it must be true that Buddy barks.",
      "proof": "To prove the statement \"If it is true that 'All dogs bark' and 'Buddy is a dog,' then it must be true that Buddy barks,\" we can use a simple logical argument based on the rules of propositional logic.\n\nLet's define the following propositions:\n\n- Let $P(x)$ be the predicate \"x is a dog.\"\n- Let $Q(x)$ be the predicate \"x barks.\"\n\nThe statement \"All dogs bark\" can be expressed in logical form as:\n\n$$ \\forall x (P(x) \\rightarrow Q(x)) $$\n\nThis means that for every object $x$, if $x$ is a dog, then $x$ barks.\n\nThe statement \"Buddy is a dog\" can be expressed as:\n\n$$ P(\\text{Buddy}) $$\n\nWe want to prove that \"Buddy barks,\" which can be expressed as:\n\n$$ Q(\\text{Buddy}) $$\n\nTo prove this, we use the rule of universal instantiation, which allows us to apply the universal statement to a specific instance. Since we know $\\forall x (P(x) \\rightarrow Q(x))$ is true, we can instantiate it for the specific case of Buddy:\n\n$$ P(\\text{Buddy}) \\rightarrow Q(\\text{Buddy}) $$\n\nWe are given that $P(\\text{Buddy})$ is true (Buddy is a dog). By modus ponens, which states that if $P \\rightarrow Q$ is true and $P$ is true, then $Q$ must be true, we can conclude:\n\n$$ Q(\\text{Buddy}) $$\n\nTherefore, it must be true that Buddy barks. This completes the proof."
    },
    {
      "id": "5",
      "statement": "A proposition is either true or false, but not both.",
      "proof": "The statement \"A proposition is either true or false, but not both\" is a fundamental principle in classical logic known as the Law of Excluded Middle and the Law of Non-Contradiction. Let's provide a proof based on these principles.\n\n### Proof:\n\n1. **Definitions:**\n   - A *proposition* is a declarative statement that is either true or false.\n   - The *Law of Excluded Middle* states that for any proposition $P$, either $P$ is true or its negation $\\neg P$ is true. Symbolically, this is expressed as $P \\lor \\neg P$.\n   - The *Law of Non-Contradiction* states that a proposition cannot be both true and false at the same time. Symbolically, this is expressed as $\\neg (P \\land \\neg P)$.\n\n2. **Proof by Contradiction:**\n   - Assume, for the sake of contradiction, that there exists a proposition $P$ that is both true and false.\n   - If $P$ is true, then by the definition of a proposition, $\\neg P$ must be false.\n   - If $P$ is false, then $\\neg P$ must be true.\n   - Thus, assuming $P$ is both true and false leads to $P \\land \\neg P$, which contradicts the Law of Non-Contradiction $\\neg (P \\land \\neg P)$.\n   - Therefore, our assumption that a proposition can be both true and false must be incorrect.\n\n3. **Conclusion:**\n   - By the Law of Excluded Middle, for any proposition $P$, either $P$ is true or $\\neg P$ is true.\n   - By the Law of Non-Contradiction, $P$ cannot be both true and false.\n   - Hence, a proposition is either true or false, but not both.\n\nThis proof relies on the foundational principles of classical logic, which are widely accepted in mathematical reasoning."
    },
    {
      "id": "6",
      "statement": "If \"If I study, then I will pass the exam\" is true, and I study, then I will pass the exam.",
      "proof": "To prove this statement, we will use the rules of propositional logic, specifically modus ponens.\n\nLet $P$ represent the statement \"I study\" and $Q$ represent the statement \"I will pass the exam.\" The given statement \"If I study, then I will pass the exam\" can be expressed in logical form as the implication $P \\rightarrow Q$.\n\nWe are given two premises:\n\n1. $P \\rightarrow Q$ (If I study, then I will pass the exam)\n2. $P$ (I study)\n\nWe want to prove $Q$ (I will pass the exam).\n\nAccording to the rule of modus ponens, if we have an implication $P \\rightarrow Q$ and the antecedent $P$ is true, then the consequent $Q$ must also be true. \n\nApplying modus ponens to our premises:\n\n- From $P \\rightarrow Q$ and $P$, we can conclude $Q$.\n\nTherefore, if \"If I study, then I will pass the exam\" is true, and I study, then I will pass the exam."
    },
    {
      "id": "7",
      "statement": "If a statement is true, its double negation is also true.",
      "proof": "To prove that if a statement is true, its double negation is also true, we can use the principles of classical logic.\n\nLet $P$ be a statement. We want to show that if $P$ is true, then $\\neg(\\neg P)$ is also true.\n\n1. **Assumption**: Assume $P$ is true.\n\n2. **Negation**: The negation of $P$, denoted $\\neg P$, is false because $P$ is true.\n\n3. **Double Negation**: The negation of $\\neg P$, denoted $\\neg(\\neg P)$, is the negation of a false statement. In classical logic, the negation of a false statement is true.\n\nTherefore, if $P$ is true, then $\\neg(\\neg P)$ is also true.\n\nThis is a fundamental principle in classical logic known as the law of double negation, which states that a statement is logically equivalent to its double negation:\n\n$$ P \\equiv \\neg(\\neg P) $$\n\nThis completes the proof."
    },
    {
      "id": "8",
      "statement": "If \"Birds can fly\" is true, and \"Penguins are birds\" is true, then \"Penguins can fly\" is not necessarily true.",
      "proof": "To prove this statement, we need to analyze the logical structure of the given premises and conclusion.\n\n1. **Premise 1**: \"Birds can fly\" is true.  \n   This can be interpreted as a general statement about birds, suggesting that typically, birds have the ability to fly. In logical terms, this can be expressed as:  \n   \\[\n   \\forall x \\, (B(x) \\rightarrow F(x))\n   \\]\n   where $B(x)$ means \"x is a bird\" and $F(x)$ means \"x can fly\".\n\n2. **Premise 2**: \"Penguins are birds\" is true.  \n   This can be expressed as:  \n   \\[\n   \\forall x \\, (P(x) \\rightarrow B(x))\n   \\]\n   where $P(x)$ means \"x is a penguin\".\n\n3. **Conclusion**: \"Penguins can fly\" is not necessarily true.  \n   We need to show that the conclusion $P(x) \\rightarrow F(x)$ does not logically follow from the premises.\n\n### Proof:\n\nTo demonstrate that the conclusion does not necessarily follow, we can provide a counterexample. \n\n- Consider the specific case of penguins. While it is true that penguins are birds (Premise 2), it is also known that penguins are a special type of bird that cannot fly. This is an exception to the general rule stated in Premise 1.\n\n- The statement \"Birds can fly\" is a generalization and does not account for exceptions. It does not imply that every single bird can fly, only that it is a common characteristic among birds.\n\n- Therefore, the existence of non-flying birds like penguins shows that the conclusion \"Penguins can fly\" does not necessarily follow from the premises.\n\nIn logical terms, the premises do not guarantee the truth of the conclusion because the implication $\\forall x \\, (B(x) \\rightarrow F(x))$ allows for exceptions. Thus, the statement \"Penguins can fly\" is not necessarily true, even though the premises are true. \n\nThis demonstrates that the argument is not valid, as the conclusion does not logically follow from the premises."
    },
    {
      "id": "9",
      "statement": "For any two propositions, either they are both true, both false, or one is true and the other is false.",
      "proof": "To prove this statement, we need to consider the possible truth values of two propositions, say $P$ and $Q$. A proposition is a statement that can either be true or false, but not both.\n\nThere are four possible combinations of truth values for the propositions $P$ and $Q$:\n\n1. **Both $P$ and $Q$ are true**: In this case, $P$ is true and $Q$ is true.\n\n2. **Both $P$ and $Q$ are false**: In this case, $P$ is false and $Q$ is false.\n\n3. **$P$ is true and $Q$ is false**: In this case, $P$ is true and $Q$ is false.\n\n4. **$P$ is false and $Q$ is true**: In this case, $P$ is false and $Q$ is true.\n\nThese four cases exhaust all possible combinations of truth values for the propositions $P$ and $Q$. \n\n- The first case corresponds to \"both are true.\"\n- The second case corresponds to \"both are false.\"\n- The third and fourth cases correspond to \"one is true and the other is false.\"\n\nThus, for any two propositions, either they are both true, both false, or one is true and the other is false. This covers all possible scenarios for the truth values of two propositions."
    },
    {
      "id": "10",
      "statement": "If \"If it is Sunday, then it is a holiday\" is true, then when it is not Sunday, the statement does not necessarily tell us whether it is a holiday.",
      "proof": "To prove this statement, we need to analyze the logical structure of the given implication: \"If it is Sunday, then it is a holiday.\" This can be expressed in logical terms as $P \\rightarrow Q$, where $P$ is \"It is Sunday\" and $Q$ is \"It is a holiday.\"\n\nThe truth table for the implication $P \\rightarrow Q$ is as follows:\n\n\\[\n\\begin{array}{c|c|c}\nP & Q & P \\rightarrow Q \\\\\n\\hline\n\\text{True} & \\text{True} & \\text{True} \\\\\n\\text{True} & \\text{False} & \\text{False} \\\\\n\\text{False} & \\text{True} & \\text{True} \\\\\n\\text{False} & \\text{False} & \\text{True} \\\\\n\\end{array}\n\\]\n\nThe implication $P \\rightarrow Q$ is false only when $P$ is true and $Q$ is false. In all other cases, the implication is true.\n\nNow, consider the case when it is not Sunday, i.e., $P$ is false. According to the truth table, when $P$ is false, the implication $P \\rightarrow Q$ is true regardless of the truth value of $Q$. This means that when it is not Sunday, the statement \"If it is Sunday, then it is a holiday\" does not provide any information about whether it is a holiday or not. The truth of the implication is maintained whether it is a holiday (when $Q$ is true) or not a holiday (when $Q$ is false).\n\nTherefore, when it is not Sunday, the statement does not necessarily tell us whether it is a holiday. This is consistent with the logical nature of implications, where the truth of the antecedent being false does not affect the truth of the implication."
    },
    {
      "id": "11",
      "statement": "The converse of an implication \"If P, then Q\" is \"If Q, then P.\"",
      "proof": "To prove that the converse of an implication \"If \\( P \\), then \\( Q \\)\" is \"If \\( Q \\), then \\( P \\),\" we need to understand the definitions of an implication and its converse.\n\n1. **Implication**: The statement \"If \\( P \\), then \\( Q \\)\" is denoted as \\( P \\implies Q \\). This means that whenever \\( P \\) is true, \\( Q \\) must also be true.\n\n2. **Converse**: The converse of the implication \\( P \\implies Q \\) is the statement \"If \\( Q \\), then \\( P \\),\" denoted as \\( Q \\implies P \\).\n\nTo show that the converse of \"If \\( P \\), then \\( Q \\)\" is indeed \"If \\( Q \\), then \\( P \\),\" we simply need to verify the definition of a converse:\n\n- The original implication \\( P \\implies Q \\) asserts that \\( Q \\) is a consequence of \\( P \\).\n- The converse \\( Q \\implies P \\) asserts that \\( P \\) is a consequence of \\( Q \\).\n\nThus, by definition, the converse of an implication \"If \\( P \\), then \\( Q \\)\" is indeed \"If \\( Q \\), then \\( P \\).\" \n\nNote that the truth of the converse is independent of the truth of the original implication. In general, \\( P \\implies Q \\) and \\( Q \\implies P \\) can have different truth values."
    },
    {
      "id": "12",
      "statement": "The statement \"It is not the case that I do not like ice cream\" implies \"I like ice cream.\"",
      "proof": "To prove the statement \"It is not the case that I do not like ice cream\" implies \"I like ice cream,\" we can use basic principles of propositional logic, specifically the law of double negation.\n\nLet $P$ be the proposition \"I like ice cream.\" The negation of $P$ is $\\neg P$, which means \"I do not like ice cream.\" The statement \"It is not the case that I do not like ice cream\" can be represented as $\\neg (\\neg P)$.\n\nAccording to the law of double negation in logic, $\\neg (\\neg P)$ is logically equivalent to $P$. Therefore, the statement \"It is not the case that I do not like ice cream\" implies \"I like ice cream.\"\n\nIn formal logical terms, we have:\n\n1. Let $P$: \"I like ice cream.\"\n2. The negation of $P$ is $\\neg P$: \"I do not like ice cream.\"\n3. The statement \"It is not the case that I do not like ice cream\" is $\\neg (\\neg P)$.\n4. By the law of double negation, $\\neg (\\neg P) \\equiv P$.\n\nThus, $\\neg (\\neg P)$ implies $P$, which means \"It is not the case that I do not like ice cream\" implies \"I like ice cream.\""
    },
    {
      "id": "13",
      "statement": "If either logic statement P or Q is true, and P is known to be false, then Q must be true.",
      "proof": "To prove this statement, we will use the properties of logical disjunction (the \"or\" operator) and the concept of logical implication.\n\nThe logical statement \"either $P$ or $Q$ is true\" can be expressed using the logical disjunction as $P \\lor Q$. The truth table for the disjunction $P \\lor Q$ is as follows:\n\n\\[\n\\begin{array}{c|c|c}\nP & Q & P \\lor Q \\\\\n\\hline\n\\text{True} & \\text{True} & \\text{True} \\\\\n\\text{True} & \\text{False} & \\text{True} \\\\\n\\text{False} & \\text{True} & \\text{True} \\\\\n\\text{False} & \\text{False} & \\text{False} \\\\\n\\end{array}\n\\]\n\nAccording to the problem, we know that $P \\lor Q$ is true, and $P$ is false. We need to show that $Q$ must be true.\n\nFrom the truth table, we see that if $P \\lor Q$ is true and $P$ is false, the only remaining possibility is the third row, where $Q$ is true. Therefore, if $P$ is false and $P \\lor Q$ is true, then $Q$ must be true.\n\nThus, we have shown that if either $P$ or $Q$ is true, and $P$ is known to be false, then $Q$ must be true."
    },
    {
      "id": "14",
      "statement": "If \"If it rains, the match will be canceled,\" is false when it is raining and the match is not canceled.",
      "proof": "To prove the statement \"If it rains, the match will be canceled\" is false when it is raining and the match is not canceled, we need to understand the logical structure of an implication.\n\nThe statement \"If it rains, the match will be canceled\" can be represented in logical form as $P \\rightarrow Q$, where:\n- $P$: \"It rains.\"\n- $Q$: \"The match will be canceled.\"\n\nIn logic, an implication $P \\rightarrow Q$ is false if and only if $P$ is true and $Q$ is false. This is the only scenario where the implication does not hold.\n\nLet's analyze the given situation:\n- It is raining, which means $P$ is true.\n- The match is not canceled, which means $Q$ is false.\n\nSince $P$ is true and $Q$ is false, the implication $P \\rightarrow Q$ is false. Therefore, the statement \"If it rains, the match will be canceled\" is indeed false in this scenario. \n\nThis aligns with the truth table for implication, where the only false case for $P \\rightarrow Q$ is when $P$ is true and $Q$ is false."
    },
    {
      "id": "15",
      "statement": "Two logical statements that are equivalent will have the same truth value in every possible scenario.",
      "proof": "To prove that two logical statements that are equivalent will have the same truth value in every possible scenario, we need to understand what it means for two statements to be equivalent.\n\nTwo logical statements $P$ and $Q$ are said to be equivalent, denoted as $P \\equiv Q$, if and only if $P \\leftrightarrow Q$ is a tautology. A tautology is a statement that is true in every possible scenario.\n\n**Proof:**\n\n1. **Definition of Logical Equivalence:**\n   Two statements $P$ and $Q$ are equivalent if $P \\leftrightarrow Q$ is a tautology. This means that $P \\leftrightarrow Q$ is true for all possible truth values of the variables involved.\n\n2. **Truth Table Analysis:**\n   Consider the truth table for $P \\leftrightarrow Q$:\n\n   \\[\n   \\begin{array}{|c|c|c|}\n   \\hline\n   P & Q & P \\leftrightarrow Q \\\\\n   \\hline\n   T & T & T \\\\\n   T & F & F \\\\\n   F & T & F \\\\\n   F & F & T \\\\\n   \\hline\n   \\end{array}\n   \\]\n\n   For $P \\leftrightarrow Q$ to be a tautology, it must be true in all rows of the truth table. This occurs only when $P$ and $Q$ have the same truth value in every possible scenario.\n\n3. **Conclusion:**\n   Since $P \\leftrightarrow Q$ is a tautology, $P$ and $Q$ must have the same truth value in every possible scenario. Therefore, if $P \\equiv Q$, then $P$ and $Q$ will have the same truth value in every possible scenario.\n\nThus, we have shown that two logical statements that are equivalent will indeed have the same truth value in every possible scenario."
    }
  ],
  "Set Theory": [
    {
      "id": "1",
      "statement": "The union of a set with the empty set is the set itself.",
      "proof": "To prove that the union of a set with the empty set is the set itself, we need to show that for any set $A$, $A \\cup \\emptyset = A$.\n\n**Proof:**\n\n1. **Show that $A \\cup \\emptyset \\subseteq A$:**\n\n   Let $x \\in A \\cup \\emptyset$. By the definition of union, $x \\in A \\cup \\emptyset$ means that $x \\in A$ or $x \\in \\emptyset$. However, since the empty set $\\emptyset$ contains no elements, $x \\in \\emptyset$ is never true. Therefore, the only possibility is $x \\in A$. Thus, $A \\cup \\emptyset \\subseteq A$.\n\n2. **Show that $A \\subseteq A \\cup \\emptyset$:**\n\n   Let $x \\in A$. By the definition of union, if $x \\in A$, then $x \\in A \\cup \\emptyset$. Therefore, $A \\subseteq A \\cup \\emptyset$.\n\nSince we have shown both $A \\cup \\emptyset \\subseteq A$ and $A \\subseteq A \\cup \\emptyset$, by the definition of set equality, we conclude that $A \\cup \\emptyset = A$.\n\nThus, the union of a set with the empty set is the set itself."
    },
    {
      "id": "2",
      "statement": "The intersection of a set with itself is the set itself.",
      "proof": "To prove that the intersection of a set with itself is the set itself, let's consider a set $A$. We want to show that $A \\cap A = A$.\n\n**Proof:**\n\n1. **Show that $A \\cap A \\subseteq A$:**\n\n   By definition of intersection, $A \\cap A = \\{ x \\mid x \\in A \\text{ and } x \\in A \\}$. Clearly, if $x \\in A \\cap A$, then $x \\in A$. Therefore, $A \\cap A \\subseteq A$.\n\n2. **Show that $A \\subseteq A \\cap A$:**\n\n   For any element $x \\in A$, it is trivially true that $x \\in A$ and $x \\in A$. Therefore, $x \\in A \\cap A$. Thus, $A \\subseteq A \\cap A$.\n\nSince we have shown both $A \\cap A \\subseteq A$ and $A \\subseteq A \\cap A$, by the definition of set equality, we conclude that $A \\cap A = A$.\n\nThis completes the proof."
    },
    {
      "id": "3",
      "statement": "An element either belongs to a set or it does not.",
      "proof": "To prove the statement \"An element either belongs to a set or it does not,\" we rely on the fundamental principles of set theory, particularly the axiom of extensionality and the law of excluded middle.\n\nLet $S$ be a set and let $x$ be an arbitrary element. According to the axiom of extensionality in set theory, a set is determined entirely by its elements. This means that for any element $x$, it is either a member of the set $S$ or it is not. \n\nFormally, we express this as:\n\n1. $x \\in S$ (meaning $x$ is an element of $S$), or\n2. $x \\notin S$ (meaning $x$ is not an element of $S$).\n\nThe law of excluded middle, a principle in classical logic, states that for any proposition $P$, either $P$ is true or its negation $\\neg P$ is true. Applying this to our context, for the proposition \"$x \\in S$,\" either \"$x \\in S$\" is true or \"$x \\notin S$\" is true.\n\nTherefore, for any element $x$ and any set $S$, it is necessarily the case that $x$ either belongs to $S$ or does not belong to $S$. This dichotomy is a fundamental aspect of classical set theory and is consistent with the binary nature of membership in a set. \n\nThus, the statement \"An element either belongs to a set or it does not\" is proven by the principles of set theory and classical logic."
    },
    {
      "id": "4",
      "statement": "If set A is a subset of set B, then every element of A is also an element of B.",
      "proof": "To prove the statement \"If set $A$ is a subset of set $B$, then every element of $A$ is also an element of $B$,\" we start by recalling the definition of a subset.\n\nBy definition, a set $A$ is a subset of a set $B$, denoted as $A \\subseteq B$, if and only if every element $x$ in $A$ is also in $B$. \n\nLet's prove this formally:\n\n**Proof:**\n\nAssume $A \\subseteq B$. By the definition of a subset, this means that for every element $x \\in A$, it must be the case that $x \\in B$.\n\nTo show this explicitly, consider an arbitrary element $x \\in A$. Since $A \\subseteq B$, by the definition of subset, it follows directly that $x \\in B$.\n\nSince our choice of $x$ was arbitrary, this argument holds for all elements of $A$. Therefore, every element of $A$ is also an element of $B$.\n\nThus, we have shown that if $A \\subseteq B$, then every element of $A$ is also an element of $B$. This completes the proof."
    },
    {
      "id": "5",
      "statement": "The number of subsets of a finite set with n elements is 2^n.",
      "proof": "To prove that the number of subsets of a finite set with $n$ elements is $2^n$, we can use the concept of binary choices for each element in the set.\n\nLet $S$ be a finite set with $n$ elements. We want to determine the number of subsets of $S$. \n\nConsider each element of $S$. For each element, there are two choices: either the element is included in a subset or it is not included. \n\nSince there are $n$ elements in the set $S$, and each element has 2 independent choices (to be included or not), the total number of different combinations of these choices is given by multiplying the number of choices for each element:\n\n\\[\n2 \\times 2 \\times \\cdots \\times 2 = 2^n\n\\]\n\nThis product has $n$ factors of 2, one for each element in the set. Therefore, the total number of subsets of a set with $n$ elements is $2^n$.\n\nThis can also be understood by considering the binary representation of numbers. Each subset can be represented by a binary string of length $n$, where each bit corresponds to an element of the set: a '1' indicates the element is included in the subset, and a '0' indicates it is not. There are $2^n$ such binary strings, corresponding to the $2^n$ possible subsets.\n\nThus, the number of subsets of a finite set with $n$ elements is indeed $2^n$."
    },
    {
      "id": "6",
      "statement": "The intersection of any set with the empty set is the empty set.",
      "proof": "To prove that the intersection of any set $A$ with the empty set $\\emptyset$ is the empty set, we need to show that $A \\cap \\emptyset = \\emptyset$.\n\n**Proof:**\n\nBy definition, the intersection of two sets $A$ and $B$, denoted $A \\cap B$, is the set of all elements that are common to both $A$ and $B$. Formally, we have:\n\n\\[ A \\cap B = \\{ x \\mid x \\in A \\text{ and } x \\in B \\}. \\]\n\nNow, consider the intersection $A \\cap \\emptyset$:\n\n\\[ A \\cap \\emptyset = \\{ x \\mid x \\in A \\text{ and } x \\in \\emptyset \\}. \\]\n\nSince the empty set $\\emptyset$ contains no elements, there is no element $x$ such that $x \\in \\emptyset$. Therefore, the condition $x \\in A \\text{ and } x \\in \\emptyset$ can never be satisfied for any element $x$. \n\nAs a result, the set of all such $x$ is empty, which means:\n\n\\[ A \\cap \\emptyset = \\emptyset. \\]\n\nThus, we have shown that the intersection of any set $A$ with the empty set $\\emptyset$ is indeed the empty set $\\emptyset$."
    },
    {
      "id": "7",
      "statement": "The union of a set with itself is the set itself.",
      "proof": "To prove that the union of a set with itself is the set itself, we need to show that for any set $A$, the union $A \\cup A = A$.\n\n**Proof:**\n\n1. **Show that $A \\cup A \\subseteq A$:**\n\n   By definition of union, $A \\cup A = \\{ x \\mid x \\in A \\text{ or } x \\in A \\}$. Since every element $x$ that satisfies $x \\in A \\text{ or } x \\in A$ is clearly in $A$, it follows that $A \\cup A \\subseteq A$.\n\n2. **Show that $A \\subseteq A \\cup A$:**\n\n   Again, by the definition of union, $A \\cup A = \\{ x \\mid x \\in A \\text{ or } x \\in A \\}$. For any element $x \\in A$, it is trivially true that $x \\in A \\text{ or } x \\in A$. Therefore, $x \\in A \\cup A$. Hence, $A \\subseteq A \\cup A$.\n\nSince we have shown both $A \\cup A \\subseteq A$ and $A \\subseteq A \\cup A$, by the definition of set equality, we conclude that $A \\cup A = A$.\n\nThus, the union of a set with itself is the set itself."
    },
    {
      "id": "8",
      "statement": "An empty set is a subset of every set.",
      "proof": "To prove that the empty set $\\emptyset$ is a subset of every set $A$, we need to show that for every element $x$, if $x \\in \\emptyset$, then $x \\in A$. \n\nThe definition of a subset is: A set $B$ is a subset of a set $A$ (denoted $B \\subseteq A$) if every element of $B$ is also an element of $A$. \n\nNow, consider the empty set $\\emptyset$. By definition, the empty set has no elements. Therefore, there are no elements $x$ such that $x \\in \\emptyset$. \n\nThe statement \"if $x \\in \\emptyset$, then $x \\in A$\" is vacuously true because the premise $x \\in \\emptyset$ is false for all $x$. In logic, a statement of the form \"if $P$, then $Q$\" is considered true if $P$ is false, regardless of the truth value of $Q$. \n\nSince there are no elements in $\\emptyset$ to contradict the condition of being in $A$, it follows that $\\emptyset \\subseteq A$ for any set $A$. \n\nThus, the empty set is a subset of every set."
    },
    {
      "id": "9",
      "statement": "If a set contains n elements, the set contains n elements.",
      "proof": "The statement \"If a set contains \\( n \\) elements, the set contains \\( n \\) elements\" is tautological and self-evident. However, we can provide a formal proof using the definition of a set and its cardinality.\n\n**Proof:**\n\nLet \\( S \\) be a set, and suppose that the cardinality of \\( S \\), denoted by \\(|S|\\), is \\( n \\). By definition, the cardinality of a set is the number of distinct elements it contains. Therefore, if \\(|S| = n\\), it means that there are exactly \\( n \\) distinct elements in the set \\( S \\).\n\nTo prove the statement, we need to show that if \\(|S| = n\\), then \\( S \\) contains \\( n \\) elements. This is directly from the definition of cardinality:\n\n1. By assumption, \\(|S| = n\\).\n2. By the definition of cardinality, this means there are exactly \\( n \\) distinct elements in \\( S \\).\n\nThus, if a set \\( S \\) contains \\( n \\) elements, then by definition, the set \\( S \\) contains \\( n \\) elements. This completes the proof. \n\nThis statement is a direct consequence of the definition of cardinality and does not require further elaboration beyond the definitions involved."
    },
    {
      "id": "10",
      "statement": "The power set of a set is the set of all subsets of the original set.",
      "proof": "To prove that the power set of a set $S$, denoted $\\mathcal{P}(S)$, is the set of all subsets of $S$, we need to show two things:\n\n1. Every element of $\\mathcal{P}(S)$ is a subset of $S$.\n2. Every subset of $S$ is an element of $\\mathcal{P}(S)$.\n\n**Proof:**\n\n1. **Every element of $\\mathcal{P}(S)$ is a subset of $S$:**\n\n   By definition, the power set $\\mathcal{P}(S)$ is the set of all subsets of $S$. This means that if $T \\in \\mathcal{P}(S)$, then $T$ is a subset of $S$. Therefore, every element of $\\mathcal{P}(S)$ is indeed a subset of $S$.\n\n2. **Every subset of $S$ is an element of $\\mathcal{P}(S)$:**\n\n   Let $T$ be any subset of $S$. By the definition of the power set, $\\mathcal{P}(S)$ includes all subsets of $S$. Therefore, $T$ must be an element of $\\mathcal{P}(S)$.\n\nSince both conditions are satisfied, we conclude that the power set $\\mathcal{P}(S)$ is indeed the set of all subsets of the original set $S$. \n\nIn terms of cardinality, if $S$ has $n$ elements, then the power set $\\mathcal{P}(S)$ has $2^n$ elements. This is because each element of $S$ can either be included in a subset or not, leading to $2^n$ possible combinations (subsets)."
    },
    {
      "id": "11",
      "statement": "The union of two identical sets is the set itself.",
      "proof": "To prove that the union of two identical sets is the set itself, let's consider a set $A$. We want to show that $A \\cup A = A$.\n\n**Proof:**\n\n1. **Show that $A \\cup A \\subseteq A$:**\n\n   By definition of union, $A \\cup A = \\{ x \\mid x \\in A \\text{ or } x \\in A \\}$. Since every element $x$ that satisfies $x \\in A \\text{ or } x \\in A$ is clearly in $A$, it follows that $A \\cup A \\subseteq A$.\n\n2. **Show that $A \\subseteq A \\cup A$:**\n\n   By definition of union, for any element $x \\in A$, it is true that $x \\in A \\cup A$ because $x \\in A$ satisfies the condition $x \\in A \\text{ or } x \\in A$. Therefore, $A \\subseteq A \\cup A$.\n\nSince we have shown both $A \\cup A \\subseteq A$ and $A \\subseteq A \\cup A$, by the definition of set equality, we conclude that $A \\cup A = A$.\n\nThus, the union of two identical sets is the set itself."
    },
    {
      "id": "12",
      "statement": "If a set is finite, it has a finite number of subsets.",
      "proof": "To prove that if a set is finite, it has a finite number of subsets, we can use the concept of the power set and the properties of finite sets.\n\nLet $S$ be a finite set with $n$ elements. We want to show that the number of subsets of $S$ is finite.\n\nThe number of subsets of a set $S$ is given by the size of its power set, denoted as $\\mathcal{P}(S)$. The power set $\\mathcal{P}(S)$ is the set of all subsets of $S$, including the empty set and $S$ itself.\n\nFor a set with $n$ elements, each element can either be included in a subset or not. Therefore, for each element, there are 2 choices: either to include it in a subset or not. Since there are $n$ elements, the total number of subsets is given by the product of choices for each element:\n\n\\[\n|\\mathcal{P}(S)| = 2^n\n\\]\n\nSince $n$ is finite (because $S$ is a finite set), $2^n$ is also finite. Therefore, the number of subsets of a finite set $S$ is finite.\n\nThus, we have shown that if a set is finite, it has a finite number of subsets."
    },
    {
      "id": "13",
      "statement": "The number of elements in a set is called the cardinality of the set.",
      "proof": "In set theory, the concept of cardinality is used to describe the number of elements in a set. Let's provide a formal definition and some properties related to cardinality.\n\n**Definition:**\n\nThe cardinality of a set $A$, denoted by $|A|$, is a measure of the \"number of elements\" in the set. For finite sets, the cardinality is simply the count of distinct elements in the set.\n\n**Proof of Cardinality for Finite Sets:**\n\nTo prove that the cardinality of a finite set $A$ is well-defined, we need to show that there is a bijection between the set $A$ and a set of natural numbers $\\{1, 2, \\ldots, n\\}$ for some non-negative integer $n$.\n\n1. **Existence of Bijection:**\n\n   For a finite set $A$, we can list its elements as $A = \\{a_1, a_2, \\ldots, a_n\\}$. We can define a function $f: \\{1, 2, \\ldots, n\\} \\to A$ by $f(i) = a_i$ for each $i = 1, 2, \\ldots, n$. This function is clearly bijective because:\n\n   - **Injective (One-to-One):** If $f(i) = f(j)$, then $a_i = a_j$, which implies $i = j$ since the elements $a_1, a_2, \\ldots, a_n$ are distinct.\n   - **Surjective (Onto):** For every element $a_i \\in A$, there exists an $i \\in \\{1, 2, \\ldots, n\\}$ such that $f(i) = a_i$.\n\n2. **Uniqueness of Cardinality:**\n\n   Suppose there is another bijection $g: \\{1, 2, \\ldots, m\\} \\to A$. By the properties of bijections, both $f$ and $g$ must have the same number of elements in their domain, which implies $n = m$. Therefore, the cardinality $|A| = n$ is uniquely determined.\n\n**Conclusion:**\n\nFor any finite set $A$, the cardinality $|A|$ is a well-defined non-negative integer that represents the number of elements in the set. For infinite sets, cardinality is defined using different concepts such as countable and uncountable sets, but for finite sets, the above proof suffices to establish the concept of cardinality."
    },
    {
      "id": "14",
      "statement": "If two sets have no elements in common, their intersection is the empty set.",
      "proof": "To prove this statement, we need to show that if two sets $A$ and $B$ have no elements in common, then their intersection $A \\cap B$ is the empty set, denoted by $\\emptyset$.\n\n**Proof:**\n\nBy definition, the intersection of two sets $A$ and $B$, denoted $A \\cap B$, is the set of all elements that are common to both $A$ and $B$. Formally, we can write:\n\n\\[ A \\cap B = \\{ x \\mid x \\in A \\text{ and } x \\in B \\}. \\]\n\nNow, we are given that $A$ and $B$ have no elements in common. This means that there is no element $x$ such that $x \\in A$ and $x \\in B$ simultaneously. In other words, for every element $x$, if $x \\in A$, then $x \\notin B$, and if $x \\in B$, then $x \\notin A$.\n\nSince there is no element $x$ that satisfies the condition $x \\in A$ and $x \\in B$, the set of such elements is empty. Therefore, the intersection of $A$ and $B$ is the empty set:\n\n\\[ A \\cap B = \\emptyset. \\]\n\nThis completes the proof that if two sets have no elements in common, their intersection is the empty set."
    },
    {
      "id": "15",
      "statement": "For any set, the set itself is a subset of it.",
      "proof": "To prove that any set $A$ is a subset of itself, we need to show that every element of $A$ is also an element of $A$. \n\nBy definition, a set $B$ is a subset of a set $A$, denoted $B \\subseteq A$, if for every element $x \\in B$, we have $x \\in A$. \n\nLet's consider the set $A$. We want to show that $A \\subseteq A$. \n\nTake any arbitrary element $x \\in A$. Since $x$ is an element of $A$ by assumption, it follows trivially that $x \\in A$. \n\nSince our choice of $x$ was arbitrary, this argument holds for all elements of $A$. Therefore, every element of $A$ is also an element of $A$, which means $A \\subseteq A$. \n\nThus, we have shown that any set $A$ is a subset of itself."
    }
  ],
  "Graph Theory": [
    {
      "id": "1",
      "statement": "A graph with no edges is called an empty graph.",
      "proof": "In graph theory, an empty graph is a graph that has a set of vertices but no edges connecting any pair of these vertices. Let's provide a formal definition and a brief proof of a property related to empty graphs.\n\n**Definition:**\n\nAn empty graph $G = (V, E)$ is a graph where $V$ is a non-empty set of vertices and $E = \\emptyset$, meaning there are no edges between any pair of vertices in $V$.\n\n**Property:**\n\nAn empty graph with $n$ vertices is $K_n$-free, where $K_n$ is the complete graph on $n$ vertices.\n\n**Proof:**\n\nTo prove that an empty graph with $n$ vertices is $K_n$-free, we need to show that it does not contain a complete subgraph on $n$ vertices.\n\n1. **Definition of $K_n$:**\n   A complete graph $K_n$ is a graph where every pair of distinct vertices is connected by a unique edge. Thus, $K_n$ has $\\binom{n}{2} = \\frac{n(n-1)}{2}$ edges.\n\n2. **Empty Graph:**\n   By definition, an empty graph $G = (V, E)$ has $E = \\emptyset$, meaning it has 0 edges.\n\n3. **Comparison:**\n   Since $G$ has 0 edges, it cannot contain any subgraph with more than 0 edges. In particular, it cannot contain a subgraph with $\\frac{n(n-1)}{2}$ edges, which is required for a complete graph $K_n$.\n\nTherefore, an empty graph with $n$ vertices is $K_n$-free, as it does not contain any complete subgraph on $n$ vertices. This completes the proof."
    },
    {
      "id": "2",
      "statement": "A graph with at least one edge has at least two vertices.",
      "proof": "To prove this statement, we need to understand the basic definitions in graph theory. A graph $G$ is defined as an ordered pair $G = (V, E)$, where $V$ is a set of vertices and $E$ is a set of edges. Each edge is a pair of vertices from $V$.\n\n**Proof:**\n\nAssume for contradiction that there is a graph $G = (V, E)$ with at least one edge and fewer than two vertices.\n\n1. If $G$ has at least one edge, then $E$ is non-empty, i.e., $|E| \\geq 1$.\n2. An edge in a graph is defined as a connection between two vertices. Therefore, for an edge to exist, there must be at least two vertices in the graph.\n3. If $|V| < 2$, then the possible values for $|V|$ are 0 or 1.\n\n   - If $|V| = 0$, there are no vertices, and hence no edges can exist because an edge requires two vertices. This contradicts the assumption that $|E| \\geq 1$.\n   - If $|V| = 1$, there is only one vertex, and again, no edge can exist because an edge requires two distinct vertices. This also contradicts the assumption that $|E| \\geq 1$.\n\nSince both cases lead to a contradiction, our initial assumption that a graph with at least one edge can have fewer than two vertices must be false. Therefore, a graph with at least one edge must have at least two vertices.\n\nThus, we conclude that a graph with at least one edge has at least two vertices."
    },
    {
      "id": "3",
      "statement": "In a complete graph with n vertices, each vertex is connected to all other n-1 vertices.",
      "proof": "To prove that in a complete graph with $n$ vertices, each vertex is connected to all other $n-1$ vertices, we start by defining what a complete graph is.\n\nA complete graph, denoted as $K_n$, is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge. This means that for a graph with $n$ vertices, each vertex must be connected to every other vertex.\n\nLet's consider a vertex $v_i$ in the complete graph $K_n$. Since $K_n$ is complete, $v_i$ must be connected to every other vertex in the graph. The total number of vertices in the graph is $n$, and $v_i$ itself is one of these vertices. Therefore, the number of vertices other than $v_i$ is $n - 1$.\n\nSince $v_i$ is connected to every other vertex, it must have an edge to each of these $n - 1$ vertices. This holds true for any vertex $v_i$ in the graph. Therefore, each vertex in a complete graph with $n$ vertices is connected to all other $n - 1$ vertices.\n\nIn summary, in a complete graph $K_n$, each vertex has a degree of $n - 1$, confirming that each vertex is connected to all other $n - 1$ vertices."
    },
    {
      "id": "4",
      "statement": "A graph is connected if there is a path between any two of its vertices.",
      "proof": "To prove that a graph is connected if and only if there is a path between any two of its vertices, we need to show two implications:\n\n1. If a graph is connected, then there is a path between any two of its vertices.\n2. If there is a path between any two vertices in a graph, then the graph is connected.\n\n**Proof:**\n\n1. **(Connected $\\implies$ Path between any two vertices):**\n\n   By definition, a graph $G = (V, E)$ is connected if for every pair of vertices $u, v \\in V$, there exists a path from $u$ to $v$. This is the direct definition of a connected graph, so this implication is trivially true.\n\n2. **(Path between any two vertices $\\implies$ Connected):**\n\n   Assume that for every pair of vertices $u, v \\in V$, there exists a path from $u$ to $v$. We need to show that the graph is connected.\n\n   Consider any two vertices $u$ and $v$ in the graph. By assumption, there is a path from $u$ to $v$. This means that $u$ and $v$ are in the same connected component of the graph. Since this is true for any arbitrary pair of vertices, it follows that all vertices are in the same connected component. Therefore, the graph is connected.\n\nSince both implications hold, we conclude that a graph is connected if and only if there is a path between any two of its vertices. \n\nThis completes the proof."
    },
    {
      "id": "5",
      "statement": "The degree of a vertex in a simple graph is the number of edges connected to it.",
      "proof": "To prove the statement, we need to understand the definition of a simple graph and the concept of the degree of a vertex.\n\nA **simple graph** is an undirected graph that does not contain multiple edges between any pair of vertices and does not contain any loops (edges connected at both ends to the same vertex).\n\nThe **degree** of a vertex in a graph is defined as the number of edges incident to that vertex. In the context of a simple graph, this definition simplifies to counting the number of distinct edges that have the vertex as one of their endpoints.\n\nLet's denote a simple graph as $G = (V, E)$, where $V$ is the set of vertices and $E$ is the set of edges. For a vertex $v \\in V$, the degree of $v$, denoted as $\\deg(v)$, is the number of edges in $E$ that are connected to $v$.\n\n**Proof:**\n\n1. **Definition of Degree:**\n   - By definition, the degree of a vertex $v$, $\\deg(v)$, is the number of edges that have $v$ as an endpoint.\n\n2. **Simple Graph Properties:**\n   - In a simple graph, each edge connects two distinct vertices, and there are no loops or multiple edges between the same pair of vertices.\n\n3. **Counting Edges:**\n   - For each edge $e \\in E$, if $e$ is incident to vertex $v$, it contributes exactly 1 to the degree of $v$.\n   - Since there are no loops, each edge can be counted at most once for any vertex.\n\n4. **Conclusion:**\n   - Therefore, the degree of a vertex $v$ in a simple graph is precisely the number of edges connected to it, as each edge contributes exactly once to the degree count.\n\nThus, the degree of a vertex in a simple graph is indeed the number of edges connected to it. This completes the proof."
    },
    {
      "id": "6",
      "statement": "A simple graph with n vertices can have at most n(n-1)/2 edges.",
      "proof": "To prove that a simple graph with $n$ vertices can have at most $\\frac{n(n-1)}{2}$ edges, we proceed as follows:\n\nA simple graph is an undirected graph that does not contain multiple edges between the same pair of vertices and does not contain loops (edges connected at both ends to the same vertex).\n\n1. **Counting Possible Edges:**\n\n   In a simple graph with $n$ vertices, each vertex can potentially be connected to every other vertex. Therefore, the number of possible pairs of vertices is given by the combination formula $\\binom{n}{2}$, which counts the number of ways to choose 2 vertices from $n$ vertices:\n\n   \\[\n   \\binom{n}{2} = \\frac{n(n-1)}{2}\n   \\]\n\n   This formula arises because for each pair of vertices $(v_i, v_j)$ where $i \\neq j$, there can be at most one edge connecting them.\n\n2. **Maximum Number of Edges:**\n\n   Since a simple graph cannot have multiple edges between the same pair of vertices and cannot have loops, the maximum number of edges is exactly the number of such pairs, which is $\\binom{n}{2}$.\n\n3. **Conclusion:**\n\n   Therefore, a simple graph with $n$ vertices can have at most $\\frac{n(n-1)}{2}$ edges, as this is the total number of unique pairs of vertices that can be connected by an edge.\n\nThis completes the proof that a simple graph with $n$ vertices can have at most $\\frac{n(n-1)}{2}$ edges."
    },
    {
      "id": "7",
      "statement": "If a graph has a vertex of degree zero, that vertex is isolated.",
      "proof": "To prove this statement, we need to show that if a vertex in a graph has degree zero, then it is an isolated vertex.\n\n**Definitions:**\n\n1. A vertex in a graph is said to have degree zero if it has no edges incident to it.\n2. An isolated vertex is a vertex with no edges connecting it to any other vertex in the graph.\n\n**Proof:**\n\nLet $G = (V, E)$ be a graph, where $V$ is the set of vertices and $E$ is the set of edges. Consider a vertex $v \\in V$ such that the degree of $v$, denoted $\\deg(v)$, is zero. By definition, $\\deg(v) = 0$ means that there are no edges in $E$ that have $v$ as an endpoint. \n\nSince there are no edges incident to $v$, $v$ is not connected to any other vertex in the graph. This is precisely the definition of an isolated vertex. Therefore, $v$ is an isolated vertex.\n\nThus, if a graph has a vertex of degree zero, that vertex is isolated. This completes the proof."
    },
    {
      "id": "8",
      "statement": "An undirected graph is bipartite if its vertex set can be divided into two disjoint sets such that no two graph vertices within the same set are adjacent.",
      "proof": "To prove that an undirected graph is bipartite if and only if its vertex set can be divided into two disjoint sets such that no two vertices within the same set are adjacent, we will use the concept of graph coloring and a characterization of bipartite graphs.\n\n**Proof:**\n\n**(\u21d2) If a graph is bipartite, then its vertex set can be divided into two disjoint sets such that no two vertices within the same set are adjacent.**\n\nBy definition, a graph $G = (V, E)$ is bipartite if its vertex set $V$ can be partitioned into two disjoint sets $U$ and $W$ such that every edge in $E$ connects a vertex in $U$ to a vertex in $W$. This means that there are no edges between vertices within the same set $U$ or within the same set $W$. Therefore, the condition is satisfied.\n\n**(\u21d0) If the vertex set of a graph can be divided into two disjoint sets such that no two vertices within the same set are adjacent, then the graph is bipartite.**\n\nAssume that the vertex set $V$ of a graph $G = (V, E)$ can be partitioned into two disjoint sets $U$ and $W$ such that no two vertices within the same set are adjacent. This implies that every edge in $E$ connects a vertex in $U$ to a vertex in $W$. Therefore, by definition, the graph is bipartite.\n\n**Alternative Characterization:**\n\nA graph is bipartite if and only if it does not contain any odd-length cycles. We can use this characterization to provide an alternative proof:\n\n1. **(\u21d2) If a graph is bipartite, it does not contain any odd-length cycles.**\n\n   Assume for contradiction that a bipartite graph contains an odd-length cycle. In a bipartite graph, vertices can be colored with two colors such that no two adjacent vertices share the same color. However, an odd-length cycle would require three colors to avoid adjacent vertices sharing the same color, which contradicts the bipartite property. Therefore, a bipartite graph cannot contain an odd-length cycle.\n\n2. **(\u21d0) If a graph does not contain any odd-length cycles, it is bipartite.**\n\n   We can use a two-coloring argument. Start by picking any vertex and color it with one color (say, color 1). Color all its adjacent vertices with the other color (color 2). Continue this process, alternating colors for adjacent vertices. If the graph contains no odd-length cycles, this coloring will be consistent, as any cycle would have an even length, allowing the colors to alternate correctly. Thus, the graph is bipartite.\n\nIn conclusion, an undirected graph is bipartite if and only if its vertex set can be divided into two disjoint sets such that no two vertices within the same set are adjacent, or equivalently, if it contains no odd-length cycles."
    },
    {
      "id": "9",
      "statement": "A tree is an acyclic connected graph.",
      "proof": "To prove that a tree is an acyclic connected graph, we need to show two things: \n\n1. A tree is connected.\n2. A tree is acyclic.\n\n**Definition of a Tree:**\nA tree is a graph $T = (V, E)$ that is connected and has no cycles.\n\n**Proof:**\n\n1. **A Tree is Connected:**\n\n   By definition, a tree is a connected graph. This means that for any two vertices $u, v \\in V$, there exists a path in $T$ that connects $u$ and $v$. Therefore, a tree is connected by its very definition.\n\n2. **A Tree is Acyclic:**\n\n   We need to show that a tree does not contain any cycles. Assume, for the sake of contradiction, that there is a cycle in the tree $T$. A cycle is a path that starts and ends at the same vertex without repeating any edges or vertices (except the starting and ending vertex).\n\n   If $T$ contains a cycle, we can remove one edge from this cycle, and the graph will still remain connected (since there is an alternative path between any two vertices of the cycle). However, removing an edge from a tree should disconnect the graph, as a tree with $n$ vertices has exactly $n-1$ edges, and removing any edge would result in a graph with fewer than $n-1$ edges, which cannot be connected. This contradiction implies that our assumption that $T$ contains a cycle is false.\n\n   Therefore, a tree is acyclic.\n\nSince a tree is both connected and acyclic by definition, we have shown that a tree is an acyclic connected graph."
    },
    {
      "id": "10",
      "statement": "A cycle in a graph is a path that starts and ends at the same vertex.",
      "proof": "To prove that a cycle in a graph is a path that starts and ends at the same vertex, we need to establish the definitions and properties of paths and cycles in graph theory.\n\n**Definitions:**\n\n1. **Graph:** A graph $G$ is an ordered pair $(V, E)$, where $V$ is a set of vertices and $E$ is a set of edges, which are 2-element subsets of $V$.\n\n2. **Path:** A path in a graph is a sequence of vertices $(v_1, v_2, \\ldots, v_k)$ such that for each $i$ (where $1 \\leq i < k$), there is an edge $\\{v_i, v_{i+1}\\} \\in E$. A path is simple if all vertices are distinct, except possibly the first and last vertices.\n\n3. **Cycle:** A cycle is a path $(v_1, v_2, \\ldots, v_k)$ with $k \\geq 3$ such that $v_1 = v_k$ and all other vertices $v_2, v_3, \\ldots, v_{k-1}$ are distinct.\n\n**Proof:**\n\nTo show that a cycle is a path that starts and ends at the same vertex, we need to verify the following:\n\n- **Path Property:** A cycle is a sequence of vertices $(v_1, v_2, \\ldots, v_k)$ such that for each $i$ (where $1 \\leq i < k$), there is an edge $\\{v_i, v_{i+1}\\} \\in E$. This satisfies the definition of a path.\n\n- **Start and End at the Same Vertex:** By definition, a cycle requires that $v_1 = v_k$. This means the path starts and ends at the same vertex.\n\n- **Distinct Intermediate Vertices:** For a cycle, all vertices $v_2, v_3, \\ldots, v_{k-1}$ must be distinct to ensure that the path does not revisit any vertex except for the starting and ending vertex.\n\nThus, by definition, a cycle in a graph is indeed a path that starts and ends at the same vertex, with the additional condition that all intermediate vertices are distinct. This completes the proof."
    },
    {
      "id": "11",
      "statement": "The sum of the degrees of all vertices in a graph is twice the number of edges.",
      "proof": "To prove that the sum of the degrees of all vertices in a graph is twice the number of edges, we can use the Handshaking Lemma, which is a fundamental result in graph theory.\n\nLet $G = (V, E)$ be an undirected graph, where $V$ is the set of vertices and $E$ is the set of edges. The degree of a vertex $v$, denoted $\\deg(v)$, is the number of edges incident to $v$.\n\nThe statement we want to prove is:\n\n\\[\n\\sum_{v \\in V} \\deg(v) = 2|E|\n\\]\n\n**Proof:**\n\n1. Consider an edge $e = \\{u, v\\} \\in E$. This edge contributes 1 to the degree of vertex $u$ and 1 to the degree of vertex $v$. Therefore, each edge contributes a total of 2 to the sum of the degrees of the vertices.\n\n2. Since this is true for every edge in the graph, the total contribution to the sum of the degrees from all edges is $2|E|$, where $|E|$ is the number of edges in the graph.\n\n3. Therefore, the sum of the degrees of all vertices in the graph is:\n\n\\[\n\\sum_{v \\in V} \\deg(v) = 2|E|\n\\]\n\nThis completes the proof. The sum of the degrees of all vertices in a graph is indeed twice the number of edges."
    },
    {
      "id": "12",
      "statement": "A cycle graph with three vertices is also known as a triangle.",
      "proof": "To prove that a cycle graph with three vertices is also known as a triangle, we need to understand the definitions involved.\n\nA **cycle graph** $C_n$ is a graph that consists of a single cycle with $n$ vertices. In a cycle graph, each vertex is connected to exactly two other vertices, forming a closed loop.\n\nFor the specific case of $n = 3$, we have a cycle graph $C_3$. This graph has three vertices, say $V_1$, $V_2$, and $V_3$, and three edges, which we can denote as $E_1 = (V_1, V_2)$, $E_2 = (V_2, V_3)$, and $E_3 = (V_3, V_1)$. \n\nThe structure of $C_3$ is such that each vertex is connected to two other vertices, forming a closed loop. This is precisely the definition of a triangle in graph theory: a simple cycle with three vertices and three edges.\n\nTo visualize, consider the vertices $V_1$, $V_2$, and $V_3$ placed at the corners of a triangle. The edges $E_1$, $E_2$, and $E_3$ form the sides of the triangle, connecting the vertices in a closed loop.\n\nTherefore, a cycle graph with three vertices, $C_3$, is indeed a triangle, as it satisfies the properties of a triangle in graph theory: three vertices connected by three edges in a closed loop."
    },
    {
      "id": "13",
      "statement": "The complement of a complete graph on n vertices is an empty graph on n vertices.",
      "proof": "To prove that the complement of a complete graph on $n$ vertices is an empty graph on $n$ vertices, we need to understand the definitions of a complete graph and its complement.\n\n1. **Complete Graph $K_n$:**\n   A complete graph $K_n$ is a graph in which every pair of distinct vertices is connected by a unique edge. Therefore, $K_n$ has $\\binom{n}{2} = \\frac{n(n-1)}{2}$ edges.\n\n2. **Complement of a Graph:**\n   The complement of a graph $G$, denoted by $\\overline{G}$, is a graph on the same set of vertices such that two distinct vertices are adjacent in $\\overline{G}$ if and only if they are not adjacent in $G$.\n\n3. **Empty Graph:**\n   An empty graph on $n$ vertices is a graph with $n$ vertices and no edges.\n\n**Proof:**\n\nConsider a complete graph $K_n$ on $n$ vertices. In $K_n$, every pair of distinct vertices is connected by an edge. Therefore, there are no pairs of vertices that are not connected by an edge.\n\nNow, let's construct the complement of $K_n$, denoted by $\\overline{K_n}$:\n\n- The vertex set of $\\overline{K_n}$ is the same as that of $K_n$, which consists of $n$ vertices.\n- In $\\overline{K_n}$, two distinct vertices are adjacent if and only if they are not adjacent in $K_n$.\n\nSince in $K_n$ every pair of distinct vertices is adjacent, in $\\overline{K_n}$, no pair of distinct vertices is adjacent. This means that $\\overline{K_n}$ has no edges.\n\nTherefore, $\\overline{K_n}$ is an empty graph on $n$ vertices, as it has $n$ vertices and $0$ edges.\n\nThus, we have shown that the complement of a complete graph on $n$ vertices is indeed an empty graph on $n$ vertices."
    },
    {
      "id": "14",
      "statement": "Adding an edge to a tree creates exactly one cycle.",
      "proof": "To prove that adding an edge to a tree creates exactly one cycle, we start by recalling some fundamental properties of trees in graph theory:\n\n1. A tree is a connected graph with no cycles.\n2. A tree with $n$ vertices has exactly $n-1$ edges.\n\nNow, consider a tree $T = (V, E)$, where $V$ is the set of vertices and $E$ is the set of edges. Since $T$ is a tree, it is connected and has no cycles, and $|E| = |V| - 1$.\n\nSuppose we add an edge $e = (u, v)$ to the tree $T$, where $u, v \\in V$. The new graph $T' = (V, E \\cup \\{e\\})$ will have $|E| + 1 = |V|$ edges.\n\n### Proof:\n\n1. **Connectivity**: Since $T$ is a tree, it is connected. Adding an edge $e = (u, v)$ between two vertices $u$ and $v$ in $T$ does not disconnect the graph. Therefore, $T'$ remains connected.\n\n2. **Cycle Formation**: In the original tree $T$, there is a unique path between any two vertices because $T$ is a tree. When we add the edge $e = (u, v)$, we create a new path between $u$ and $v$ that did not exist before. This new edge $e$ forms a cycle with the existing unique path between $u$ and $v$ in $T$. \n\n   Specifically, the cycle is formed by the path from $u$ to $v$ in the original tree $T$, plus the new edge $e = (u, v)$. Since there was exactly one path between $u$ and $v$ in $T$, adding the edge $e$ creates exactly one cycle.\n\n3. **No Additional Cycles**: Since $T$ was acyclic before adding the edge $e$, and we only added one edge, no other cycles can be formed. The only cycle is the one that includes the new edge $e$ and the path between $u$ and $v$ in the original tree.\n\nThus, adding an edge to a tree creates exactly one cycle. This completes the proof."
    },
    {
      "id": "15",
      "statement": "In a directed graph, the in-degree of a vertex is the number of edges pointing towards it.",
      "proof": "To prove the statement, we need to show that the in-degree of a vertex in a directed graph is indeed the number of edges pointing towards it.\n\nConsider a directed graph $G = (V, E)$, where $V$ is the set of vertices and $E$ is the set of directed edges. Each edge in $E$ is an ordered pair $(u, v)$, where $u, v \\in V$, indicating a directed edge from vertex $u$ to vertex $v$.\n\nThe in-degree of a vertex $v \\in V$, denoted as $\\text{in-deg}(v)$, is defined as the number of edges in $E$ for which $v$ is the terminal vertex. In other words, it is the number of edges of the form $(u, v)$ for some $u \\in V$.\n\nTo count the in-degree of vertex $v$, we consider all edges in the graph $G$ and count those that have $v$ as their endpoint. Specifically, we count the number of edges $(u, v)$ such that $u \\in V$ and $(u, v) \\in E$.\n\nThus, by definition, the in-degree of a vertex $v$ is precisely the number of edges pointing towards it. This completes the proof."
    }
  ],
  "Topology": [
    {
      "id": "1",
      "statement": "An open interval in the real numbers is an open set in the standard topology.",
      "proof": "To prove that an open interval in the real numbers is an open set in the standard topology, we need to show that for any point in the open interval, there exists an epsilon-neighborhood around that point which is entirely contained within the interval.\n\nLet $(a, b)$ be an open interval in $\\mathbb{R}$, where $a < b$. We need to show that $(a, b)$ is an open set in the standard topology on $\\mathbb{R}$.\n\nIn the standard topology on $\\mathbb{R}$, a set $U \\subseteq \\mathbb{R}$ is open if for every point $x \\in U$, there exists an $\\epsilon > 0$ such that the interval $(x - \\epsilon, x + \\epsilon) \\subseteq U$.\n\nConsider any point $x \\in (a, b)$. We need to find an $\\epsilon > 0$ such that $(x - \\epsilon, x + \\epsilon) \\subseteq (a, b)$.\n\nSince $x \\in (a, b)$, we have $a < x < b$. Define $\\epsilon_1 = x - a$ and $\\epsilon_2 = b - x$. Both $\\epsilon_1$ and $\\epsilon_2$ are positive because $x$ is strictly between $a$ and $b$.\n\nLet $\\epsilon = \\min(\\epsilon_1, \\epsilon_2)$. Then $\\epsilon > 0$, and we have:\n\n1. $x - \\epsilon \\geq x - \\epsilon_1 = a$, so $x - \\epsilon > a$.\n2. $x + \\epsilon \\leq x + \\epsilon_2 = b$, so $x + \\epsilon < b$.\n\nThus, $(x - \\epsilon, x + \\epsilon) \\subseteq (a, b)$.\n\nSince for every $x \\in (a, b)$, we can find such an $\\epsilon > 0$, the interval $(a, b)$ is open in the standard topology on $\\mathbb{R}$.\n\nTherefore, an open interval in the real numbers is an open set in the standard topology."
    },
    {
      "id": "2",
      "statement": "The union of two open sets in a topological space is also an open set.",
      "proof": "Let $(X, \\tau)$ be a topological space, where $X$ is a set and $\\tau$ is the topology on $X$. By definition, a topology $\\tau$ is a collection of subsets of $X$ that includes the empty set and $X$ itself, and is closed under arbitrary unions and finite intersections.\n\nTo prove that the union of two open sets is open, let $U$ and $V$ be two open sets in the topological space $(X, \\tau)$. By the definition of a topology, since $U \\in \\tau$ and $V \\in \\tau$, we need to show that $U \\cup V \\in \\tau$.\n\nThe property of a topology that is relevant here is that it is closed under arbitrary unions. This means that for any collection of open sets $\\{U_i\\}_{i \\in I}$ where each $U_i \\in \\tau$, the union $\\bigcup_{i \\in I} U_i$ is also in $\\tau$. In our case, we have a finite collection of open sets, specifically $U$ and $V$. Therefore, the union $U \\cup V$ is also an open set in $X$.\n\nThus, the union of two open sets $U$ and $V$ in a topological space $(X, \\tau)$ is open, as $U \\cup V \\in \\tau$. This completes the proof."
    },
    {
      "id": "3",
      "statement": "The intersection of any collection of closed sets in a topological space is also a closed set.",
      "proof": "To prove that the intersection of any collection of closed sets in a topological space is also a closed set, we start by recalling the definition of a closed set in a topological space.\n\nLet $(X, \\tau)$ be a topological space, where $X$ is a set and $\\tau$ is a topology on $X$. A subset $F \\subseteq X$ is called closed if its complement $X \\setminus F$ is open, i.e., $X \\setminus F \\in \\tau$.\n\nNow, let $\\{F_i\\}_{i \\in I}$ be a collection of closed sets in $X$, where $I$ is an index set. We want to show that the intersection $\\bigcap_{i \\in I} F_i$ is also a closed set.\n\nSince each $F_i$ is closed, by definition, $X \\setminus F_i$ is open for each $i \\in I$. We need to show that the complement of the intersection, $X \\setminus \\bigcap_{i \\in I} F_i$, is open.\n\nObserve that:\n\\[\nX \\setminus \\bigcap_{i \\in I} F_i = \\bigcup_{i \\in I} (X \\setminus F_i).\n\\]\n\nThis equality holds because an element $x \\in X$ is not in the intersection $\\bigcap_{i \\in I} F_i$ if and only if there exists at least one index $i \\in I$ such that $x \\notin F_i$, which is equivalent to saying $x \\in X \\setminus F_i$. Therefore, $x$ is in the union $\\bigcup_{i \\in I} (X \\setminus F_i)$.\n\nSince each $X \\setminus F_i$ is open and the union of any collection of open sets is open in a topological space, it follows that $\\bigcup_{i \\in I} (X \\setminus F_i)$ is open.\n\nThus, $X \\setminus \\bigcap_{i \\in I} F_i$ is open, which implies that $\\bigcap_{i \\in I} F_i$ is closed.\n\nTherefore, the intersection of any collection of closed sets in a topological space is also a closed set."
    },
    {
      "id": "4",
      "statement": "The entire set and the empty set are both open and closed in any topology.",
      "proof": "To prove that the entire set and the empty set are both open and closed in any topology, we need to refer to the definitions of open and closed sets in the context of topology.\n\nLet $X$ be a topological space with a topology $\\mathcal{T}$, which is a collection of subsets of $X$ satisfying the following properties:\n\n1. The empty set $\\emptyset$ and the entire set $X$ are in $\\mathcal{T}$.\n2. The union of any collection of sets in $\\mathcal{T}$ is also in $\\mathcal{T}$.\n3. The intersection of any finite collection of sets in $\\mathcal{T}$ is also in $\\mathcal{T}$.\n\n**Proof that $\\emptyset$ and $X$ are open:**\n\nBy definition of a topology, the empty set $\\emptyset$ and the entire set $X$ are elements of the topology $\\mathcal{T}$. Therefore, by definition, both $\\emptyset$ and $X$ are open sets.\n\n**Proof that $\\emptyset$ and $X$ are closed:**\n\nA set $A \\subseteq X$ is closed if its complement $X \\setminus A$ is open. \n\n- For the empty set $\\emptyset$, its complement is $X \\setminus \\emptyset = X$. Since $X$ is open (as it is in the topology $\\mathcal{T}$), $\\emptyset$ is closed.\n\n- For the entire set $X$, its complement is $X \\setminus X = \\emptyset$. Since $\\emptyset$ is open (as it is in the topology $\\mathcal{T}$), $X$ is closed.\n\nThus, in any topological space, both the entire set $X$ and the empty set $\\emptyset$ are open and closed."
    },
    {
      "id": "5",
      "statement": "In the standard topology of the real numbers, the interval (0, 1) is an open set.",
      "proof": "To prove that the interval $(0, 1)$ is an open set in the standard topology on the real numbers $\\mathbb{R}$, we need to show that for every point $x \\in (0, 1)$, there exists an open interval centered at $x$ that is entirely contained within $(0, 1)$.\n\nThe standard topology on $\\mathbb{R}$ is generated by the basis of open intervals $(a, b)$, where $a, b \\in \\mathbb{R}$ and $a < b$. A set $U \\subseteq \\mathbb{R}$ is open if for every point $x \\in U$, there exists an open interval $(c, d)$ such that $x \\in (c, d) \\subseteq U$.\n\nLet $x \\in (0, 1)$. We need to find an open interval $(c, d)$ such that $x \\in (c, d) \\subseteq (0, 1)$. Since $x \\in (0, 1)$, we have $0 < x < 1$. Define $\\epsilon = \\min\\{x - 0, 1 - x\\} = \\min\\{x, 1 - x\\}$. Note that $\\epsilon > 0$ because $0 < x < 1$.\n\nConsider the open interval $(x - \\epsilon, x + \\epsilon)$. We have:\n- $x - \\epsilon \\geq 0$ because $\\epsilon \\leq x$.\n- $x + \\epsilon \\leq 1$ because $\\epsilon \\leq 1 - x$.\n\nThus, $(x - \\epsilon, x + \\epsilon) \\subseteq (0, 1)$, and $x \\in (x - \\epsilon, x + \\epsilon)$.\n\nSince for every $x \\in (0, 1)$, we can find such an open interval $(x - \\epsilon, x + \\epsilon)$ contained in $(0, 1)$, the interval $(0, 1)$ is open in the standard topology on $\\mathbb{R}$."
    },
    {
      "id": "6",
      "statement": "If a set is finite, it is compact in the discrete topology.",
      "proof": "To prove that a finite set is compact in the discrete topology, we need to show that every open cover of the set has a finite subcover.\n\nLet $X$ be a finite set, and consider the discrete topology on $X$. In the discrete topology, every subset of $X$ is open. Let $\\mathcal{U}$ be an open cover of $X$. This means that $\\mathcal{U}$ is a collection of open sets such that $X = \\bigcup_{U \\in \\mathcal{U}} U$.\n\nSince $X$ is finite, we can write $X = \\{x_1, x_2, \\ldots, x_n\\}$ for some positive integer $n$. Each element $x_i \\in X$ must be contained in at least one of the open sets in $\\mathcal{U}$. For each $x_i$, choose an open set $U_i \\in \\mathcal{U}$ such that $x_i \\in U_i$. \n\nNow, consider the collection $\\{U_1, U_2, \\ldots, U_n\\}$. This is a finite subcollection of $\\mathcal{U}$, and we claim that it covers $X$. Indeed, for each $x_i \\in X$, we have $x_i \\in U_i$, so $x_i$ is covered by this subcollection. Therefore, $X = \\bigcup_{i=1}^n U_i$, which shows that $\\{U_1, U_2, \\ldots, U_n\\}$ is a finite subcover of $\\mathcal{U}$.\n\nSince every open cover of $X$ has a finite subcover, $X$ is compact in the discrete topology."
    },
    {
      "id": "7",
      "statement": "A continuous image of a compact space is compact.",
      "proof": "To prove that a continuous image of a compact space is compact, let us consider a topological space $(X, \\tau_X)$ and a compact space $(Y, \\tau_Y)$. Let $f: X \\to Y$ be a continuous function. We want to show that the image $f(X)$ is compact in $Y$.\n\n**Proof:**\n\n1. **Definition of Compactness:**\n   A space is compact if every open cover has a finite subcover. So, we need to show that for every open cover of $f(X)$, there exists a finite subcover.\n\n2. **Consider an Open Cover of $f(X)$:**\n   Let $\\{V_\\alpha\\}_{\\alpha \\in A}$ be an open cover of $f(X)$ in $Y$. This means that $f(X) \\subseteq \\bigcup_{\\alpha \\in A} V_\\alpha$ and each $V_\\alpha$ is open in $Y$.\n\n3. **Preimage and Continuity:**\n   Since $f$ is continuous, for each $V_\\alpha$, the preimage $f^{-1}(V_\\alpha)$ is open in $X$. \n\n4. **Construct an Open Cover of $X$:**\n   The collection $\\{f^{-1}(V_\\alpha)\\}_{\\alpha \\in A}$ is an open cover of $X$. This is because:\n   \\[\n   X = f^{-1}(f(X)) \\subseteq \\bigcup_{\\alpha \\in A} f^{-1}(V_\\alpha)\n   \\]\n   Therefore, $\\{f^{-1}(V_\\alpha)\\}_{\\alpha \\in A}$ covers $X$.\n\n5. **Compactness of $X$:**\n   Since $X$ is compact, there exists a finite subcover $\\{f^{-1}(V_{\\alpha_1}), f^{-1}(V_{\\alpha_2}), \\ldots, f^{-1}(V_{\\alpha_n})\\}$ of $X$.\n\n6. **Finite Subcover of $f(X)$:**\n   The corresponding collection $\\{V_{\\alpha_1}, V_{\\alpha_2}, \\ldots, V_{\\alpha_n}\\}$ is a finite subcover of $f(X)$ because:\n   \\[\n   f(X) \\subseteq \\bigcup_{i=1}^n V_{\\alpha_i}\n   \\]\n   This is due to the fact that if $x \\in X$, then $f(x) \\in f(X)$ and $x \\in f^{-1}(V_{\\alpha_i})$ for some $i$, which implies $f(x) \\in V_{\\alpha_i}$.\n\n7. **Conclusion:**\n   Since every open cover of $f(X)$ has a finite subcover, $f(X)$ is compact in $Y$.\n\nThus, we have shown that a continuous image of a compact space is compact."
    },
    {
      "id": "8",
      "statement": "In any topological space, the closure of a set is closed.",
      "proof": "To prove that the closure of a set is closed in any topological space, we need to show that for any subset $A$ of a topological space $(X, \\tau)$, the closure of $A$, denoted by $\\overline{A}$, is a closed set.\n\n**Proof:**\n\n1. **Definition of Closure:**\n   The closure of a set $A$ in a topological space $(X, \\tau)$ is defined as the smallest closed set containing $A$. It can also be characterized as the intersection of all closed sets containing $A$. Alternatively, $\\overline{A}$ can be defined as the set of all points $x \\in X$ such that every open set containing $x$ intersects $A$.\n\n2. **Characterization of Closed Sets:**\n   A set $C \\subseteq X$ is closed if and only if its complement $X \\setminus C$ is open.\n\n3. **Closure as an Intersection of Closed Sets:**\n   By definition, $\\overline{A}$ is the intersection of all closed sets containing $A$. Let $\\{C_i\\}_{i \\in I}$ be the collection of all closed sets containing $A$. Then:\n   \\[\n   \\overline{A} = \\bigcap_{i \\in I} C_i\n   \\]\n   Since each $C_i$ is closed, and the intersection of any collection of closed sets is closed (a basic property of topological spaces), it follows that $\\overline{A}$ is closed.\n\n4. **Conclusion:**\n   Therefore, the closure $\\overline{A}$ is closed in the topological space $(X, \\tau)$.\n\nThis completes the proof that in any topological space, the closure of a set is closed."
    },
    {
      "id": "9",
      "statement": "The interior of an empty set is empty.",
      "proof": "To prove that the interior of the empty set is empty, we need to understand the definition of the interior of a set in a topological space.\n\nLet $X$ be a topological space and $\\emptyset$ be the empty set. The interior of a set $A \\subseteq X$, denoted by $\\operatorname{int}(A)$, is the largest open set contained in $A$. \n\nNow, consider the empty set $\\emptyset \\subseteq X$. We want to find the largest open set contained in $\\emptyset$. \n\n1. **The empty set is open:** By definition, the empty set is considered an open set in any topological space.\n\n2. **The empty set is contained in itself:** Clearly, $\\emptyset \\subseteq \\emptyset$.\n\n3. **No non-empty open set is contained in the empty set:** Suppose there exists a non-empty open set $U \\subseteq X$ such that $U \\subseteq \\emptyset$. This would imply that $U$ is both non-empty and a subset of the empty set, which is a contradiction because the empty set has no elements.\n\nTherefore, the only open set that is contained in the empty set is the empty set itself. Hence, the interior of the empty set is the empty set:\n\n\\[\n\\operatorname{int}(\\emptyset) = \\emptyset.\n\\]\n\nThis completes the proof that the interior of the empty set is empty."
    },
    {
      "id": "10",
      "statement": "If a set is compact in a Hausdorff space, then it is closed.",
      "proof": "To prove that a compact set in a Hausdorff space is closed, let $X$ be a Hausdorff space and let $K \\subseteq X$ be a compact set. We need to show that $K$ is closed in $X$.\n\nRecall that a set is closed if its complement is open. Therefore, we need to show that $X \\setminus K$ is open. To do this, we will show that for any point $x \\in X \\setminus K$, there exists an open neighborhood of $x$ that is entirely contained in $X \\setminus K$.\n\nSince $K$ is compact and $X$ is Hausdorff, for each point $x \\in X \\setminus K$, and for each point $y \\in K$, there exist disjoint open sets $U_y$ and $V_y$ such that $x \\in U_y$ and $y \\in V_y$. The collection $\\{V_y\\}_{y \\in K}$ is an open cover of $K$.\n\nSince $K$ is compact, there exists a finite subcover $\\{V_{y_1}, V_{y_2}, \\ldots, V_{y_n}\\}$ that covers $K$. Correspondingly, we have the open sets $U_{y_1}, U_{y_2}, \\ldots, U_{y_n}$ such that $x \\in U_{y_i}$ for each $i = 1, 2, \\ldots, n$.\n\nDefine $U = \\bigcap_{i=1}^n U_{y_i}$. The set $U$ is open because it is a finite intersection of open sets, and $x \\in U$ because $x \\in U_{y_i}$ for each $i$. Moreover, $U \\cap K = \\emptyset$ because if there were a point $z \\in U \\cap K$, then $z \\in V_{y_i}$ for some $i$, which would contradict the disjointness of $U_{y_i}$ and $V_{y_i}$.\n\nThus, $U \\subseteq X \\setminus K$, and $x \\in U$. Since $x$ was an arbitrary point in $X \\setminus K$, this shows that $X \\setminus K$ is open.\n\nTherefore, $K$ is closed in $X$."
    },
    {
      "id": "11",
      "statement": "In the product topology, the projection maps are continuous.",
      "proof": "To prove that the projection maps are continuous in the product topology, let's first set up the context and definitions.\n\nLet $\\{X_i\\}_{i \\in I}$ be a family of topological spaces indexed by a set $I$. The product space $\\prod_{i \\in I} X_i$ is the set of all functions $x: I \\to \\bigcup_{i \\in I} X_i$ such that $x(i) \\in X_i$ for each $i \\in I$. The product topology on $\\prod_{i \\in I} X_i$ is the topology generated by the basis consisting of all sets of the form $\\prod_{i \\in I} U_i$, where $U_i$ is open in $X_i$ for each $i \\in I$, and $U_i = X_i$ for all but finitely many $i$.\n\nFor each $j \\in I$, the projection map $\\pi_j: \\prod_{i \\in I} X_i \\to X_j$ is defined by $\\pi_j(x) = x(j)$ for each $x \\in \\prod_{i \\in I} X_i$.\n\n**Claim:** The projection map $\\pi_j$ is continuous for each $j \\in I$.\n\n**Proof:**\n\nTo show that $\\pi_j$ is continuous, we need to show that for every open set $V \\subseteq X_j$, the preimage $\\pi_j^{-1}(V)$ is open in the product topology on $\\prod_{i \\in I} X_i$.\n\nLet $V \\subseteq X_j$ be an open set. Consider the preimage $\\pi_j^{-1}(V)$. We have:\n\n\\[\n\\pi_j^{-1}(V) = \\{ x \\in \\prod_{i \\in I} X_i \\mid \\pi_j(x) \\in V \\} = \\{ x \\in \\prod_{i \\in I} X_i \\mid x(j) \\in V \\}.\n\\]\n\nNow, consider the basic open set $U = \\prod_{i \\in I} U_i$ in the product topology, where $U_j = V$ and $U_i = X_i$ for all $i \\neq j$. Clearly, $U \\subseteq \\pi_j^{-1}(V)$ because if $x \\in U$, then $x(j) \\in U_j = V$.\n\nConversely, if $x \\in \\pi_j^{-1}(V)$, then $x(j) \\in V$, and we can choose $U_i = X_i$ for all $i \\neq j$, which means $x \\in \\prod_{i \\in I} U_i$. Therefore, $\\pi_j^{-1}(V) = \\prod_{i \\in I} U_i$, which is a basic open set in the product topology.\n\nSince $\\pi_j^{-1}(V)$ is a basic open set in the product topology, it follows that $\\pi_j$ is continuous.\n\nThus, the projection maps $\\pi_j$ are continuous in the product topology."
    },
    {
      "id": "12",
      "statement": "An open set in the standard topology of the real numbers can be expressed as a union of open intervals.",
      "proof": "To prove that an open set in the standard topology of the real numbers $\\mathbb{R}$ can be expressed as a union of open intervals, we start by recalling the definition of an open set in $\\mathbb{R}$.\n\nIn the standard topology on $\\mathbb{R}$, a set $U \\subseteq \\mathbb{R}$ is open if for every point $x \\in U$, there exists an $\\epsilon > 0$ such that the open interval $(x - \\epsilon, x + \\epsilon)$ is entirely contained in $U$. \n\nNow, let $U$ be an open set in $\\mathbb{R}$. For each point $x \\in U$, there exists an open interval $(x - \\epsilon_x, x + \\epsilon_x) \\subseteq U$ for some $\\epsilon_x > 0$. Consider the collection of all such open intervals:\n\n$$ \\mathcal{I} = \\{ (x - \\epsilon_x, x + \\epsilon_x) \\mid x \\in U \\}. $$\n\nThe set $U$ can be expressed as the union of these open intervals:\n\n$$ U = \\bigcup_{x \\in U} (x - \\epsilon_x, x + \\epsilon_x). $$\n\nEach $(x - \\epsilon_x, x + \\epsilon_x)$ is an open interval, and the union of open intervals is open. Therefore, $U$ is a union of open intervals.\n\nTo show that this union is indeed $U$, consider any point $y \\in U$. By the definition of an open set, there exists an interval $(y - \\epsilon_y, y + \\epsilon_y) \\subseteq U$. Clearly, $y \\in (y - \\epsilon_y, y + \\epsilon_y)$, and this interval is part of the union $\\bigcup_{x \\in U} (x - \\epsilon_x, x + \\epsilon_x)$. Thus, every point in $U$ is contained in the union, and hence $U$ is exactly the union of these open intervals.\n\nTherefore, any open set in the standard topology on $\\mathbb{R}$ can be expressed as a union of open intervals."
    },
    {
      "id": "13",
      "statement": "In a metric space, balls are open sets.",
      "proof": "To prove that balls are open sets in a metric space, we need to show that for any point inside a ball, there exists a neighborhood around that point which is entirely contained within the ball.\n\nLet $(X, d)$ be a metric space, and let $B(x_0, r) = \\{ x \\in X \\mid d(x, x_0) < r \\}$ be an open ball centered at $x_0$ with radius $r > 0$. We want to show that $B(x_0, r)$ is an open set.\n\nTake any point $x \\in B(x_0, r)$. By the definition of the ball, we have $d(x, x_0) < r$. Let $\\epsilon = r - d(x, x_0) > 0$. We will show that the open ball $B(x, \\epsilon) = \\{ y \\in X \\mid d(y, x) < \\epsilon \\}$ is contained within $B(x_0, r)$.\n\nConsider any point $y \\in B(x, \\epsilon)$. By definition, $d(y, x) < \\epsilon$. We need to show that $y \\in B(x_0, r)$, i.e., $d(y, x_0) < r$.\n\nUsing the triangle inequality, we have:\n\\[\nd(y, x_0) \\leq d(y, x) + d(x, x_0).\n\\]\n\nSince $d(y, x) < \\epsilon$ and $d(x, x_0) < r$, we can substitute these into the inequality:\n\\[\nd(y, x_0) < \\epsilon + d(x, x_0).\n\\]\n\nBy the choice of $\\epsilon = r - d(x, x_0)$, we have:\n\\[\n\\epsilon + d(x, x_0) = (r - d(x, x_0)) + d(x, x_0) = r.\n\\]\n\nThus, we have:\n\\[\nd(y, x_0) < r.\n\\]\n\nThis shows that $y \\in B(x_0, r)$. Since $y$ was an arbitrary point in $B(x, \\epsilon)$, we have $B(x, \\epsilon) \\subseteq B(x_0, r)$.\n\nTherefore, for every point $x \\in B(x_0, r)$, there exists an $\\epsilon$-neighborhood $B(x, \\epsilon)$ entirely contained within $B(x_0, r)$. This proves that $B(x_0, r)$ is an open set in the metric space $(X, d)$."
    },
    {
      "id": "14",
      "statement": "A topological space is connected if it cannot be partitioned into two non-empty open sets.",
      "proof": "To prove that a topological space $X$ is connected if and only if it cannot be partitioned into two non-empty open sets, we need to show two implications:\n\n1. If $X$ is connected, then it cannot be partitioned into two non-empty open sets.\n2. If $X$ cannot be partitioned into two non-empty open sets, then it is connected.\n\n**Proof:**\n\n1. **(Connected implies no partition into two non-empty open sets):**\n\n   Assume $X$ is connected. By definition, this means there do not exist two non-empty disjoint open sets $U$ and $V$ such that $X = U \\cup V$. Suppose, for the sake of contradiction, that $X$ can be partitioned into two non-empty open sets $U$ and $V$. Then $U \\cap V = \\emptyset$ and $U \\cup V = X$. This contradicts the assumption that $X$ is connected. Therefore, if $X$ is connected, it cannot be partitioned into two non-empty open sets.\n\n2. **(No partition into two non-empty open sets implies connected):**\n\n   Assume $X$ cannot be partitioned into two non-empty open sets. We need to show that $X$ is connected. Suppose, for the sake of contradiction, that $X$ is not connected. Then there exist two non-empty disjoint open sets $U$ and $V$ such that $X = U \\cup V$. This is a partition of $X$ into two non-empty open sets, which contradicts our assumption. Therefore, if $X$ cannot be partitioned into two non-empty open sets, it must be connected.\n\nThus, a topological space $X$ is connected if and only if it cannot be partitioned into two non-empty open sets."
    },
    {
      "id": "15",
      "statement": "In a topological space, a finite intersection of open sets is open.",
      "proof": "To prove that a finite intersection of open sets is open in a topological space, we start by recalling the definition of a topological space. A topological space $(X, \\tau)$ consists of a set $X$ and a collection $\\tau$ of subsets of $X$ satisfying the following properties:\n\n1. The empty set $\\emptyset$ and the entire set $X$ are in $\\tau$.\n2. The union of any collection of sets in $\\tau$ is also in $\\tau$.\n3. The intersection of any finite collection of sets in $\\tau$ is also in $\\tau$.\n\nThe sets in $\\tau$ are called open sets.\n\nWe need to show that if $U_1, U_2, \\ldots, U_n$ are open sets in $X$, then their intersection $U_1 \\cap U_2 \\cap \\cdots \\cap U_n$ is also an open set in $X$.\n\n**Proof:**\n\nWe proceed by induction on the number of open sets being intersected.\n\n**Base Case:** For $n = 1$, the intersection of a single open set $U_1$ is just $U_1$ itself, which is open by definition.\n\n**Inductive Step:** Assume that the intersection of any $k$ open sets is open. That is, for open sets $U_1, U_2, \\ldots, U_k$, the intersection $U_1 \\cap U_2 \\cap \\cdots \\cap U_k$ is open.\n\nNow consider $k+1$ open sets $U_1, U_2, \\ldots, U_k, U_{k+1}$. By the inductive hypothesis, the intersection $U_1 \\cap U_2 \\cap \\cdots \\cap U_k$ is open. Since $U_{k+1}$ is also open, the intersection $(U_1 \\cap U_2 \\cap \\cdots \\cap U_k) \\cap U_{k+1}$ is the intersection of two open sets, which is open by the definition of a topological space (finite intersections of open sets are open).\n\nThus, by induction, the intersection of any finite number of open sets is open.\n\nTherefore, in a topological space, a finite intersection of open sets is open."
    }
  ],
  "Real Analysis": [
    {
      "id": "1",
      "statement": "The sum of any two real numbers is a real number.",
      "proof": "To prove that the sum of any two real numbers is a real number, we rely on the properties of the real number system, specifically the closure property of addition.\n\n**Proof:**\n\nLet $a$ and $b$ be any two real numbers. We need to show that their sum, $a + b$, is also a real number.\n\nThe set of real numbers, denoted by $\\mathbb{R}$, is equipped with the operation of addition, which satisfies several properties, one of which is the closure property. The closure property of addition states that for any two real numbers $a$ and $b$, the sum $a + b$ is also a real number.\n\nSince $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, by the closure property of addition in the real numbers, it follows that:\n\n\\[ a + b \\in \\mathbb{R}. \\]\n\nTherefore, the sum of any two real numbers is indeed a real number. This completes the proof."
    },
    {
      "id": "2",
      "statement": "The product of any two real numbers is a real number.",
      "proof": "To prove that the product of any two real numbers is a real number, we rely on the properties of the real number system, specifically the closure property of multiplication.\n\n**Proof:**\n\nLet $a$ and $b$ be any two real numbers. We need to show that the product $a \\cdot b$ is also a real number.\n\nThe set of real numbers, denoted by $\\mathbb{R}$, is defined to be closed under multiplication. This means that for any two elements $a, b \\in \\mathbb{R}$, the product $a \\cdot b$ is also an element of $\\mathbb{R}$.\n\nThe closure property of multiplication for real numbers is one of the fundamental axioms of the real number system. It is part of the field axioms, which define the operations of addition and multiplication for real numbers. Specifically, the closure property states:\n\n- For all $a, b \\in \\mathbb{R}$, $a \\cdot b \\in \\mathbb{R}$.\n\nSince $a$ and $b$ are arbitrary real numbers, by the closure property, their product $a \\cdot b$ must also be a real number.\n\nTherefore, the product of any two real numbers is a real number. This completes the proof."
    },
    {
      "id": "3",
      "statement": "For any real number, adding zero to it yields the same number.",
      "proof": "To prove that for any real number $x$, adding zero to it yields the same number, we need to show that $x + 0 = x$ for all $x \\in \\mathbb{R}$.\n\nThis property is a fundamental part of the axioms of the real numbers, specifically the additive identity axiom. According to this axiom, there exists a unique element $0 \\in \\mathbb{R}$ such that for every real number $x$, the equation $x + 0 = x$ holds true.\n\n**Proof:**\n\n1. **Axiom of Additive Identity:** By the definition of the real numbers, there exists an additive identity, denoted as $0$, such that for any real number $x$, the equation $x + 0 = x$ holds.\n\n2. **Verification:** Let $x$ be any arbitrary real number. According to the additive identity axiom, we have:\n   \\[\n   x + 0 = x\n   \\]\n\n3. **Conclusion:** Since the above equation holds for any arbitrary real number $x$, we conclude that adding zero to any real number yields the same number.\n\nThus, the statement is proven: for any real number $x$, $x + 0 = x$."
    },
    {
      "id": "4",
      "statement": "A sequence of real numbers that is constantly equal to zero converges to zero.",
      "proof": "To prove that a sequence of real numbers that is constantly equal to zero converges to zero, we need to show that for every $\\epsilon > 0$, there exists a natural number $N$ such that for all $n \\geq N$, the absolute difference between the sequence term and the limit is less than $\\epsilon$.\n\nConsider the sequence $\\{a_n\\}$ defined by $a_n = 0$ for all $n \\in \\mathbb{N}$. We want to show that $\\{a_n\\}$ converges to 0.\n\nBy definition of convergence, for every $\\epsilon > 0$, we must find an $N \\in \\mathbb{N}$ such that for all $n \\geq N$, $|a_n - 0| < \\epsilon$.\n\nSince $a_n = 0$ for all $n$, we have:\n\n\\[\n|a_n - 0| = |0 - 0| = 0\n\\]\n\nClearly, $0 < \\epsilon$ for any $\\epsilon > 0$. Therefore, the condition $|a_n - 0| < \\epsilon$ is satisfied for all $n \\in \\mathbb{N}$, regardless of the choice of $N$.\n\nThus, we can choose $N = 1$, or any natural number, and the condition holds for all $n \\geq N$. Therefore, the sequence $\\{a_n\\}$ converges to 0. \n\nThis completes the proof."
    },
    {
      "id": "5",
      "statement": "The absolute value of a real number is always greater than or equal to zero.",
      "proof": "To prove that the absolute value of a real number is always greater than or equal to zero, let's consider a real number $x$. The absolute value of $x$, denoted by $|x|$, is defined as:\n\n\\[\n|x| = \n\\begin{cases} \nx, & \\text{if } x \\geq 0, \\\\\n-x, & \\text{if } x < 0.\n\\end{cases}\n\\]\n\nWe need to show that $|x| \\geq 0$ for all real numbers $x$.\n\n1. **Case 1: $x \\geq 0$**\n\n   In this case, by definition, $|x| = x$. Since $x \\geq 0$, it follows directly that $|x| = x \\geq 0$.\n\n2. **Case 2: $x < 0$**\n\n   In this case, by definition, $|x| = -x$. Since $x < 0$, the negative of $x$ is positive, i.e., $-x > 0$. Therefore, $|x| = -x \\geq 0$.\n\nIn both cases, we have shown that $|x| \\geq 0$. Thus, the absolute value of any real number $x$ is always greater than or equal to zero. This completes the proof."
    },
    {
      "id": "6",
      "statement": "The square of any real number is always non-negative.",
      "proof": "To prove that the square of any real number is always non-negative, let us consider a real number $x$. We need to show that $x^2 \\geq 0$ for all real numbers $x$.\n\nWe can analyze this by considering two cases based on the value of $x$:\n\n1. **Case 1: $x \\geq 0$**\n\n   If $x$ is non-negative, then $x^2 = x \\cdot x$. Since $x \\geq 0$, the product $x \\cdot x$ is also non-negative. Therefore, $x^2 \\geq 0$.\n\n2. **Case 2: $x < 0$**\n\n   If $x$ is negative, then $x = -y$ for some positive real number $y$ (i.e., $y = -x > 0$). In this case, $x^2 = (-y)^2 = y^2$. Since $y$ is positive, $y^2 = y \\cdot y$ is non-negative. Therefore, $x^2 = y^2 \\geq 0$.\n\nIn both cases, we have shown that $x^2 \\geq 0$. Thus, the square of any real number is always non-negative."
    },
    {
      "id": "7",
      "statement": "For any real number x, the inequality -x \u2264 x holds.",
      "proof": "To prove the inequality $-x \\leq x$ for any real number $x$, we can consider two cases based on the sign of $x$.\n\n**Case 1: $x \\geq 0$**\n\nIf $x$ is non-negative, then $-x \\leq 0 \\leq x$. The inequality $-x \\leq x$ holds because $-x$ is less than or equal to $0$, and $0$ is less than or equal to $x$.\n\n**Case 2: $x < 0$**\n\nIf $x$ is negative, then $-x$ is positive. In this case, $-x > 0$ and $x < 0$. Since $-x$ is positive and $x$ is negative, it follows that $-x > x$. However, the inequality $-x \\leq x$ is trivially true because $x < x$ is not possible, and the inequality is not strict.\n\nIn both cases, the inequality $-x \\leq x$ holds for any real number $x$. Therefore, we have shown that for any real number $x$, the inequality $-x \\leq x$ is true."
    },
    {
      "id": "8",
      "statement": "The function f(x) = x is continuous at every real number x.",
      "proof": "To prove that the function $f(x) = x$ is continuous at every real number $x$, we need to show that for every $\\epsilon > 0$, there exists a $\\delta > 0$ such that for all $x$ in the domain of $f$, if $|x - c| < \\delta$, then $|f(x) - f(c)| < \\epsilon$, where $c$ is any real number.\n\nLet's consider the function $f(x) = x$ and a point $c \\in \\mathbb{R}$. We want to show that $f$ is continuous at $c$. \n\n1. **Definition of Continuity**: According to the definition of continuity, $f$ is continuous at $c$ if for every $\\epsilon > 0$, there exists a $\\delta > 0$ such that if $|x - c| < \\delta$, then $|f(x) - f(c)| < \\epsilon$.\n\n2. **Apply the Definition**: For the function $f(x) = x$, we have $f(c) = c$. Therefore, $|f(x) - f(c)| = |x - c|$.\n\n3. **Choose $\\delta$**: Given $\\epsilon > 0$, we can choose $\\delta = \\epsilon$. \n\n4. **Verify the Condition**: If $|x - c| < \\delta$, then $|x - c| < \\epsilon$. Therefore, $|f(x) - f(c)| = |x - c| < \\epsilon$.\n\nSince we can find such a $\\delta$ for every $\\epsilon > 0$, the function $f(x) = x$ is continuous at every real number $c$.\n\nThus, we have shown that the function $f(x) = x$ is continuous at every real number $x$."
    },
    {
      "id": "9",
      "statement": "If a real number is greater than another, their sum is greater than twice the smaller number.",
      "proof": "To prove this statement, let's consider two real numbers $a$ and $b$ such that $a > b$. We need to show that the sum $a + b$ is greater than twice the smaller number $b$, i.e., $a + b > 2b$.\n\n**Proof:**\n\nSince $a > b$, we can subtract $b$ from both sides of the inequality to obtain:\n\n\\[\na - b > 0\n\\]\n\nNow, add $b$ to both sides of this inequality:\n\n\\[\na - b + b > 0 + b\n\\]\n\nSimplifying the left side, we have:\n\n\\[\na > b\n\\]\n\nNow, add $b$ to both sides of the inequality $a > b$:\n\n\\[\na + b > b + b\n\\]\n\nThis simplifies to:\n\n\\[\na + b > 2b\n\\]\n\nThus, we have shown that if $a > b$, then $a + b > 2b$. This completes the proof."
    },
    {
      "id": "10",
      "statement": "For any real number x, the expression x^2 + 1 is always positive.",
      "proof": "To prove that for any real number $x$, the expression $x^2 + 1$ is always positive, we proceed as follows:\n\n1. **Consider the expression $x^2 + 1$:**\n\n   The expression $x^2 + 1$ consists of two terms: $x^2$ and $1$.\n\n2. **Analyze $x^2$:**\n\n   The square of any real number $x$, denoted $x^2$, is always non-negative. This is because:\n   - If $x = 0$, then $x^2 = 0$.\n   - If $x > 0$, then $x^2 > 0$.\n   - If $x < 0$, then $x^2 > 0$ (since the square of a negative number is positive).\n\n   Therefore, $x^2 \\geq 0$ for all real numbers $x$.\n\n3. **Add 1 to $x^2$:**\n\n   Since $x^2 \\geq 0$, adding 1 to $x^2$ gives $x^2 + 1 \\geq 0 + 1 = 1$.\n\n4. **Conclusion:**\n\n   Therefore, for any real number $x$, $x^2 + 1 \\geq 1 > 0$. \n\nThus, the expression $x^2 + 1$ is always positive for any real number $x$."
    },
    {
      "id": "11",
      "statement": "The limit of a constant sequence is the value of that constant.",
      "proof": "To prove that the limit of a constant sequence is the value of that constant, let's consider a constant sequence $\\{a_n\\}$ where $a_n = c$ for all $n \\in \\mathbb{N}$, and $c$ is a constant real number.\n\nWe want to show that $\\lim_{n \\to \\infty} a_n = c$.\n\nBy definition, the sequence $\\{a_n\\}$ converges to the limit $L$ if for every $\\epsilon > 0$, there exists a natural number $N$ such that for all $n \\geq N$, $|a_n - L| < \\epsilon$.\n\nIn the case of our constant sequence, $a_n = c$ for all $n$. Therefore, for any $\\epsilon > 0$, we have:\n\n\\[\n|a_n - c| = |c - c| = 0 < \\epsilon\n\\]\n\nThis inequality holds for all $n \\in \\mathbb{N}$, regardless of the choice of $N$. Thus, we can choose $N = 1$, or any natural number, and the condition $|a_n - c| < \\epsilon$ will be satisfied for all $n \\geq N$.\n\nTherefore, by the definition of the limit of a sequence, we conclude that:\n\n\\[\n\\lim_{n \\to \\infty} a_n = c\n\\]\n\nThis completes the proof that the limit of a constant sequence is the value of that constant."
    },
    {
      "id": "12",
      "statement": "An increasing sequence of real numbers that is bounded above converges.",
      "proof": "To prove that an increasing sequence of real numbers that is bounded above converges, we can use the Monotone Convergence Theorem. Let's go through the proof step by step.\n\nLet $\\{a_n\\}$ be an increasing sequence of real numbers that is bounded above. This means:\n\n1. The sequence is increasing: $a_n \\leq a_{n+1}$ for all $n \\in \\mathbb{N}$.\n2. The sequence is bounded above: there exists a real number $M$ such that $a_n \\leq M$ for all $n \\in \\mathbb{N}$.\n\nWe need to show that $\\{a_n\\}$ converges to some limit $L \\in \\mathbb{R}$.\n\n**Proof:**\n\nSince $\\{a_n\\}$ is bounded above, by the Completeness Axiom (or the Least Upper Bound Property) of the real numbers, the set $\\{a_n : n \\in \\mathbb{N}\\}$ has a least upper bound. Let $L = \\sup\\{a_n : n \\in \\mathbb{N}\\}$.\n\nWe claim that $\\{a_n\\}$ converges to $L$. To prove this, we need to show that for every $\\epsilon > 0$, there exists an $N \\in \\mathbb{N}$ such that for all $n \\geq N$, $|a_n - L| < \\epsilon$.\n\nSince $L = \\sup\\{a_n : n \\in \\mathbb{N}\\}$, for any $\\epsilon > 0$, $L - \\epsilon$ is not an upper bound for the sequence $\\{a_n\\}$. Therefore, there exists some $N \\in \\mathbb{N}$ such that $a_N > L - \\epsilon$.\n\nBecause the sequence is increasing, for all $n \\geq N$, we have $a_n \\geq a_N > L - \\epsilon$. Also, since $L$ is an upper bound for the sequence, $a_n \\leq L$ for all $n \\in \\mathbb{N}$.\n\nThus, for all $n \\geq N$, we have:\n\\[\nL - \\epsilon < a_n \\leq L.\n\\]\n\nThis implies:\n\\[\n|a_n - L| = L - a_n < \\epsilon.\n\\]\n\nTherefore, for every $\\epsilon > 0$, there exists an $N \\in \\mathbb{N}$ such that for all $n \\geq N$, $|a_n - L| < \\epsilon$. This shows that $\\{a_n\\}$ converges to $L$.\n\nHence, an increasing sequence of real numbers that is bounded above converges."
    },
    {
      "id": "13",
      "statement": "The limit as x approaches zero of x over x is 1.",
      "proof": "To prove that \n\n\\[\n\\lim_{x \\to 0} \\frac{x}{x} = 1,\n\\]\n\nwe need to consider the function \\( f(x) = \\frac{x}{x} \\) for \\( x \\neq 0 \\). For all \\( x \\neq 0 \\), the function simplifies to \\( f(x) = 1 \\).\n\nThe limit of a constant function is simply the constant itself. Therefore, for all \\( x \\neq 0 \\), we have:\n\n\\[\nf(x) = 1.\n\\]\n\nThus, the limit as \\( x \\) approaches 0 of \\( f(x) \\) is:\n\n\\[\n\\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} 1 = 1.\n\\]\n\nIt is important to note that the function \\( f(x) = \\frac{x}{x} \\) is not defined at \\( x = 0 \\) because division by zero is undefined. However, the limit only concerns the behavior of the function as \\( x \\) approaches 0, not the value of the function at 0 itself. Since \\( f(x) = 1 \\) for all \\( x \\neq 0 \\), the limit is indeed 1."
    },
    {
      "id": "14",
      "statement": "For a non-zero real number x, x^2 > 0 holds true.",
      "proof": "To prove that for a non-zero real number $x$, $x^2 > 0$, we can use the properties of real numbers and the definition of squares.\n\n**Proof:**\n\nLet $x$ be a non-zero real number. We need to show that $x^2 > 0$.\n\n1. **Case 1: $x > 0$**\n\n   If $x$ is a positive real number, then $x > 0$. By the properties of real numbers, the product of two positive numbers is positive. Therefore, $x \\cdot x = x^2 > 0$.\n\n2. **Case 2: $x < 0$**\n\n   If $x$ is a negative real number, then $x < 0$. The product of two negative numbers is positive. Therefore, $x \\cdot x = x^2 > 0$.\n\nIn both cases, whether $x$ is positive or negative, $x^2 > 0$. \n\nThus, for any non-zero real number $x$, $x^2 > 0$ holds true."
    },
    {
      "id": "15",
      "statement": "For a non-positive real number x, -x is non-negative.",
      "proof": "To prove that for a non-positive real number $x$, $-x$ is non-negative, we start by considering the definition of a non-positive real number.\n\nA real number $x$ is said to be non-positive if $x \\leq 0$. We need to show that if $x \\leq 0$, then $-x \\geq 0$.\n\n**Proof:**\n\n1. Assume $x \\leq 0$. This means that $x$ is either zero or a negative number.\n\n2. We want to show that $-x \\geq 0$. \n\n3. Consider the expression $-x$. By the properties of inequalities, if $x \\leq 0$, then multiplying both sides of the inequality by $-1$ (and reversing the inequality sign) gives $-x \\geq 0$.\n\n   - If $x = 0$, then $-x = -0 = 0$, which is non-negative.\n   - If $x < 0$, then $-x > 0$, which is also non-negative.\n\n4. Therefore, in both cases, $-x$ is non-negative.\n\nThus, we have shown that for any non-positive real number $x$, the number $-x$ is non-negative. This completes the proof."
    }
  ],
  "Differential Equations": [
    {
      "id": "1",
      "statement": "A constant function is a solution to the differential equation dy/dx = 0.",
      "proof": "To prove that a constant function is a solution to the differential equation $\\frac{dy}{dx} = 0$, let's consider a constant function $y(x) = c$, where $c$ is a constant.\n\nThe derivative of a constant function with respect to $x$ is zero. Therefore, we have:\n\n\\[\n\\frac{dy}{dx} = \\frac{d}{dx}(c) = 0\n\\]\n\nThis shows that the derivative of $y(x) = c$ is indeed zero, which satisfies the differential equation $\\frac{dy}{dx} = 0$.\n\nThus, any constant function $y(x) = c$ is a solution to the differential equation $\\frac{dy}{dx} = 0$."
    },
    {
      "id": "2",
      "statement": "If a function solves the differential equation dy/dx = y, then its derivative is proportional to the function itself.",
      "proof": "To prove that if a function $y(x)$ solves the differential equation $\\frac{dy}{dx} = y$, then its derivative is proportional to the function itself, we start by analyzing the given differential equation:\n\n\\[\n\\frac{dy}{dx} = y\n\\]\n\nThis equation states that the derivative of the function $y(x)$ with respect to $x$ is equal to the function itself. In other words, the rate of change of the function is directly proportional to the value of the function at any point $x$. \n\nTo solve this differential equation, we can separate variables and integrate:\n\n1. Separate the variables:\n\n\\[\n\\frac{dy}{y} = dx\n\\]\n\n2. Integrate both sides:\n\n\\[\n\\int \\frac{1}{y} \\, dy = \\int 1 \\, dx\n\\]\n\nThis gives:\n\n\\[\n\\ln |y| = x + C\n\\]\n\nwhere $C$ is the constant of integration.\n\n3. Solve for $y$:\n\nExponentiate both sides to eliminate the natural logarithm:\n\n\\[\n|y| = e^{x+C} = e^x \\cdot e^C\n\\]\n\nLet $e^C = C_1$, where $C_1$ is a positive constant (since $e^C > 0$), then:\n\n\\[\n|y| = C_1 e^x\n\\]\n\nThus, $y = \\pm C_1 e^x$. For simplicity, we can write:\n\n\\[\ny = C e^x\n\\]\n\nwhere $C$ is any real constant (positive or negative).\n\nThis solution shows that the function $y(x)$ is of the form $C e^x$, where $C$ is a constant. The derivative of this function is:\n\n\\[\n\\frac{dy}{dx} = C e^x\n\\]\n\nSince $y = C e^x$, we have:\n\n\\[\n\\frac{dy}{dx} = y\n\\]\n\nThis confirms that the derivative of the function is indeed proportional to the function itself, with the proportionality constant being 1. Thus, the statement is proven."
    },
    {
      "id": "3",
      "statement": "The differential equation d^2y/dx^2 = 0 has linear functions as solutions.",
      "proof": "To prove that the differential equation $\\frac{d^2y}{dx^2} = 0$ has linear functions as solutions, we need to solve the differential equation.\n\nThe given differential equation is:\n\n\\[\n\\frac{d^2y}{dx^2} = 0\n\\]\n\nThis is a second-order homogeneous linear differential equation with constant coefficients. The general approach to solving such an equation is to integrate it with respect to $x$.\n\n1. **First Integration:**\n\n   Integrate both sides with respect to $x$:\n\n   \\[\n   \\int \\frac{d^2y}{dx^2} \\, dx = \\int 0 \\, dx\n   \\]\n\n   This gives:\n\n   \\[\n   \\frac{dy}{dx} = C_1\n   \\]\n\n   where $C_1$ is a constant of integration.\n\n2. **Second Integration:**\n\n   Integrate again with respect to $x$:\n\n   \\[\n   \\int \\frac{dy}{dx} \\, dx = \\int C_1 \\, dx\n   \\]\n\n   This results in:\n\n   \\[\n   y = C_1 x + C_2\n   \\]\n\n   where $C_2$ is another constant of integration.\n\nThe solution $y = C_1 x + C_2$ is a linear function of $x$. Therefore, the general solution to the differential equation $\\frac{d^2y}{dx^2} = 0$ is a linear function, confirming that linear functions are indeed solutions to this differential equation."
    },
    {
      "id": "4",
      "statement": "The solution to the differential equation dy/dx = 1 is a linear function with a slope of 1.",
      "proof": "To solve the differential equation $\\frac{dy}{dx} = 1$, we need to find a function $y(x)$ such that its derivative with respect to $x$ is equal to 1.\n\nThe differential equation can be rewritten as:\n\n\\[\n\\frac{dy}{dx} = 1\n\\]\n\nTo solve this, we integrate both sides with respect to $x$:\n\n\\[\n\\int \\frac{dy}{dx} \\, dx = \\int 1 \\, dx\n\\]\n\nThe left-hand side simplifies to $y(x)$, and the right-hand side integrates to $x + C$, where $C$ is the constant of integration. Therefore, we have:\n\n\\[\ny(x) = x + C\n\\]\n\nThis is the general solution to the differential equation. It represents a family of linear functions with a slope of 1. The constant $C$ determines the vertical shift of the line, but the slope remains 1 for all values of $C$. Thus, the solution to the differential equation $\\frac{dy}{dx} = 1$ is indeed a linear function with a slope of 1."
    },
    {
      "id": "5",
      "statement": "If y(x) = x^2 is a solution to a differential equation, then y'(x) = 2x is part of the equation.",
      "proof": "To prove this statement, we need to show that if $y(x) = x^2$ is a solution to a differential equation, then its derivative $y'(x) = 2x$ must appear in the differential equation.\n\nLet's start by differentiating the given function $y(x) = x^2$ with respect to $x$:\n\n\\[\ny'(x) = \\frac{d}{dx}(x^2) = 2x.\n\\]\n\nNow, consider a general first-order differential equation of the form:\n\n\\[\nF(x, y, y') = 0.\n\\]\n\nSince $y(x) = x^2$ is a solution to this differential equation, substituting $y = x^2$ and $y' = 2x$ into the equation should satisfy it:\n\n\\[\nF(x, x^2, 2x) = 0.\n\\]\n\nThis implies that the derivative $y' = 2x$ is indeed part of the differential equation, as it is necessary for the equation to hold true when $y(x) = x^2$ is a solution.\n\nTherefore, if $y(x) = x^2$ is a solution to a differential equation, then $y'(x) = 2x$ must be part of the equation, as it is required to satisfy the differential equation with the given solution."
    },
    {
      "id": "6",
      "statement": "The differential equation dy/dx = 2x has a general solution involving x^2.",
      "proof": "To solve the differential equation $\\frac{dy}{dx} = 2x$, we need to find a function $y(x)$ whose derivative with respect to $x$ is $2x$. \n\nThe given differential equation is separable, and we can integrate both sides with respect to $x$:\n\n\\[\n\\int \\frac{dy}{dx} \\, dx = \\int 2x \\, dx.\n\\]\n\nThe left-hand side simplifies to:\n\n\\[\n\\int \\frac{dy}{dx} \\, dx = \\int dy = y.\n\\]\n\nThe right-hand side is:\n\n\\[\n\\int 2x \\, dx = 2 \\int x \\, dx = 2 \\left( \\frac{x^2}{2} \\right) + C = x^2 + C,\n\\]\n\nwhere $C$ is the constant of integration.\n\nThus, the general solution to the differential equation $\\frac{dy}{dx} = 2x$ is:\n\n\\[\ny = x^2 + C.\n\\]\n\nThis solution involves $x^2$, as required."
    },
    {
      "id": "7",
      "statement": "If y(x) = e^x is a solution to a differential equation, then the derivative of y(x) is also e^x.",
      "proof": "To prove that if $y(x) = e^x$ is a solution to a differential equation, then the derivative of $y(x)$ is also $e^x$, we need to differentiate $y(x)$ with respect to $x$.\n\nGiven the function:\n\n\\[ y(x) = e^x \\]\n\nwe differentiate both sides with respect to $x$:\n\n\\[ \\frac{dy}{dx} = \\frac{d}{dx}(e^x) \\]\n\nThe derivative of $e^x$ with respect to $x$ is a well-known result:\n\n\\[ \\frac{d}{dx}(e^x) = e^x \\]\n\nThus, we have:\n\n\\[ \\frac{dy}{dx} = e^x \\]\n\nThis shows that the derivative of $y(x) = e^x$ is indeed $e^x$. Therefore, if $y(x) = e^x$ is a solution to a differential equation, its derivative is also $e^x$."
    },
    {
      "id": "8",
      "statement": "Any solution to the differential equation dy/dx = 3 has a slope of 3 at every point.",
      "proof": "To prove that any solution to the differential equation $\\frac{dy}{dx} = 3$ has a slope of 3 at every point, we start by considering the given differential equation:\n\n\\[\n\\frac{dy}{dx} = 3\n\\]\n\nThis equation states that the derivative of $y$ with respect to $x$ is constant and equal to 3. The derivative $\\frac{dy}{dx}$ represents the slope of the tangent line to the curve $y(x)$ at any point $(x, y)$.\n\nTo find the general solution to this differential equation, we integrate both sides with respect to $x$:\n\n\\[\n\\int \\frac{dy}{dx} \\, dx = \\int 3 \\, dx\n\\]\n\nThis yields:\n\n\\[\ny(x) = 3x + C\n\\]\n\nwhere $C$ is the constant of integration. The function $y(x) = 3x + C$ is a family of linear functions, each with a slope of 3.\n\nSince the slope of a linear function $y = mx + b$ is given by the coefficient $m$ of $x$, we see that for $y(x) = 3x + C$, the slope is indeed 3 for all values of $x$.\n\nTherefore, any solution to the differential equation $\\frac{dy}{dx} = 3$ is a linear function with a constant slope of 3 at every point. This confirms that the statement is true."
    },
    {
      "id": "9",
      "statement": "A separable differential equation can be expressed as a product of a function of x and a function of y.",
      "proof": "To prove that a separable differential equation can be expressed as a product of a function of $x$ and a function of $y$, let's start with the definition of a separable differential equation.\n\nA first-order differential equation is said to be separable if it can be written in the form:\n\n\\[\n\\frac{dy}{dx} = g(x)h(y)\n\\]\n\nwhere $g(x)$ is a function of $x$ alone, and $h(y)$ is a function of $y$ alone.\n\nTo solve this equation, we can separate the variables by rearranging the terms:\n\n\\[\n\\frac{1}{h(y)} \\, dy = g(x) \\, dx\n\\]\n\nThis equation is now in a form where the variables are separated, with all terms involving $y$ on one side and all terms involving $x$ on the other side. We can integrate both sides to find the solution:\n\n\\[\n\\int \\frac{1}{h(y)} \\, dy = \\int g(x) \\, dx\n\\]\n\nLet the antiderivatives be:\n\n\\[\nH(y) = \\int \\frac{1}{h(y)} \\, dy \\quad \\text{and} \\quad G(x) = \\int g(x) \\, dx\n\\]\n\nThen the equation becomes:\n\n\\[\nH(y) = G(x) + C\n\\]\n\nwhere $C$ is the constant of integration. This equation implicitly defines $y$ as a function of $x$.\n\nThus, a separable differential equation can indeed be expressed as a product of a function of $x$ and a function of $y$, allowing us to separate the variables and integrate to find the solution."
    },
    {
      "id": "10",
      "statement": "The particular solution of a differential equation passes through a given point in the xy-plane.",
      "proof": "To find the particular solution of a differential equation that passes through a given point in the xy-plane, we need to follow these steps:\n\n1. **Solve the Differential Equation**: First, find the general solution of the differential equation. This solution will typically involve one or more arbitrary constants.\n\n2. **Apply the Initial Condition**: Use the given point to determine the specific values of the arbitrary constants in the general solution. This will give us the particular solution.\n\nLet's illustrate this process with an example.\n\n**Example:**\n\nConsider the first-order differential equation:\n\n\\[\n\\frac{dy}{dx} = 3x^2\n\\]\n\nWe are given that the particular solution passes through the point \\((1, 4)\\).\n\n**Step 1: Solve the Differential Equation**\n\nTo find the general solution, integrate both sides with respect to \\(x\\):\n\n\\[\ny = \\int 3x^2 \\, dx = x^3 + C\n\\]\n\nwhere \\(C\\) is the constant of integration.\n\n**Step 2: Apply the Initial Condition**\n\nWe know that the solution passes through the point \\((1, 4)\\). This means when \\(x = 1\\), \\(y = 4\\). Substitute these values into the general solution to find \\(C\\):\n\n\\[\n4 = (1)^3 + C \\implies 4 = 1 + C \\implies C = 3\n\\]\n\n**Particular Solution**\n\nSubstitute \\(C = 3\\) back into the general solution:\n\n\\[\ny = x^3 + 3\n\\]\n\nThus, the particular solution of the differential equation that passes through the point \\((1, 4)\\) is:\n\n\\[\ny = x^3 + 3\n\\]"
    },
    {
      "id": "11",
      "statement": "The integration constant in the solution to a differential equation accounts for initial conditions.",
      "proof": "To prove that the integration constant in the solution to a differential equation accounts for initial conditions, let's consider a simple first-order differential equation:\n\n\\[\n\\frac{dy}{dx} = f(x)\n\\]\n\nTo solve this differential equation, we integrate both sides with respect to \\(x\\):\n\n\\[\ny(x) = \\int f(x) \\, dx + C\n\\]\n\nwhere \\(C\\) is the constant of integration. This constant arises because the process of integration is the inverse of differentiation, and differentiation of a constant yields zero. Therefore, when we integrate, we must include an arbitrary constant to account for all possible antiderivatives.\n\nNow, let's consider an initial condition given by:\n\n\\[\ny(x_0) = y_0\n\\]\n\nwhere \\(x_0\\) is a specific value of \\(x\\) and \\(y_0\\) is the corresponding value of \\(y\\). We use this initial condition to determine the value of the integration constant \\(C\\).\n\nSubstituting the initial condition into the general solution, we have:\n\n\\[\ny_0 = \\int f(x_0) \\, dx + C\n\\]\n\nSince the integral of a constant function is simply the function times the variable of integration, we can solve for \\(C\\):\n\n\\[\nC = y_0 - \\int f(x_0) \\, dx\n\\]\n\nThus, the integration constant \\(C\\) is determined by the initial condition, ensuring that the particular solution satisfies the given initial condition. This demonstrates that the integration constant accounts for initial conditions in the solution to a differential equation."
    },
    {
      "id": "12",
      "statement": "The exponential function is often a solution to linear differential equations with constant coefficients.",
      "proof": "To show that the exponential function is often a solution to linear differential equations with constant coefficients, consider a linear differential equation of the form:\n\n\\[\na_n \\frac{d^n y}{dx^n} + a_{n-1} \\frac{d^{n-1} y}{dx^{n-1}} + \\cdots + a_1 \\frac{dy}{dx} + a_0 y = 0\n\\]\n\nwhere $a_0, a_1, \\ldots, a_n$ are constants.\n\nAssume a solution of the form $y(x) = e^{rx}$, where $r$ is a constant to be determined. The derivatives of $y(x)$ are:\n\n\\[\n\\frac{dy}{dx} = re^{rx}, \\quad \\frac{d^2 y}{dx^2} = r^2 e^{rx}, \\quad \\ldots, \\quad \\frac{d^n y}{dx^n} = r^n e^{rx}\n\\]\n\nSubstituting these into the differential equation gives:\n\n\\[\na_n r^n e^{rx} + a_{n-1} r^{n-1} e^{rx} + \\cdots + a_1 r e^{rx} + a_0 e^{rx} = 0\n\\]\n\nFactoring out $e^{rx}$ (which is never zero for any real or complex $r$), we obtain:\n\n\\[\ne^{rx} (a_n r^n + a_{n-1} r^{n-1} + \\cdots + a_1 r + a_0) = 0\n\\]\n\nFor this equation to hold for all $x$, the polynomial in $r$ must be zero:\n\n\\[\na_n r^n + a_{n-1} r^{n-1} + \\cdots + a_1 r + a_0 = 0\n\\]\n\nThis is known as the characteristic equation of the differential equation. The roots of this polynomial, $r_1, r_2, \\ldots, r_n$, determine the form of the solutions. Each root $r_i$ corresponds to a solution of the form $e^{r_i x}$.\n\nThus, the exponential function $e^{rx}$ is a solution to the linear differential equation with constant coefficients, where $r$ is a root of the characteristic equation. If the roots are distinct, the general solution is a linear combination of these exponential functions:\n\n\\[\ny(x) = c_1 e^{r_1 x} + c_2 e^{r_2 x} + \\cdots + c_n e^{r_n x}\n\\]\n\nwhere $c_1, c_2, \\ldots, c_n$ are constants determined by initial conditions. If there are repeated roots, the solution will include terms of the form $x^k e^{r_i x}$ for the $k$-th repeated root."
    },
    {
      "id": "13",
      "statement": "An equilibrium solution of a differential equation does not change over time.",
      "proof": "To prove that an equilibrium solution of a differential equation does not change over time, let's consider a first-order ordinary differential equation of the form:\n\n$$ \\frac{dy}{dt} = f(y). $$\n\nAn equilibrium solution, also known as a steady-state solution, is a constant solution $y(t) = y_e$ such that the derivative $\\frac{dy}{dt}$ is zero. This means that the function $f(y)$ must satisfy:\n\n$$ f(y_e) = 0. $$\n\nTo show that $y(t) = y_e$ does not change over time, we substitute $y(t) = y_e$ into the differential equation:\n\n$$ \\frac{dy}{dt} = f(y_e) = 0. $$\n\nSince $\\frac{dy}{dt} = 0$, the rate of change of $y(t)$ with respect to time $t$ is zero. This implies that $y(t)$ is constant over time, and thus $y(t) = y_e$ for all $t$. Therefore, the equilibrium solution $y_e$ does not change over time, confirming that it is indeed a steady-state solution."
    },
    {
      "id": "14",
      "statement": "A first-order linear differential equation typically involves the first derivative of a function.",
      "proof": "A first-order linear differential equation can be expressed in the standard form:\n\n\\[\n\\frac{dy}{dx} + P(x)y = Q(x)\n\\]\n\nwhere \\( y \\) is the dependent variable, \\( x \\) is the independent variable, and \\( P(x) \\) and \\( Q(x) \\) are functions of \\( x \\).\n\nTo solve this equation, we use an integrating factor, which is given by:\n\n\\[\n\\mu(x) = e^{\\int P(x) \\, dx}\n\\]\n\nMultiplying the entire differential equation by this integrating factor transforms it into an exact equation:\n\n\\[\n\\mu(x) \\frac{dy}{dx} + \\mu(x) P(x) y = \\mu(x) Q(x)\n\\]\n\nThe left-hand side of this equation can be rewritten as the derivative of the product \\(\\mu(x) y\\):\n\n\\[\n\\frac{d}{dx} \\left( \\mu(x) y \\right) = \\mu(x) Q(x)\n\\]\n\nIntegrating both sides with respect to \\( x \\) gives:\n\n\\[\n\\mu(x) y = \\int \\mu(x) Q(x) \\, dx + C\n\\]\n\nwhere \\( C \\) is the constant of integration. Solving for \\( y \\), we obtain:\n\n\\[\ny = \\frac{1}{\\mu(x)} \\left( \\int \\mu(x) Q(x) \\, dx + C \\right)\n\\]\n\nThis is the general solution to the first-order linear differential equation. The specific solution depends on the initial conditions provided."
    },
    {
      "id": "15",
      "statement": "The method of separation of variables can be used to solve some first-order differential equations.",
      "proof": "The method of separation of variables is a technique used to solve certain first-order ordinary differential equations (ODEs) that can be expressed in the form:\n\n\\[\n\\frac{dy}{dx} = g(x)h(y)\n\\]\n\nThe key idea is to rearrange the equation so that all terms involving \\(y\\) are on one side and all terms involving \\(x\\) are on the other side. This allows us to integrate both sides separately. Here is a step-by-step proof of the method:\n\n1. **Separate the Variables:**\n\n   Start with the differential equation:\n\n   \\[\n   \\frac{dy}{dx} = g(x)h(y)\n   \\]\n\n   Rearrange the equation to separate the variables:\n\n   \\[\n   \\frac{1}{h(y)} \\, dy = g(x) \\, dx\n   \\]\n\n2. **Integrate Both Sides:**\n\n   Integrate both sides with respect to their respective variables:\n\n   \\[\n   \\int \\frac{1}{h(y)} \\, dy = \\int g(x) \\, dx\n   \\]\n\n   Let \\(H(y)\\) be an antiderivative of \\(\\frac{1}{h(y)}\\), and \\(G(x)\\) be an antiderivative of \\(g(x)\\). Then we have:\n\n   \\[\n   H(y) = G(x) + C\n   \\]\n\n   where \\(C\\) is the constant of integration.\n\n3. **Solve for \\(y\\):**\n\n   If possible, solve the equation \\(H(y) = G(x) + C\\) for \\(y\\) to find the explicit solution of the differential equation. The solution will depend on the specific forms of \\(H(y)\\) and \\(G(x)\\).\n\n4. **Example:**\n\n   Consider the differential equation:\n\n   \\[\n   \\frac{dy}{dx} = xy\n   \\]\n\n   Separate the variables:\n\n   \\[\n   \\frac{1}{y} \\, dy = x \\, dx\n   \\]\n\n   Integrate both sides:\n\n   \\[\n   \\int \\frac{1}{y} \\, dy = \\int x \\, dx\n   \\]\n\n   This gives:\n\n   \\[\n   \\ln |y| = \\frac{x^2}{2} + C\n   \\]\n\n   Solve for \\(y\\):\n\n   \\[\n   |y| = e^{\\frac{x^2}{2} + C} = e^C e^{\\frac{x^2}{2}}\n   \\]\n\n   Let \\(C_1 = e^C\\), then:\n\n   \\[\n   y = \\pm C_1 e^{\\frac{x^2}{2}}\n   \\]\n\n   The general solution is:\n\n   \\[\n   y = C e^{\\frac{x^2}{2}}\n   \\]\n\n   where \\(C\\) is an arbitrary constant.\n\nThis method is applicable when the differential equation can be expressed in a form that allows the separation of variables. If the equation cannot be separated, other methods must be used to find the solution."
    }
  ],
  "Abstract Algebra": [
    {
      "id": "1",
      "statement": "The identity element in a group leaves any element unchanged when used in the operation.",
      "proof": "To prove that the identity element in a group leaves any element unchanged when used in the operation, we need to refer to the definition of a group and the properties of the identity element.\n\nLet $(G, \\cdot)$ be a group, where $G$ is a set and $\\cdot$ is a binary operation on $G$. By definition, a group must satisfy the following properties:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity Element**: There exists an element $e \\in G$ such that for every element $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse Element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element.\n\nWe are interested in the property of the identity element. According to the definition, the identity element $e \\in G$ satisfies:\n\n\\[ e \\cdot a = a \\cdot e = a \\]\n\nfor every element $a \\in G$. This means that when the identity element is used in the operation with any element $a$ of the group, it leaves $a$ unchanged.\n\n**Proof:**\n\nLet $a \\in G$ be any element of the group. By the definition of the identity element, we have:\n\n1. $e \\cdot a = a$: This shows that when the identity element $e$ is on the left of $a$, the result is $a$.\n2. $a \\cdot e = a$: This shows that when the identity element $e$ is on the right of $a$, the result is also $a$.\n\nThus, in both cases, the identity element $e$ leaves the element $a$ unchanged when used in the operation. Therefore, the identity element in a group indeed leaves any element unchanged when used in the operation. This completes the proof."
    },
    {
      "id": "2",
      "statement": "For a set with a binary operation to be a group, every element must have an inverse in that set.",
      "proof": "To prove that for a set with a binary operation to be a group, every element must have an inverse in that set, we need to recall the definition of a group. A group $(G, \\cdot)$ is a set $G$ equipped with a binary operation $\\cdot$ that satisfies the following four axioms:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity Element**: There exists an element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse Element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element.\n\nWe need to show that the fourth axiom, the existence of an inverse for every element, is necessary for $(G, \\cdot)$ to be a group.\n\n**Proof:**\n\nAssume $(G, \\cdot)$ is a group. By definition, it satisfies the four axioms listed above. We focus on the fourth axiom, which states that for every element $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element of the group.\n\n1. **Existence of Inverses**: \n   - Let $a \\in G$. By the group definition, there must exist an element $b \\in G$ such that $a \\cdot b = e$ and $b \\cdot a = e$. This element $b$ is called the inverse of $a$, and it is denoted by $a^{-1}$.\n   - The existence of such an inverse is crucial because it allows us to \"undo\" the operation. For example, if we have $a \\cdot b = c$, we can multiply both sides by $a^{-1}$ to obtain $b = a^{-1} \\cdot c$, effectively \"cancelling\" $a$.\n\n2. **Necessity of Inverses**:\n   - If there were an element $a \\in G$ without an inverse, then for such an $a$, there would be no element $b \\in G$ such that $a \\cdot b = e$. This would violate the group axiom that requires every element to have an inverse.\n   - Without inverses, we cannot guarantee the ability to solve equations of the form $a \\cdot x = b$ for $x \\in G$, which is a fundamental property of groups.\n\nTherefore, the existence of an inverse for every element is not only part of the definition of a group but also essential for the structure and properties that define a group. Without inverses, the set with the binary operation cannot satisfy the group axioms, and thus cannot be considered a group."
    },
    {
      "id": "3",
      "statement": "The product of a group element and its inverse is the identity element.",
      "proof": "Let \\( G \\) be a group, and let \\( g \\in G \\) be an arbitrary element of the group. By the definition of a group, every element \\( g \\) in \\( G \\) has an inverse, denoted by \\( g^{-1} \\), such that the product of the element and its inverse is the identity element of the group.\n\nThe identity element of a group \\( G \\) is denoted by \\( e \\) and has the property that for any element \\( g \\in G \\), the following holds:\n\n\\[ g \\cdot e = e \\cdot g = g. \\]\n\nTo prove that the product of an element and its inverse is the identity element, we need to show:\n\n\\[ g \\cdot g^{-1} = e \\quad \\text{and} \\quad g^{-1} \\cdot g = e. \\]\n\n1. **Existence of Inverse:**\n\n   By the definition of a group, for each element \\( g \\in G \\), there exists an element \\( g^{-1} \\in G \\) such that:\n\n   \\[\n   g \\cdot g^{-1} = e \\quad \\text{and} \\quad g^{-1} \\cdot g = e.\n   \\]\n\n2. **Associativity:**\n\n   The operation in the group \\( G \\) is associative, meaning for any elements \\( a, b, c \\in G \\), we have:\n\n   \\[\n   (a \\cdot b) \\cdot c = a \\cdot (b \\cdot c).\n   \\]\n\n3. **Identity Element:**\n\n   The identity element \\( e \\) satisfies:\n\n   \\[\n   g \\cdot e = g \\quad \\text{and} \\quad e \\cdot g = g \\quad \\text{for all } g \\in G.\n   \\]\n\n4. **Verification:**\n\n   By the definition of the inverse, we have:\n\n   \\[\n   g \\cdot g^{-1} = e.\n   \\]\n\n   This shows that the product of an element \\( g \\) and its inverse \\( g^{-1} \\) is indeed the identity element \\( e \\).\n\nTherefore, the product of a group element and its inverse is the identity element, as required by the group axioms. This completes the proof."
    },
    {
      "id": "4",
      "statement": "A set with a binary operation that is associative and has an identity element but not necessarily inverses is called a monoid.",
      "proof": "To prove that a set with a binary operation that is associative and has an identity element is a monoid, we need to verify the definition of a monoid.\n\n**Definition of a Monoid:**\n\nA monoid is a set $M$ equipped with a binary operation $\\ast: M \\times M \\to M$ that satisfies the following properties:\n\n1. **Associativity:** For all $a, b, c \\in M$, we have $(a \\ast b) \\ast c = a \\ast (b \\ast c)$.\n2. **Identity Element:** There exists an element $e \\in M$ such that for all $a \\in M$, $e \\ast a = a \\ast e = a$.\n\n**Proof:**\n\nLet $M$ be a set with a binary operation $\\ast$ that is associative and has an identity element.\n\n1. **Associativity:** By assumption, the operation $\\ast$ is associative. This means for all $a, b, c \\in M$, the equation $(a \\ast b) \\ast c = a \\ast (b \\ast c)$ holds.\n\n2. **Identity Element:** By assumption, there exists an element $e \\in M$ such that for all $a \\in M$, $e \\ast a = a \\ast e = a$. This element $e$ is called the identity element of the monoid.\n\nSince both properties required for a monoid are satisfied, the set $M$ with the binary operation $\\ast$ is indeed a monoid.\n\nThus, we have shown that a set with a binary operation that is associative and has an identity element is a monoid."
    },
    {
      "id": "5",
      "statement": "If you take a ring and restrict it to its elements that are multiples of a fixed integer, this subset is called an ideal.",
      "proof": "To prove that the set of multiples of a fixed integer $n$ in a ring $R$ forms an ideal, we need to show that this set satisfies the two defining properties of an ideal in a ring:\n\n1. It is a subgroup of the additive group of the ring $R$.\n2. It is closed under multiplication by any element of the ring $R$.\n\nLet $I = \\{ rn \\mid r \\in R \\}$ be the set of all multiples of a fixed integer $n$ in the ring $R$. We will show that $I$ is an ideal of $R$.\n\n### Step 1: $I$ is a subgroup of the additive group of $R$.\n\nTo show that $I$ is a subgroup, we need to verify the following:\n\n- **Closure under addition**: If $a, b \\in I$, then $a = rn$ and $b = sn$ for some $r, s \\in R$. Thus, $a + b = rn + sn = (r+s)n$, which is in $I$ because $r+s \\in R$.\n\n- **Existence of additive identity**: The additive identity in $R$ is $0$, and $0 = 0 \\cdot n \\in I$.\n\n- **Existence of additive inverses**: If $a \\in I$, then $a = rn$ for some $r \\in R$. The additive inverse of $a$ is $-a = -rn = (-r)n$, which is in $I$ because $-r \\in R$.\n\nSince $I$ satisfies closure under addition, contains the additive identity, and contains additive inverses, $I$ is a subgroup of the additive group of $R$.\n\n### Step 2: $I$ is closed under multiplication by any element of $R$.\n\nTo show this, let $a \\in I$ and $r \\in R$. Then $a = sn$ for some $s \\in R$. We need to show that $ra \\in I$. We have:\n\n\\[ ra = r(sn) = (rs)n. \\]\n\nSince $rs \\in R$, it follows that $ra \\in I$.\n\n### Conclusion\n\nSince $I$ is a subgroup of the additive group of $R$ and is closed under multiplication by any element of $R$, $I$ is an ideal of the ring $R$. Thus, the set of multiples of a fixed integer $n$ in a ring $R$ forms an ideal."
    },
    {
      "id": "6",
      "statement": "A vector space over a field has a zero vector that, when added to any vector in the space, returns that vector.",
      "proof": "To prove that a vector space $V$ over a field $F$ has a zero vector that, when added to any vector in the space, returns that vector, we need to show the existence of an element $0_V \\in V$ such that for any vector $v \\in V$, the equation $v + 0_V = v$ holds.\n\n**Proof:**\n\nBy definition, a vector space $V$ over a field $F$ is a set equipped with two operations: vector addition and scalar multiplication, satisfying certain axioms. One of these axioms is the existence of an additive identity, often called the zero vector.\n\nThe axiom states:\n\n- There exists an element $0_V \\in V$ such that for every vector $v \\in V$, the equation $v + 0_V = v$ holds.\n\nTo prove this, we rely on the axioms of a vector space:\n\n1. **Existence of the Zero Vector:** By the definition of a vector space, there must exist a zero vector $0_V \\in V$ such that for every vector $v \\in V$, $v + 0_V = v$. This is one of the fundamental axioms of vector spaces.\n\n2. **Uniqueness of the Zero Vector:** Suppose there are two zero vectors, $0_V$ and $0_V'$, in $V$. By the definition of a zero vector, we have:\n   - $v + 0_V = v$ for all $v \\in V$.\n   - $v + 0_V' = v$ for all $v \\in V$.\n\n   Consider the vector $0_V$. By the property of the zero vector, we have:\n   \\[\n   0_V + 0_V' = 0_V\n   \\]\n   and also\n   \\[\n   0_V + 0_V' = 0_V'.\n   \\]\n   Therefore, $0_V = 0_V'$. This shows that the zero vector is unique.\n\nThus, the existence and uniqueness of the zero vector in a vector space $V$ over a field $F$ are guaranteed by the axioms of vector spaces. The zero vector $0_V$ satisfies the property that for any vector $v \\in V$, $v + 0_V = v$."
    },
    {
      "id": "7",
      "statement": "The only element in an abelian group that is both the identity element and its own inverse is the identity element itself.",
      "proof": "To prove this statement, let us consider an abelian group $(G, \\cdot)$ with identity element $e$. We want to show that if an element $x \\in G$ is both the identity element and its own inverse, then $x = e$.\n\n1. **Identity Element Property**: By definition, the identity element $e$ in a group satisfies $e \\cdot g = g \\cdot e = g$ for all $g \\in G$.\n\n2. **Inverse Element Property**: An element $x \\in G$ is its own inverse if $x \\cdot x = e$.\n\nNow, assume $x$ is both the identity element and its own inverse. We need to show $x = e$.\n\n- Since $x$ is the identity element, for any $g \\in G$, we have:\n  \\[\n  x \\cdot g = g \\cdot x = g\n  \\]\n\n- Since $x$ is its own inverse, we have:\n  \\[\n  x \\cdot x = e\n  \\]\n\nNow, consider the identity element $e$. By the property of the identity element, we have:\n\\[\ne \\cdot e = e\n\\]\n\nSince $x$ is the identity element, $x = e$. Therefore, $x \\cdot x = e \\cdot e = e$. This is consistent with the property of $x$ being its own inverse.\n\nThus, the only element in an abelian group that is both the identity element and its own inverse is the identity element itself, i.e., $x = e$. \n\nThis completes the proof."
    },
    {
      "id": "8",
      "statement": "In a field, every non-zero element has a multiplicative inverse.",
      "proof": "To prove that in a field every non-zero element has a multiplicative inverse, we start by recalling the definition of a field. A field $F$ is a set equipped with two operations: addition and multiplication, satisfying the following properties:\n\n1. **Additive Identity**: There exists an element $0 \\in F$ such that for all $a \\in F$, $a + 0 = a$.\n2. **Additive Inverses**: For each $a \\in F$, there exists an element $-a \\in F$ such that $a + (-a) = 0$.\n3. **Multiplicative Identity**: There exists an element $1 \\in F$, $1 \\neq 0$, such that for all $a \\in F$, $a \\cdot 1 = a$.\n4. **Multiplicative Inverses**: For each non-zero element $a \\in F$, there exists an element $a^{-1} \\in F$ such that $a \\cdot a^{-1} = 1$.\n5. **Associativity and Commutativity**: Both addition and multiplication are associative and commutative.\n6. **Distributive Law**: Multiplication is distributive over addition, i.e., for all $a, b, c \\in F$, $a \\cdot (b + c) = a \\cdot b + a \\cdot c$.\n\nWe need to show that for every non-zero element $a \\in F$, there exists an element $a^{-1} \\in F$ such that $a \\cdot a^{-1} = 1$.\n\n**Proof:**\n\nLet $a \\in F$ be a non-zero element. By the definition of a field, we are guaranteed the existence of a multiplicative identity $1 \\in F$. The field axioms also ensure that the set $F \\setminus \\{0\\}$, which is the set of all non-zero elements of $F$, forms a group under multiplication. This is because:\n\n- The operation is associative and commutative (inherited from the field properties).\n- The identity element $1$ is in $F \\setminus \\{0\\}$.\n- For each non-zero element $a \\in F$, there exists an inverse $a^{-1} \\in F$ such that $a \\cdot a^{-1} = 1$.\n\nThus, by the definition of a group, every non-zero element $a \\in F$ must have a multiplicative inverse $a^{-1} \\in F$.\n\nTherefore, in a field, every non-zero element has a multiplicative inverse, as required."
    },
    {
      "id": "9",
      "statement": "A homomorphism between two groups preserves the group operation.",
      "proof": "To prove that a homomorphism between two groups preserves the group operation, let's consider two groups $(G, \\cdot)$ and $(H, *)$, and a function $\\phi: G \\to H$ that is a homomorphism. By definition, a homomorphism satisfies the property:\n\n\\[\n\\phi(a \\cdot b) = \\phi(a) * \\phi(b)\n\\]\n\nfor all $a, b \\in G$. We need to show that this property implies that the group operation is preserved under the mapping $\\phi$.\n\n**Proof:**\n\n1. **Closure under the operation:**\n\n   Since $G$ is a group, for any $a, b \\in G$, the product $a \\cdot b$ is also in $G$. The homomorphism $\\phi$ maps this product to $H$, and by the homomorphism property, we have:\n\n   \\[\n   \\phi(a \\cdot b) = \\phi(a) * \\phi(b)\n   \\]\n\n   This shows that the image of the product $a \\cdot b$ under $\\phi$ is the same as the product of the images $\\phi(a)$ and $\\phi(b)$ in $H$.\n\n2. **Identity element:**\n\n   Let $e_G$ be the identity element in $G$. Then, by the homomorphism property:\n\n   \\[\n   \\phi(e_G \\cdot e_G) = \\phi(e_G) * \\phi(e_G)\n   \\]\n\n   Since $e_G \\cdot e_G = e_G$, we have:\n\n   \\[\n   \\phi(e_G) = \\phi(e_G) * \\phi(e_G)\n   \\]\n\n   This implies that $\\phi(e_G)$ is the identity element in $H$, denoted by $e_H$, because the only element in a group that satisfies $x * x = x$ is the identity element.\n\n3. **Inverses:**\n\n   For any $a \\in G$, let $a^{-1}$ be its inverse in $G$. Then:\n\n   \\[\n   \\phi(a \\cdot a^{-1}) = \\phi(e_G) = e_H\n   \\]\n\n   By the homomorphism property:\n\n   \\[\n   \\phi(a) * \\phi(a^{-1}) = e_H\n   \\]\n\n   This shows that $\\phi(a^{-1})$ is the inverse of $\\phi(a)$ in $H$.\n\nThus, a homomorphism $\\phi: G \\to H$ preserves the group operation, the identity element, and inverses, which means it respects the group structure. Therefore, $\\phi$ is a structure-preserving map between the groups $G$ and $H$."
    },
    {
      "id": "10",
      "statement": "The kernel of a group homomorphism is a normal subgroup of the domain group.",
      "proof": "To prove that the kernel of a group homomorphism is a normal subgroup of the domain group, let's first recall some definitions:\n\n- Let $G$ and $H$ be groups, and let $\\phi: G \\to H$ be a group homomorphism.\n- The kernel of $\\phi$, denoted $\\ker(\\phi)$, is defined as $\\ker(\\phi) = \\{ g \\in G \\mid \\phi(g) = e_H \\}$, where $e_H$ is the identity element of $H$.\n\nWe need to show that $\\ker(\\phi)$ is a normal subgroup of $G$. This involves two steps:\n\n1. **Show that $\\ker(\\phi)$ is a subgroup of $G$:**\n\n   - **Identity:** Since $\\phi$ is a homomorphism, $\\phi(e_G) = e_H$, where $e_G$ is the identity element of $G$. Thus, $e_G \\in \\ker(\\phi)$.\n   \n   - **Closure:** Let $a, b \\in \\ker(\\phi)$. Then $\\phi(a) = e_H$ and $\\phi(b) = e_H$. We need to show that $ab \\in \\ker(\\phi)$. Since $\\phi$ is a homomorphism, $\\phi(ab) = \\phi(a)\\phi(b) = e_H e_H = e_H$. Thus, $ab \\in \\ker(\\phi)$.\n   \n   - **Inverses:** Let $a \\in \\ker(\\phi)$. Then $\\phi(a) = e_H$. We need to show that $a^{-1} \\in \\ker(\\phi)$. Since $\\phi$ is a homomorphism, $\\phi(a^{-1}) = \\phi(a)^{-1} = e_H^{-1} = e_H$. Thus, $a^{-1} \\in \\ker(\\phi)$.\n\n   Therefore, $\\ker(\\phi)$ is a subgroup of $G$.\n\n2. **Show that $\\ker(\\phi)$ is a normal subgroup of $G$:**\n\n   A subgroup $N$ of $G$ is normal if for all $g \\in G$ and $n \\in N$, the element $gng^{-1} \\in N$. We need to show that for all $g \\in G$ and $k \\in \\ker(\\phi)$, the element $gkg^{-1} \\in \\ker(\\phi)$.\n\n   - Let $g \\in G$ and $k \\in \\ker(\\phi)$. Then $\\phi(k) = e_H$.\n   - Consider the element $gkg^{-1}$. We need to show that $\\phi(gkg^{-1}) = e_H$.\n   - Since $\\phi$ is a homomorphism, $\\phi(gkg^{-1}) = \\phi(g)\\phi(k)\\phi(g^{-1})$.\n   - We know $\\phi(k) = e_H$, so $\\phi(gkg^{-1}) = \\phi(g)e_H\\phi(g^{-1}) = \\phi(g)\\phi(g^{-1})$.\n   - Since $\\phi$ is a homomorphism, $\\phi(g)\\phi(g^{-1}) = \\phi(gg^{-1}) = \\phi(e_G) = e_H$.\n\n   Therefore, $gkg^{-1} \\in \\ker(\\phi)$ for all $g \\in G$ and $k \\in \\ker(\\phi)$, which means $\\ker(\\phi)$ is a normal subgroup of $G$.\n\nThus, we have shown that the kernel of a group homomorphism is a normal subgroup of the domain group."
    },
    {
      "id": "11",
      "statement": "The set of real numbers forms a field with respect to addition and multiplication.",
      "proof": "To prove that the set of real numbers $\\mathbb{R}$ forms a field with respect to addition and multiplication, we need to verify that $\\mathbb{R}$ satisfies the field axioms. A field is a set equipped with two operations (addition and multiplication) that satisfy the following properties:\n\n1. **Closure**: \n   - For addition: For all $a, b \\in \\mathbb{R}$, $a + b \\in \\mathbb{R}$.\n   - For multiplication: For all $a, b \\in \\mathbb{R}$, $a \\cdot b \\in \\mathbb{R}$.\n\n2. **Associativity**:\n   - For addition: For all $a, b, c \\in \\mathbb{R}$, $(a + b) + c = a + (b + c)$.\n   - For multiplication: For all $a, b, c \\in \\mathbb{R}$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n\n3. **Commutativity**:\n   - For addition: For all $a, b \\in \\mathbb{R}$, $a + b = b + a$.\n   - For multiplication: For all $a, b \\in \\mathbb{R}$, $a \\cdot b = b \\cdot a$.\n\n4. **Identity elements**:\n   - For addition: There exists an element $0 \\in \\mathbb{R}$ such that for all $a \\in \\mathbb{R}$, $a + 0 = a$.\n   - For multiplication: There exists an element $1 \\in \\mathbb{R}$ (where $1 \\neq 0$) such that for all $a \\in \\mathbb{R}$, $a \\cdot 1 = a$.\n\n5. **Inverses**:\n   - For addition: For each $a \\in \\mathbb{R}$, there exists an element $-a \\in \\mathbb{R}$ such that $a + (-a) = 0$.\n   - For multiplication: For each $a \\in \\mathbb{R}$, $a \\neq 0$, there exists an element $a^{-1} \\in \\mathbb{R}$ such that $a \\cdot a^{-1} = 1$.\n\n6. **Distributive property**:\n   - For all $a, b, c \\in \\mathbb{R}$, $a \\cdot (b + c) = a \\cdot b + a \\cdot c$.\n\nNow, let's verify these properties for the set of real numbers $\\mathbb{R}$:\n\n1. **Closure**: The sum and product of any two real numbers are real numbers. This is a fundamental property of real numbers.\n\n2. **Associativity**: Addition and multiplication of real numbers are associative. This is a well-known property of real numbers.\n\n3. **Commutativity**: Addition and multiplication of real numbers are commutative. This is also a well-known property of real numbers.\n\n4. **Identity elements**: \n   - The additive identity is $0$, since for any real number $a$, $a + 0 = a$.\n   - The multiplicative identity is $1$, since for any real number $a$, $a \\cdot 1 = a$.\n\n5. **Inverses**:\n   - For any real number $a$, the additive inverse is $-a$, since $a + (-a) = 0$.\n   - For any nonzero real number $a$, the multiplicative inverse is $\\frac{1}{a}$, since $a \\cdot \\frac{1}{a} = 1$.\n\n6. **Distributive property**: For all real numbers $a, b, c$, the distributive law holds: $a \\cdot (b + c) = a \\cdot b + a \\cdot c$.\n\nSince all these properties are satisfied, the set of real numbers $\\mathbb{R}$ forms a field with respect to addition and multiplication."
    },
    {
      "id": "12",
      "statement": "A subgroup of a group that is closed under the group operation and taking inverses forms a group itself.",
      "proof": "To prove that a subgroup of a group forms a group itself, we need to verify that it satisfies the group axioms: closure, associativity, identity, and inverses. Let's consider a group $(G, \\cdot)$ and a subset $H \\subseteq G$ that is closed under the group operation and taking inverses. We will show that $H$ is a group under the operation $\\cdot$.\n\n1. **Closure**: By assumption, $H$ is closed under the group operation. This means that for any $a, b \\in H$, the product $a \\cdot b \\in H$.\n\n2. **Associativity**: Since $H \\subseteq G$ and the operation $\\cdot$ is associative in $G$, it is also associative in $H$. That is, for any $a, b, c \\in H$, we have $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n\n3. **Identity**: Since $H$ is a subgroup of $G$, it must contain the identity element $e$ of $G$. Thus, for any $a \\in H$, we have $e \\cdot a = a \\cdot e = a$, satisfying the identity property.\n\n4. **Inverses**: By assumption, $H$ is closed under taking inverses. This means that for any $a \\in H$, the inverse $a^{-1} \\in H$. Thus, for any $a \\in H$, there exists an element $a^{-1} \\in H$ such that $a \\cdot a^{-1} = a^{-1} \\cdot a = e$.\n\nSince $H$ satisfies all the group axioms, $H$ is a group under the operation $\\cdot$. Therefore, a subgroup of a group that is closed under the group operation and taking inverses forms a group itself."
    },
    {
      "id": "13",
      "statement": "The set of integers forms a ring with respect to addition and multiplication.",
      "proof": "To prove that the set of integers $\\mathbb{Z}$ forms a ring with respect to addition and multiplication, we need to verify that $\\mathbb{Z}$ satisfies the axioms of a ring. A ring is a set equipped with two binary operations (usually called addition and multiplication) that satisfy the following properties:\n\n1. **Additive Closure**: For all $a, b \\in \\mathbb{Z}$, $a + b \\in \\mathbb{Z}$.\n2. **Additive Associativity**: For all $a, b, c \\in \\mathbb{Z}$, $(a + b) + c = a + (b + c)$.\n3. **Additive Identity**: There exists an element $0 \\in \\mathbb{Z}$ such that for all $a \\in \\mathbb{Z}$, $a + 0 = a$.\n4. **Additive Inverses**: For each $a \\in \\mathbb{Z}$, there exists an element $-a \\in \\mathbb{Z}$ such that $a + (-a) = 0$.\n5. **Additive Commutativity**: For all $a, b \\in \\mathbb{Z}$, $a + b = b + a$.\n6. **Multiplicative Closure**: For all $a, b \\in \\mathbb{Z}$, $a \\cdot b \\in \\mathbb{Z}$.\n7. **Multiplicative Associativity**: For all $a, b, c \\in \\mathbb{Z}$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n8. **Distributive Laws**: For all $a, b, c \\in \\mathbb{Z}$, $a \\cdot (b + c) = a \\cdot b + a \\cdot c$ and $(a + b) \\cdot c = a \\cdot c + b \\cdot c$.\n\nNow, let's verify these properties for $\\mathbb{Z}$:\n\n1. **Additive Closure**: The sum of any two integers is an integer. Thus, $\\mathbb{Z}$ is closed under addition.\n\n2. **Additive Associativity**: Addition of integers is associative. For any $a, b, c \\in \\mathbb{Z}$, $(a + b) + c = a + (b + c)$.\n\n3. **Additive Identity**: The integer $0$ serves as the additive identity since for any $a \\in \\mathbb{Z}$, $a + 0 = a$.\n\n4. **Additive Inverses**: For any integer $a$, the integer $-a$ is its additive inverse because $a + (-a) = 0$.\n\n5. **Additive Commutativity**: Addition of integers is commutative. For any $a, b \\in \\mathbb{Z}$, $a + b = b + a$.\n\n6. **Multiplicative Closure**: The product of any two integers is an integer. Thus, $\\mathbb{Z}$ is closed under multiplication.\n\n7. **Multiplicative Associativity**: Multiplication of integers is associative. For any $a, b, c \\in \\mathbb{Z}$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n\n8. **Distributive Laws**: Multiplication distributes over addition for integers. For any $a, b, c \\in \\mathbb{Z}$, $a \\cdot (b + c) = a \\cdot b + a \\cdot c$ and $(a + b) \\cdot c = a \\cdot c + b \\cdot c$.\n\nSince all these properties are satisfied, the set of integers $\\mathbb{Z}$ forms a ring with respect to addition and multiplication."
    },
    {
      "id": "14",
      "statement": "The multiplication of complex numbers is commutative, just like the multiplication of real numbers.",
      "proof": "To prove that the multiplication of complex numbers is commutative, we need to show that for any two complex numbers $z_1 = a + bi$ and $z_2 = c + di$, the product $z_1 z_2$ is equal to $z_2 z_1$. Here, $a, b, c, d$ are real numbers, and $i$ is the imaginary unit with the property $i^2 = -1$.\n\nThe product $z_1 z_2$ is given by:\n\n\\[\nz_1 z_2 = (a + bi)(c + di)\n\\]\n\nUsing the distributive property, we expand this product:\n\n\\[\nz_1 z_2 = a(c + di) + bi(c + di) = ac + adi + bci + bdi^2\n\\]\n\nSince $i^2 = -1$, we have $bdi^2 = -bd$. Therefore, the expression becomes:\n\n\\[\nz_1 z_2 = ac + adi + bci - bd\n\\]\n\nWe can rearrange the terms:\n\n\\[\nz_1 z_2 = (ac - bd) + (ad + bc)i\n\\]\n\nNow, let's compute $z_2 z_1$:\n\n\\[\nz_2 z_1 = (c + di)(a + bi)\n\\]\n\nAgain, using the distributive property, we expand this product:\n\n\\[\nz_2 z_1 = c(a + bi) + di(a + bi) = ca + cbi + dai + dbi^2\n\\]\n\nSubstituting $i^2 = -1$, we have $dbi^2 = -db$. Therefore, the expression becomes:\n\n\\[\nz_2 z_1 = ca + cbi + dai - db\n\\]\n\nRearranging the terms gives:\n\n\\[\nz_2 z_1 = (ca - db) + (cb + da)i\n\\]\n\nNotice that $ac = ca$ and $bd = db$, so $ac - bd = ca - db$. Similarly, $ad + bc = da + cb$. Therefore, we have:\n\n\\[\nz_1 z_2 = (ac - bd) + (ad + bc)i = (ca - db) + (cb + da)i = z_2 z_1\n\\]\n\nThus, $z_1 z_2 = z_2 z_1$, proving that the multiplication of complex numbers is commutative."
    },
    {
      "id": "15",
      "statement": "For any group, the order of the identity element is 1.",
      "proof": "To prove that the order of the identity element in any group is 1, we start by recalling the definition of the order of an element in a group.\n\nLet $G$ be a group with identity element $e$. The order of an element $g \\in G$ is the smallest positive integer $n$ such that $g^n = e$. If no such positive integer exists, the element is said to have infinite order.\n\nNow, consider the identity element $e \\in G$. We need to show that the smallest positive integer $n$ such that $e^n = e$ is $n = 1$.\n\n1. **Identity Property**: By the definition of the identity element, for any element $g \\in G$, we have $e \\cdot g = g \\cdot e = g$. In particular, for the identity element itself, we have $e \\cdot e = e$.\n\n2. **Order of the Identity**: Consider $e^1 = e$. This satisfies the condition for the order of an element, as $e^1 = e$. \n\n3. **Minimality**: We need to show that there is no smaller positive integer than 1 that satisfies this condition. Since 1 is the smallest positive integer, there cannot be a smaller positive integer $n$ such that $e^n = e$.\n\nTherefore, the order of the identity element $e$ is 1, as it is the smallest positive integer for which $e^n = e$. Thus, the order of the identity element in any group is indeed 1."
    }
  ],
  "Group Theory": [
    {
      "id": "1",
      "statement": "The identity element in a group leaves any element unchanged when operated with it.",
      "proof": "To prove that the identity element in a group leaves any element unchanged when operated with it, we need to refer to the definition of a group and the properties of the identity element.\n\nLet $(G, \\cdot)$ be a group, where $G$ is a set and $\\cdot$ is a binary operation on $G$. By definition, a group must satisfy the following properties:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity Element**: There exists an element $e \\in G$ such that for every element $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse Element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element.\n\nWe are interested in the third property, the identity element. \n\n**Proof:**\n\nLet $e \\in G$ be the identity element. By the definition of the identity element, for any element $a \\in G$, we have:\n\n\\[ e \\cdot a = a \\]\n\nand\n\n\\[ a \\cdot e = a. \\]\n\nThese equations show that when the identity element $e$ is operated with any element $a$ in the group $G$, the result is the element $a$ itself. Therefore, the identity element leaves any element unchanged when operated with it.\n\nThis completes the proof that the identity element in a group leaves any element unchanged when operated with it."
    },
    {
      "id": "2",
      "statement": "In any group, every element has an inverse.",
      "proof": "To prove that in any group, every element has an inverse, we start by recalling the definition of a group. A group $(G, \\cdot)$ is a set $G$ equipped with a binary operation $\\cdot$ that satisfies the following axioms:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity element**: There exists an element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element.\n\nWe need to show that the fourth axiom, the existence of an inverse, holds for every element in a group.\n\n**Proof:**\n\nLet $G$ be a group and let $a \\in G$. By the definition of a group, there exists an identity element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n\nWe need to show that there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$.\n\nBy the group axiom of inverses, for each element $a \\in G$, there exists an element $b \\in G$ such that:\n\n1. $a \\cdot b = e$ (right inverse)\n2. $b \\cdot a = e$ (left inverse)\n\nThe existence of such an element $b$ is guaranteed by the definition of a group. Therefore, for every element $a \\in G$, there exists an inverse element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$.\n\nThus, in any group, every element has an inverse. This completes the proof."
    },
    {
      "id": "3",
      "statement": "The operation in a group is associative.",
      "proof": "To prove that the operation in a group is associative, we need to refer to the definition of a group. A group $(G, \\cdot)$ is a set $G$ equipped with a binary operation $\\cdot$ that satisfies the following properties:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, the equation $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$ holds.\n3. **Identity element**: There exists an element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$, where $e$ is the identity element.\n\nThe associativity property is one of the axioms that define a group. Therefore, by definition, the operation in a group is associative. \n\nTo illustrate this with a proof, consider any three elements $a, b, c \\in G$. By the associativity axiom of a group, we have:\n\n$$(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c).$$\n\nThis equation holds for any choice of $a, b, c \\in G$, demonstrating that the operation $\\cdot$ is associative in the group $G$. \n\nThus, the associativity of the operation is an inherent property of the group structure, and it is guaranteed by the definition of a group."
    },
    {
      "id": "4",
      "statement": "For any element in a group, operating it with the identity element yields the element itself.",
      "proof": "Let \\( G \\) be a group with the binary operation denoted by \\(*\\), and let \\( e \\) be the identity element of \\( G \\). By the definition of a group, the identity element \\( e \\) has the property that for any element \\( a \\in G \\), the operation of \\( a \\) with \\( e \\) yields \\( a \\) itself. We need to show that:\n\n\\[ a * e = a \\quad \\text{and} \\quad e * a = a \\]\n\nfor all \\( a \\in G \\).\n\n**Proof:**\n\n1. **Left Identity:**\n\n   By the definition of the identity element in a group, for any element \\( a \\in G \\), the operation of \\( a \\) with the identity element \\( e \\) on the left must satisfy:\n\n   \\[\n   e * a = a\n   \\]\n\n2. **Right Identity:**\n\n   Similarly, the operation of \\( a \\) with the identity element \\( e \\) on the right must satisfy:\n\n   \\[\n   a * e = a\n   \\]\n\nSince both conditions are satisfied for any element \\( a \\in G \\), the identity element \\( e \\) indeed acts as an identity for the operation \\(*\\) in the group \\( G \\). Therefore, operating any element in a group with the identity element yields the element itself, as required."
    },
    {
      "id": "5",
      "statement": "If you operate an element with its inverse in a group, you get the identity element.",
      "proof": "To prove this statement, we need to use the definition of a group and the properties of group elements.\n\nLet $G$ be a group with a binary operation denoted by $\\cdot$. By definition, a group is a set equipped with an operation that satisfies the following properties:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity Element**: There exists an element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse Element**: For each $a \\in G$, there exists an element $a^{-1} \\in G$ such that $a \\cdot a^{-1} = a^{-1} \\cdot a = e$, where $e$ is the identity element.\n\nWe want to show that if you operate an element with its inverse, you get the identity element.\n\nLet $a \\in G$ be an arbitrary element. By the definition of a group, there exists an inverse element $a^{-1} \\in G$ such that:\n\n\\[ a \\cdot a^{-1} = e \\]\n\nand\n\n\\[ a^{-1} \\cdot a = e \\]\n\nwhere $e$ is the identity element of the group $G$.\n\nThus, operating an element $a$ with its inverse $a^{-1}$ results in the identity element $e$. This completes the proof."
    },
    {
      "id": "6",
      "statement": "In a group, the inverse of the inverse of an element is the element itself.",
      "proof": "To prove that in a group, the inverse of the inverse of an element is the element itself, we start by considering a group $G$ with an element $a \\in G$. By the definition of a group, every element $a$ in $G$ has an inverse, denoted by $a^{-1}$, such that:\n\n\\[ a \\cdot a^{-1} = e \\quad \\text{and} \\quad a^{-1} \\cdot a = e, \\]\n\nwhere $e$ is the identity element of the group $G$.\n\nWe need to show that the inverse of $a^{-1}$ is $a$. In other words, we need to show that:\n\n\\[ (a^{-1})^{-1} = a. \\]\n\nTo prove this, we will verify that $a$ satisfies the definition of the inverse for $a^{-1}$:\n\n1. We need to show that $a \\cdot a^{-1} = e$. This is true by the definition of the inverse of $a$.\n\n2. We also need to show that $a^{-1} \\cdot a = e$. This is also true by the definition of the inverse of $a$.\n\nSince $a$ satisfies both conditions to be the inverse of $a^{-1}$, we conclude that:\n\n\\[ (a^{-1})^{-1} = a. \\]\n\nThus, in a group, the inverse of the inverse of an element is the element itself."
    },
    {
      "id": "7",
      "statement": "If you have two elements in a group and you operate them in one order, you can just as easily operate them in the reverse order if the group is abelian.",
      "proof": "To prove this statement, we need to show that if a group $G$ is abelian, then for any two elements $a, b \\in G$, the operation $a \\cdot b$ is equal to $b \\cdot a$.\n\n**Proof:**\n\nBy definition, a group $G$ is called abelian (or commutative) if for all elements $a, b \\in G$, the group operation satisfies the commutative property:\n\n\\[\na \\cdot b = b \\cdot a\n\\]\n\nLet's consider two arbitrary elements $a$ and $b$ in the group $G$. Since $G$ is abelian, by the definition of an abelian group, we have:\n\n\\[\na \\cdot b = b \\cdot a\n\\]\n\nThis directly shows that the order in which we operate the elements $a$ and $b$ does not matter; the result is the same regardless of the order. Therefore, in an abelian group, the operation of any two elements can be performed in either order, and the result will be the same.\n\nThus, the statement is proven: if a group is abelian, then for any two elements in the group, the operation can be performed in either order."
    },
    {
      "id": "8",
      "statement": "The identity element in a group is unique.",
      "proof": "To prove that the identity element in a group is unique, let's consider a group \\( G \\) with a binary operation \\(*\\). By definition, a group must have an identity element, which we will denote as \\( e \\).\n\nSuppose, for the sake of contradiction, that there are two identity elements in the group, say \\( e \\) and \\( e' \\).\n\nBy the definition of an identity element, for any element \\( a \\in G \\), the following must hold:\n1. \\( e * a = a \\)\n2. \\( a * e = a \\)\n3. \\( e' * a = a \\)\n4. \\( a * e' = a \\)\n\nNow, let's consider the element \\( e \\) itself. Since \\( e' \\) is an identity element, we have:\n\\[ e' * e = e \\]\n\nSimilarly, since \\( e \\) is an identity element, we have:\n\\[ e * e' = e' \\]\n\nThus, we have:\n\\[ e' = e' * e = e \\]\n\nThis shows that \\( e' = e \\), which contradicts the assumption that there are two distinct identity elements. Therefore, the identity element in a group must be unique."
    },
    {
      "id": "9",
      "statement": "The inverse of the identity element in a group is the identity element itself.",
      "proof": "To prove that the inverse of the identity element in a group is the identity element itself, we start by recalling the definition of a group and the properties of the identity element.\n\nLet $G$ be a group with the binary operation $\\cdot$. The identity element in $G$, denoted by $e$, satisfies the property that for any element $a \\in G$, we have:\n\n\\[ e \\cdot a = a \\cdot e = a. \\]\n\nWe need to show that the inverse of the identity element $e$ is $e$ itself. By the definition of an inverse in a group, for any element $a \\in G$, there exists an element $a^{-1} \\in G$ such that:\n\n\\[ a \\cdot a^{-1} = a^{-1} \\cdot a = e. \\]\n\nNow, consider the identity element $e$. We need to find an element $x \\in G$ such that:\n\n\\[ e \\cdot x = x \\cdot e = e. \\]\n\nSince $e$ is the identity element, it satisfies:\n\n\\[ e \\cdot e = e \\quad \\text{and} \\quad e \\cdot e = e. \\]\n\nThus, $e$ itself satisfies the condition for being its own inverse:\n\n\\[ e \\cdot e = e \\quad \\text{and} \\quad e \\cdot e = e. \\]\n\nTherefore, the inverse of the identity element $e$ is $e$ itself. This completes the proof."
    },
    {
      "id": "10",
      "statement": "In a group, operating the identity element with the identity element results in the identity element.",
      "proof": "To prove this statement, we need to show that in any group $G$, the operation of the identity element $e$ with itself results in the identity element $e$. \n\nBy definition, a group $(G, \\cdot)$ is a set $G$ equipped with a binary operation $\\cdot$ that satisfies the following properties:\n\n1. **Closure**: For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n2. **Associativity**: For all $a, b, c \\in G$, $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n3. **Identity Element**: There exists an element $e \\in G$ such that for all $a \\in G$, $e \\cdot a = a \\cdot e = a$.\n4. **Inverse Element**: For each $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$.\n\nWe are interested in the identity element $e$. By the identity property, for any element $a \\in G$, we have:\n\n\\[ e \\cdot a = a \\quad \\text{and} \\quad a \\cdot e = a. \\]\n\nIn particular, we can choose $a = e$. Applying the identity property to $e$, we have:\n\n\\[ e \\cdot e = e. \\]\n\nThus, operating the identity element with itself results in the identity element. This completes the proof."
    },
    {
      "id": "11",
      "statement": "If an element squared in a group gives the identity element, then the element is its own inverse.",
      "proof": "To prove this statement, let $G$ be a group and let $a \\in G$ be an element such that $a^2 = e$, where $e$ is the identity element of the group $G$. We want to show that $a$ is its own inverse, i.e., $a^{-1} = a$.\n\nBy the definition of an inverse, an element $a \\in G$ has an inverse $a^{-1} \\in G$ such that:\n\n\\[ a \\cdot a^{-1} = e \\quad \\text{and} \\quad a^{-1} \\cdot a = e. \\]\n\nGiven that $a^2 = e$, we have:\n\n\\[ a \\cdot a = e. \\]\n\nTo show that $a$ is its own inverse, we need to demonstrate that $a \\cdot a = e$ implies $a^{-1} = a$. \n\nSince $a \\cdot a = e$, we can multiply both sides of this equation on the right by $a^{-1}$:\n\n\\[ (a \\cdot a) \\cdot a^{-1} = e \\cdot a^{-1}. \\]\n\nUsing the associativity property of group operations, we have:\n\n\\[ a \\cdot (a \\cdot a^{-1}) = a^{-1}. \\]\n\nSince $a \\cdot a^{-1} = e$, we substitute to get:\n\n\\[ a \\cdot e = a^{-1}. \\]\n\nSince $a \\cdot e = a$ (by the identity property of the group), we have:\n\n\\[ a = a^{-1}. \\]\n\nThus, $a$ is indeed its own inverse. Therefore, if an element squared in a group gives the identity element, then the element is its own inverse."
    },
    {
      "id": "12",
      "statement": "A group with only one element is a trivial group.",
      "proof": "To prove that a group with only one element is a trivial group, we need to verify that it satisfies the group axioms: closure, associativity, identity, and inverses.\n\nLet $G = \\{ e \\}$ be a group with a single element $e$. We will show that $G$ satisfies all the group axioms:\n\n1. **Closure**: For any elements $a, b \\in G$, the product $a \\cdot b$ must also be in $G$. Since $G$ has only one element, $a = e$ and $b = e$. Thus, $a \\cdot b = e \\cdot e = e$, which is in $G$. Therefore, $G$ is closed under the group operation.\n\n2. **Associativity**: For any elements $a, b, c \\in G$, the equation $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$ must hold. Since $G$ has only one element, $a = b = c = e$. Thus, $(e \\cdot e) \\cdot e = e \\cdot (e \\cdot e) = e$. Therefore, the operation is associative.\n\n3. **Identity**: There must be an element $e \\in G$ such that for any element $a \\in G$, $e \\cdot a = a \\cdot e = a$. Since $G$ has only one element, $a = e$. Thus, $e \\cdot e = e$ and $e \\cdot e = e$, satisfying the identity property.\n\n4. **Inverses**: For each element $a \\in G$, there must be an element $b \\in G$ such that $a \\cdot b = b \\cdot a = e$. Since $G$ has only one element, $a = e$. Thus, $e \\cdot e = e$, and $e$ is its own inverse.\n\nSince $G = \\{ e \\}$ satisfies all the group axioms, it is indeed a group. Moreover, since it contains only one element, it is called the trivial group."
    },
    {
      "id": "13",
      "statement": "In a finite group, the order (number of times to combine an element with itself to get the identity) of any element is a divisor of the group's order.",
      "proof": "This statement is a well-known result in group theory known as Lagrange's Theorem. Let's provide a proof for it.\n\n**Theorem (Lagrange's Theorem):** Let $G$ be a finite group and let $H$ be a subgroup of $G$. Then the order of $H$ (the number of elements in $H$) divides the order of $G$ (the number of elements in $G$).\n\nTo prove the statement about the order of an element, we will use Lagrange's Theorem.\n\n**Proof:**\n\nLet $G$ be a finite group and let $g \\in G$ be an element of $G$. The order of the element $g$, denoted by $\\operatorname{ord}(g)$, is the smallest positive integer $m$ such that $g^m = e$, where $e$ is the identity element of $G$.\n\nConsider the cyclic subgroup generated by $g$, denoted by $\\langle g \\rangle = \\{ e, g, g^2, \\ldots, g^{m-1} \\}$. The order of this subgroup is exactly $m$, since $g^m = e$ and $m$ is the smallest such positive integer.\n\nBy Lagrange's Theorem, the order of any subgroup of $G$ divides the order of $G$. Therefore, the order of the cyclic subgroup $\\langle g \\rangle$, which is $m = \\operatorname{ord}(g)$, divides the order of the group $G$, denoted by $|G|$.\n\nThus, we have shown that the order of any element $g \\in G$ divides the order of the group $G$. This completes the proof."
    },
    {
      "id": "14",
      "statement": "The set of integers with addition as the operation forms a group.",
      "proof": "To prove that the set of integers $\\mathbb{Z}$ with addition as the operation forms a group, we need to verify that it satisfies the four group axioms: closure, associativity, identity, and inverses.\n\n1. **Closure**: For any two integers $a, b \\in \\mathbb{Z}$, their sum $a + b$ is also an integer. Therefore, $\\mathbb{Z}$ is closed under addition.\n\n2. **Associativity**: Addition of integers is associative. For any integers $a, b, c \\in \\mathbb{Z}$, we have $(a + b) + c = a + (b + c)$. This property holds for all integers.\n\n3. **Identity Element**: The identity element in a group is an element that, when combined with any element of the group, leaves the element unchanged. For addition, the identity element is $0$ because for any integer $a \\in \\mathbb{Z}$, $a + 0 = a$ and $0 + a = a$.\n\n4. **Inverses**: For each integer $a \\in \\mathbb{Z}$, there exists an inverse element $-a \\in \\mathbb{Z}$ such that $a + (-a) = 0$ and $(-a) + a = 0$. Thus, every element has an inverse in $\\mathbb{Z}$.\n\nSince all four group axioms are satisfied, the set of integers $\\mathbb{Z}$ with addition as the operation forms a group."
    },
    {
      "id": "15",
      "statement": "The set of nonzero real numbers with multiplication as the operation forms a group.",
      "proof": "To prove that the set of nonzero real numbers $\\mathbb{R}^*$ with multiplication as the operation forms a group, we need to verify the group axioms: closure, associativity, identity, and inverses.\n\n1. **Closure**: For any $a, b \\in \\mathbb{R}^*$, the product $a \\cdot b$ is also a nonzero real number. This is because the product of two nonzero real numbers is nonzero. Therefore, $\\mathbb{R}^*$ is closed under multiplication.\n\n2. **Associativity**: Multiplication of real numbers is associative. For any $a, b, c \\in \\mathbb{R}^*$, we have $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$. This property holds because it is a fundamental property of real number multiplication.\n\n3. **Identity Element**: The identity element with respect to multiplication in $\\mathbb{R}^*$ is $1$. For any $a \\in \\mathbb{R}^*$, we have $a \\cdot 1 = 1 \\cdot a = a$. Thus, $1$ is the identity element in $\\mathbb{R}^*$.\n\n4. **Inverses**: For every $a \\in \\mathbb{R}^*$, there exists an inverse $a^{-1} = \\frac{1}{a} \\in \\mathbb{R}^*$ such that $a \\cdot a^{-1} = a^{-1} \\cdot a = 1$. Since $a$ is nonzero, $\\frac{1}{a}$ is also a nonzero real number, ensuring that the inverse exists in $\\mathbb{R}^*$.\n\nSince all four group axioms are satisfied, the set of nonzero real numbers $\\mathbb{R}^*$ with multiplication as the operation forms a group."
    }
  ],
  "Complex Analysis": [
    {
      "id": "1",
      "statement": "The complex number \\( i \\) is the imaginary unit, where \\( i^2 = -1 \\).",
      "proof": "The statement that the complex number \\( i \\) is the imaginary unit, where \\( i^2 = -1 \\), is a fundamental definition in complex analysis. To understand this, let's delve into the properties and implications of this definition.\n\n### Proof of Properties of the Imaginary Unit \\( i \\)\n\n1. **Definition of \\( i \\):**\n   The imaginary unit \\( i \\) is defined such that:\n   \\[\n   i^2 = -1\n   \\]\n\n2. **Powers of \\( i \\):**\n   To explore the properties of \\( i \\), let's compute the first few powers of \\( i \\):\n   - \\( i^1 = i \\)\n   - \\( i^2 = -1 \\) (by definition)\n   - \\( i^3 = i^2 \\cdot i = (-1) \\cdot i = -i \\)\n   - \\( i^4 = i^3 \\cdot i = (-i) \\cdot i = -i^2 = -(-1) = 1 \\)\n\n   Notice that the powers of \\( i \\) cycle every four terms:\n   \\[\n   i^1 = i, \\quad i^2 = -1, \\quad i^3 = -i, \\quad i^4 = 1\n   \\]\n   \\[\n   i^5 = i^4 \\cdot i = 1 \\cdot i = i, \\quad \\text{and so on.}\n   \\]\n\n3. **Generalization:**\n   For any integer \\( n \\), the power \\( i^n \\) can be determined by the remainder of \\( n \\) when divided by 4:\n   - If \\( n \\equiv 0 \\pmod{4} \\), then \\( i^n = 1 \\).\n   - If \\( n \\equiv 1 \\pmod{4} \\), then \\( i^n = i \\).\n   - If \\( n \\equiv 2 \\pmod{4} \\), then \\( i^n = -1 \\).\n   - If \\( n \\equiv 3 \\pmod{4} \\), then \\( i^n = -i \\).\n\n4. **Complex Numbers:**\n   A complex number is generally expressed as \\( z = a + bi \\), where \\( a \\) and \\( b \\) are real numbers. The real part of \\( z \\) is \\( a \\), and the imaginary part is \\( b \\).\n\n5. **Conjugate and Modulus:**\n   - The conjugate of \\( z = a + bi \\) is \\( \\overline{z} = a - bi \\).\n   - The modulus of \\( z \\) is given by:\n     \\[\n     |z| = \\sqrt{a^2 + b^2}\n     \\]\n\n6. **Multiplication and Division:**\n   - Multiplication of complex numbers: If \\( z_1 = a + bi \\) and \\( z_2 = c + di \\), then:\n     \\[\n     z_1 \\cdot z_2 = (a + bi)(c + di) = (ac - bd) + (ad + bc)i\n     \\]\n   - Division of complex numbers: To divide \\( z_1 \\) by \\( z_2 \\), multiply the numerator and denominator by the conjugate of the denominator:\n     \\[\n     \\frac{z_1}{z_2} = \\frac{(a + bi)(c - di)}{(c + di)(c - di)} = \\frac{(ac + bd) + (bc - ad)i}{c^2 + d^2}\n     \\]\n\nThe definition of \\( i \\) as the imaginary unit with \\( i^2 = -1 \\) is crucial for the algebraic structure of complex numbers, allowing for the extension of real numbers to the complex plane and enabling the solution of equations that have no real solutions, such as \\( x^2 + 1 = 0 \\)."
    },
    {
      "id": "2",
      "statement": "The modulus of a complex number \\( a + bi \\) is always non-negative.",
      "proof": "To prove that the modulus of a complex number \\( a + bi \\) is always non-negative, we start by recalling the definition of the modulus of a complex number. For a complex number \\( z = a + bi \\), where \\( a \\) and \\( b \\) are real numbers, the modulus of \\( z \\), denoted by \\( |z| \\), is given by:\n\n\\[\n|z| = \\sqrt{a^2 + b^2}\n\\]\n\nWe need to show that \\( |z| \\geq 0 \\) for all real numbers \\( a \\) and \\( b \\).\n\n1. **Non-negativity of squares**: For any real number \\( a \\), \\( a^2 \\geq 0 \\). Similarly, for any real number \\( b \\), \\( b^2 \\geq 0 \\). Therefore, the sum \\( a^2 + b^2 \\) is also non-negative:\n\n   \\[\n   a^2 + b^2 \\geq 0\n   \\]\n\n2. **Non-negativity of the square root**: The square root function, \\( \\sqrt{x} \\), is defined to be non-negative for all \\( x \\geq 0 \\). Therefore, since \\( a^2 + b^2 \\geq 0 \\), it follows that:\n\n   \\[\n   \\sqrt{a^2 + b^2} \\geq 0\n   \\]\n\n3. **Conclusion**: By the definition of the modulus and the properties of squares and square roots, we have:\n\n   \\[\n   |z| = \\sqrt{a^2 + b^2} \\geq 0\n   \\]\n\nThus, the modulus of a complex number \\( a + bi \\) is always non-negative."
    },
    {
      "id": "3",
      "statement": "If two complex numbers are equal, then their real parts are equal, and their imaginary parts are equal.",
      "proof": "To prove this statement, let's consider two complex numbers $z_1$ and $z_2$. We can express these complex numbers in terms of their real and imaginary parts as follows:\n\n\\[ z_1 = a + bi \\]\n\\[ z_2 = c + di \\]\n\nwhere $a, b, c, d \\in \\mathbb{R}$, and $i$ is the imaginary unit with the property $i^2 = -1$.\n\nAccording to the problem statement, we have $z_1 = z_2$. This implies:\n\n\\[ a + bi = c + di \\]\n\nFor two complex numbers to be equal, both their real and imaginary parts must be equal. Therefore, we equate the real parts and the imaginary parts separately:\n\n1. Equating the real parts:\n   \\[ a = c \\]\n\n2. Equating the imaginary parts:\n   \\[ b = d \\]\n\nThus, if two complex numbers $z_1$ and $z_2$ are equal, then their real parts $a$ and $c$ are equal, and their imaginary parts $b$ and $d$ are equal. This completes the proof."
    },
    {
      "id": "4",
      "statement": "The complex conjugate of a real number is the number itself.",
      "proof": "To prove that the complex conjugate of a real number is the number itself, let's start by considering a real number $a$. In the context of complex numbers, a real number can be expressed as $a + 0i$, where $a$ is the real part and $0$ is the imaginary part.\n\nThe complex conjugate of a complex number $z = x + yi$ is defined as $\\overline{z} = x - yi$. Applying this definition to our real number $a + 0i$, we have:\n\n\\[\n\\overline{a + 0i} = a - 0i = a\n\\]\n\nSince the imaginary part is zero, the complex conjugate of $a + 0i$ is simply $a$. Therefore, the complex conjugate of a real number $a$ is the number itself, $a$. This completes the proof."
    },
    {
      "id": "5",
      "statement": "The real part of the complex number \\( a + bi \\) is \\( a \\).",
      "proof": "To prove that the real part of the complex number \\( a + bi \\) is \\( a \\), we start by considering the definition of a complex number. A complex number is typically expressed in the form \\( z = a + bi \\), where \\( a \\) and \\( b \\) are real numbers, and \\( i \\) is the imaginary unit with the property \\( i^2 = -1 \\).\n\nThe real part of a complex number \\( z = a + bi \\) is defined as the component of the complex number that does not involve the imaginary unit \\( i \\). In this expression, \\( a \\) is the real part, and \\( bi \\) is the imaginary part.\n\nFormally, the real part of a complex number \\( z = a + bi \\) is denoted by \\( \\operatorname{Re}(z) \\). Therefore, by definition:\n\n\\[\n\\operatorname{Re}(z) = a\n\\]\n\nThis definition is consistent with the standard representation of complex numbers in the complex plane, where the real part \\( a \\) corresponds to the horizontal axis (real axis), and the imaginary part \\( b \\) corresponds to the vertical axis (imaginary axis).\n\nThus, the real part of the complex number \\( a + bi \\) is indeed \\( a \\)."
    },
    {
      "id": "6",
      "statement": "The imaginary part of the complex number \\( a + bi \\) is \\( b \\).",
      "proof": "To prove that the imaginary part of the complex number \\( a + bi \\) is \\( b \\), we start by considering the definition of a complex number.\n\nA complex number is typically expressed in the form \\( z = a + bi \\), where \\( a \\) and \\( b \\) are real numbers, and \\( i \\) is the imaginary unit, defined by the property \\( i^2 = -1 \\).\n\nIn this expression:\n- \\( a \\) is called the real part of the complex number, denoted as \\( \\text{Re}(z) \\).\n- \\( b \\) is called the imaginary part of the complex number, denoted as \\( \\text{Im}(z) \\).\n\nThe imaginary part of a complex number is defined as the coefficient of the imaginary unit \\( i \\) in the expression of the complex number. Therefore, for the complex number \\( z = a + bi \\), the imaginary part is simply the real number \\( b \\).\n\nThus, we have:\n\\[\n\\text{Im}(a + bi) = b\n\\]\n\nThis completes the proof that the imaginary part of the complex number \\( a + bi \\) is indeed \\( b \\)."
    },
    {
      "id": "7",
      "statement": "Adding a complex number to its complex conjugate results in a real number.",
      "proof": "Let $z$ be a complex number, which can be expressed in the form $z = a + bi$, where $a$ and $b$ are real numbers, and $i$ is the imaginary unit with the property $i^2 = -1$.\n\nThe complex conjugate of $z$, denoted by $\\overline{z}$, is given by $\\overline{z} = a - bi$.\n\nWe want to show that the sum of a complex number and its complex conjugate is a real number. Consider the sum:\n\n\\[\nz + \\overline{z} = (a + bi) + (a - bi)\n\\]\n\nSimplifying the expression, we have:\n\n\\[\nz + \\overline{z} = a + bi + a - bi = 2a\n\\]\n\nSince $2a$ is a real number (as $a$ is a real number), the sum $z + \\overline{z}$ is indeed a real number.\n\nThus, adding a complex number to its complex conjugate results in a real number."
    },
    {
      "id": "8",
      "statement": "The imaginary part of the sum of a complex number and its conjugate is zero.",
      "proof": "Let $z$ be a complex number, which can be expressed in the form $z = a + bi$, where $a$ and $b$ are real numbers, and $i$ is the imaginary unit with $i^2 = -1$. The conjugate of $z$, denoted by $\\overline{z}$, is given by $\\overline{z} = a - bi$.\n\nWe want to show that the imaginary part of the sum $z + \\overline{z}$ is zero.\n\nFirst, compute the sum of $z$ and its conjugate:\n\n\\[\nz + \\overline{z} = (a + bi) + (a - bi) = a + bi + a - bi.\n\\]\n\nSimplifying the expression, we have:\n\n\\[\nz + \\overline{z} = (a + a) + (bi - bi) = 2a + 0i.\n\\]\n\nThe imaginary part of a complex number $c + di$ is $d$. In this case, the imaginary part of $z + \\overline{z}$ is $0$, since the expression simplifies to $2a + 0i$.\n\nTherefore, the imaginary part of the sum of a complex number and its conjugate is indeed zero."
    },
    {
      "id": "9",
      "statement": "The complex plane is a two-dimensional plane where each point is associated with a complex number.",
      "proof": "The complex plane, also known as the Argand plane, is a two-dimensional plane used to represent complex numbers geometrically. Each point in this plane corresponds to a complex number. A complex number is of the form $z = x + yi$, where $x$ and $y$ are real numbers, and $i$ is the imaginary unit with the property $i^2 = -1$.\n\nIn the complex plane:\n\n- The horizontal axis (real axis) represents the real part of the complex number, $x$.\n- The vertical axis (imaginary axis) represents the imaginary part of the complex number, $y$.\n\nA point in the complex plane is denoted by the coordinates $(x, y)$, which corresponds to the complex number $z = x + yi$. \n\nTo visualize this, consider the following:\n\n1. **Real Part**: The real part of $z$, denoted as $\\operatorname{Re}(z)$, is the $x$-coordinate of the point in the complex plane.\n2. **Imaginary Part**: The imaginary part of $z$, denoted as $\\operatorname{Im}(z)$, is the $y$-coordinate of the point in the complex plane.\n\nThe complex plane allows us to perform operations on complex numbers geometrically:\n\n- **Addition**: Adding two complex numbers corresponds to vector addition in the plane.\n- **Multiplication**: Multiplying two complex numbers involves both scaling and rotating vectors in the plane.\n\nThe modulus of a complex number $z = x + yi$ is given by $|z| = \\sqrt{x^2 + y^2}$, which represents the distance of the point $(x, y)$ from the origin $(0, 0)$ in the complex plane.\n\nThe argument of a complex number, $\\arg(z)$, is the angle $\\theta$ formed with the positive real axis, measured counterclockwise. It can be calculated using $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$.\n\nIn summary, the complex plane provides a powerful geometric interpretation of complex numbers, facilitating operations and visualizations that are essential in complex analysis."
    },
    {
      "id": "10",
      "statement": "The modulus of a complex number is the distance from the origin to the point on the complex plane.",
      "proof": "To prove that the modulus of a complex number is the distance from the origin to the point on the complex plane, let's consider a complex number $z = x + yi$, where $x$ and $y$ are real numbers, and $i$ is the imaginary unit.\n\nThe modulus of $z$, denoted as $|z|$, is defined as:\n\n$$ |z| = \\sqrt{x^2 + y^2}. $$\n\nThis definition is derived from the Pythagorean theorem. In the complex plane, the complex number $z = x + yi$ corresponds to the point $(x, y)$. The origin in the complex plane is the point $(0, 0)$.\n\nThe distance $d$ between the point $(x, y)$ and the origin $(0, 0)$ is given by the Euclidean distance formula:\n\n$$ d = \\sqrt{(x - 0)^2 + (y - 0)^2} = \\sqrt{x^2 + y^2}. $$\n\nComparing this with the definition of the modulus, we see that:\n\n$$ |z| = \\sqrt{x^2 + y^2} = d. $$\n\nTherefore, the modulus of the complex number $z$ is indeed the distance from the origin to the point $(x, y)$ on the complex plane. This completes the proof."
    },
    {
      "id": "11",
      "statement": "The argument of a complex number is the angle the number makes with the positive real axis.",
      "proof": "To prove that the argument of a complex number is the angle it makes with the positive real axis, let's consider a complex number $z = x + yi$, where $x$ and $y$ are real numbers, and $i$ is the imaginary unit.\n\nThe complex number $z$ can be represented in the complex plane as a point $(x, y)$. The modulus (or magnitude) of $z$ is given by:\n\n\\[\n|z| = \\sqrt{x^2 + y^2}\n\\]\n\nThe argument of $z$, denoted as $\\arg(z)$, is the angle $\\theta$ that the line connecting the origin to the point $(x, y)$ makes with the positive real axis. This angle can be determined using trigonometry. Specifically, the tangent of the angle $\\theta$ is given by the ratio of the imaginary part to the real part of the complex number:\n\n\\[\n\\tan(\\theta) = \\frac{y}{x}\n\\]\n\nThus, the argument $\\theta$ can be expressed as:\n\n\\[\n\\theta = \\arg(z) = \\tan^{-1}\\left(\\frac{y}{x}\\right)\n\\]\n\nThis angle $\\theta$ is measured counterclockwise from the positive real axis. The principal value of the argument is typically taken to be in the interval $(-\\pi, \\pi]$.\n\nTherefore, the argument of a complex number $z = x + yi$ is indeed the angle that the line from the origin to the point $(x, y)$ makes with the positive real axis, confirming the statement."
    },
    {
      "id": "12",
      "statement": "Multiplying a complex number by its complex conjugate results in a real number.",
      "proof": "Let $z$ be a complex number, expressed in the form $z = a + bi$, where $a$ and $b$ are real numbers, and $i$ is the imaginary unit with the property $i^2 = -1$. The complex conjugate of $z$, denoted by $\\overline{z}$, is given by $\\overline{z} = a - bi$.\n\nTo show that multiplying a complex number by its complex conjugate results in a real number, we compute the product $z \\cdot \\overline{z}$:\n\n\\[\nz \\cdot \\overline{z} = (a + bi)(a - bi)\n\\]\n\nUsing the distributive property, we expand the product:\n\n\\[\nz \\cdot \\overline{z} = a^2 - abi + abi - b^2i^2\n\\]\n\nNotice that the imaginary parts $-abi$ and $abi$ cancel each other out:\n\n\\[\nz \\cdot \\overline{z} = a^2 - b^2i^2\n\\]\n\nSince $i^2 = -1$, we have:\n\n\\[\nz \\cdot \\overline{z} = a^2 - b^2(-1) = a^2 + b^2\n\\]\n\nThe expression $a^2 + b^2$ is a sum of squares of real numbers, which is always a non-negative real number. Therefore, the product of a complex number and its complex conjugate is indeed a real number."
    },
    {
      "id": "13",
      "statement": "The modulus of the product of two complex numbers is the product of their moduli.",
      "proof": "To prove that the modulus of the product of two complex numbers is the product of their moduli, let us consider two complex numbers $z_1 = a + bi$ and $z_2 = c + di$, where $a, b, c, d \\in \\mathbb{R}$.\n\nThe modulus of a complex number $z = x + yi$ is given by $|z| = \\sqrt{x^2 + y^2}$.\n\nFirst, we calculate the product $z_1 z_2$:\n\n\\[\nz_1 z_2 = (a + bi)(c + di) = ac + adi + bci + bdi^2.\n\\]\n\nSince $i^2 = -1$, this simplifies to:\n\n\\[\nz_1 z_2 = (ac - bd) + (ad + bc)i.\n\\]\n\nThe modulus of $z_1 z_2$ is:\n\n\\[\n|z_1 z_2| = \\sqrt{(ac - bd)^2 + (ad + bc)^2}.\n\\]\n\nNow, let's calculate the moduli of $z_1$ and $z_2$:\n\n\\[\n|z_1| = \\sqrt{a^2 + b^2},\n\\]\n\\[\n|z_2| = \\sqrt{c^2 + d^2}.\n\\]\n\nThe product of the moduli is:\n\n\\[\n|z_1| \\cdot |z_2| = \\sqrt{a^2 + b^2} \\cdot \\sqrt{c^2 + d^2} = \\sqrt{(a^2 + b^2)(c^2 + d^2)}.\n\\]\n\nExpanding the expression under the square root, we have:\n\n\\[\n(a^2 + b^2)(c^2 + d^2) = a^2c^2 + a^2d^2 + b^2c^2 + b^2d^2.\n\\]\n\nRearranging terms, we get:\n\n\\[\n= (ac)^2 + (ad)^2 + (bc)^2 + (bd)^2.\n\\]\n\nNotice that:\n\n\\[\n(ac - bd)^2 = a^2c^2 - 2abcd + b^2d^2,\n\\]\n\\[\n(ad + bc)^2 = a^2d^2 + 2abcd + b^2c^2.\n\\]\n\nAdding these two equations, we obtain:\n\n\\[\n(ac - bd)^2 + (ad + bc)^2 = a^2c^2 - 2abcd + b^2d^2 + a^2d^2 + 2abcd + b^2c^2.\n\\]\n\nSimplifying, we find:\n\n\\[\n= a^2c^2 + b^2d^2 + a^2d^2 + b^2c^2.\n\\]\n\nThis matches the expanded form of $(a^2 + b^2)(c^2 + d^2)$, confirming that:\n\n\\[\n|z_1 z_2| = \\sqrt{(ac - bd)^2 + (ad + bc)^2} = \\sqrt{(a^2 + b^2)(c^2 + d^2)} = |z_1| \\cdot |z_2|.\n\\]\n\nThus, the modulus of the product of two complex numbers is indeed the product of their moduli."
    },
    {
      "id": "14",
      "statement": "The conjugate of the sum of two complex numbers is equal to the sum of their conjugates.",
      "proof": "To prove that the conjugate of the sum of two complex numbers is equal to the sum of their conjugates, let's consider two complex numbers $z_1 = a + bi$ and $z_2 = c + di$, where $a, b, c, d \\in \\mathbb{R}$ and $i = \\sqrt{-1}$.\n\nThe conjugate of a complex number $z = x + yi$ is given by $\\overline{z} = x - yi$.\n\nFirst, we find the sum of the two complex numbers:\n\n\\[\nz_1 + z_2 = (a + bi) + (c + di) = (a + c) + (b + d)i\n\\]\n\nNext, we find the conjugate of this sum:\n\n\\[\n\\overline{z_1 + z_2} = \\overline{(a + c) + (b + d)i} = (a + c) - (b + d)i\n\\]\n\nNow, let's find the conjugates of $z_1$ and $z_2$ individually and then sum them:\n\n\\[\n\\overline{z_1} = \\overline{a + bi} = a - bi\n\\]\n\n\\[\n\\overline{z_2} = \\overline{c + di} = c - di\n\\]\n\nThe sum of the conjugates is:\n\n\\[\n\\overline{z_1} + \\overline{z_2} = (a - bi) + (c - di) = (a + c) - (b + d)i\n\\]\n\nWe observe that:\n\n\\[\n\\overline{z_1 + z_2} = (a + c) - (b + d)i = \\overline{z_1} + \\overline{z_2}\n\\]\n\nThus, the conjugate of the sum of two complex numbers is equal to the sum of their conjugates:\n\n\\[\n\\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}\n\\]\n\nThis completes the proof."
    },
    {
      "id": "15",
      "statement": "The exponential form of a complex number can be expressed as \\( re^{i\\theta} \\), where \\( r \\) is the modulus and \\( \\theta \\) is the argument.",
      "proof": "To prove that a complex number can be expressed in the exponential form \\( re^{i\\theta} \\), where \\( r \\) is the modulus and \\( \\theta \\) is the argument, we start by considering a complex number \\( z = x + yi \\), where \\( x \\) and \\( y \\) are real numbers.\n\n1. **Modulus**: The modulus \\( r \\) of the complex number \\( z \\) is given by:\n   \\[\n   r = |z| = \\sqrt{x^2 + y^2}\n   \\]\n\n2. **Argument**: The argument \\( \\theta \\) is the angle formed with the positive real axis, and it can be determined using the arctangent function:\n   \\[\n   \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)\n   \\]\n   Note that the argument \\( \\theta \\) is typically adjusted to ensure it lies within the correct quadrant.\n\n3. **Exponential Form**: The exponential form of a complex number is derived from Euler's formula, which states:\n   \\[\n   e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)\n   \\]\n   Therefore, the complex number \\( z \\) can be expressed as:\n   \\[\n   z = r(\\cos(\\theta) + i\\sin(\\theta)) = re^{i\\theta}\n   \\]\n\n4. **Verification**: To verify, we expand \\( re^{i\\theta} \\) using Euler's formula:\n   \\[\n   re^{i\\theta} = r(\\cos(\\theta) + i\\sin(\\theta)) = r\\cos(\\theta) + ir\\sin(\\theta)\n   \\]\n   Comparing with the original form \\( z = x + yi \\), we have:\n   \\[\n   x = r\\cos(\\theta) \\quad \\text{and} \\quad y = r\\sin(\\theta)\n   \\]\n   These equations are consistent with the definitions of \\( r \\) and \\( \\theta \\) given above.\n\nThus, any complex number \\( z = x + yi \\) can indeed be expressed in the exponential form \\( re^{i\\theta} \\), where \\( r \\) is the modulus and \\( \\theta \\) is the argument."
    }
  ],
  "Vector Calculus": [
    {
      "id": "1",
      "statement": "The dot product of two orthogonal vectors is zero.",
      "proof": "To prove that the dot product of two orthogonal vectors is zero, let's consider two vectors $\\mathbf{a}$ and $\\mathbf{b}$ in $\\mathbb{R}^n$. The dot product of these vectors is defined as:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n\n\\]\n\nwhere $\\mathbf{a} = (a_1, a_2, \\ldots, a_n)$ and $\\mathbf{b} = (b_1, b_2, \\ldots, b_n)$.\n\nVectors $\\mathbf{a}$ and $\\mathbf{b}$ are said to be orthogonal if the angle $\\theta$ between them is $90^\\circ$. The dot product can also be expressed in terms of the magnitudes of the vectors and the cosine of the angle between them:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta\n\\]\n\nwhere $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ are the magnitudes (or lengths) of vectors $\\mathbf{a}$ and $\\mathbf{b}$, respectively.\n\nSince $\\mathbf{a}$ and $\\mathbf{b}$ are orthogonal, $\\theta = 90^\\circ$, and we know that $\\cos 90^\\circ = 0$. Therefore, the dot product becomes:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos 90^\\circ = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cdot 0 = 0\n\\]\n\nThus, the dot product of two orthogonal vectors is zero."
    },
    {
      "id": "2",
      "statement": "The cross product of a vector with itself is zero.",
      "proof": "To prove that the cross product of a vector with itself is zero, let's consider a vector $\\mathbf{a} = \\langle a_1, a_2, a_3 \\rangle$ in $\\mathbb{R}^3$. The cross product of $\\mathbf{a}$ with itself, denoted as $\\mathbf{a} \\times \\mathbf{a}$, is given by:\n\n\\[\n\\mathbf{a} \\times \\mathbf{a} = \\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\na_1 & a_2 & a_3 \\\\\na_1 & a_2 & a_3\n\\end{vmatrix}\n\\]\n\nExpanding this determinant, we have:\n\n\\[\n\\mathbf{a} \\times \\mathbf{a} = \\mathbf{i}(a_2 a_3 - a_3 a_2) - \\mathbf{j}(a_1 a_3 - a_3 a_1) + \\mathbf{k}(a_1 a_2 - a_2 a_1)\n\\]\n\nSimplifying each term, we find:\n\n- The $\\mathbf{i}$ component: $a_2 a_3 - a_3 a_2 = 0$\n- The $\\mathbf{j}$ component: $a_1 a_3 - a_3 a_1 = 0$\n- The $\\mathbf{k}$ component: $a_1 a_2 - a_2 a_1 = 0$\n\nThus, the cross product is:\n\n\\[\n\\mathbf{a} \\times \\mathbf{a} = \\langle 0, 0, 0 \\rangle\n\\]\n\nTherefore, the cross product of a vector with itself is the zero vector, confirming that $\\mathbf{a} \\times \\mathbf{a} = \\mathbf{0}$."
    },
    {
      "id": "3",
      "statement": "Adding two vectors yields a third vector.",
      "proof": "To prove that adding two vectors yields a third vector, we will use the properties of vector addition in a vector space.\n\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be two vectors in a vector space $V$. The sum of these two vectors, denoted by $\\mathbf{u} + \\mathbf{v}$, is defined as a vector in the same vector space $V$. \n\nThe operation of vector addition is defined by the following properties:\n\n1. **Closure**: For any vectors $\\mathbf{u}, \\mathbf{v} \\in V$, the sum $\\mathbf{u} + \\mathbf{v}$ is also in $V$.\n\n2. **Commutativity**: For any vectors $\\mathbf{u}, \\mathbf{v} \\in V$, $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$.\n\n3. **Associativity**: For any vectors $\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$, $(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$.\n\n4. **Existence of an Identity Element**: There exists a zero vector $\\mathbf{0} \\in V$ such that for any vector $\\mathbf{u} \\in V$, $\\mathbf{u} + \\mathbf{0} = \\mathbf{u}$.\n\n5. **Existence of Inverse Elements**: For every vector $\\mathbf{u} \\in V$, there exists a vector $-\\mathbf{u} \\in V$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$.\n\nGiven these properties, when we add two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the result $\\mathbf{u} + \\mathbf{v}$ is indeed a vector in the same vector space $V$. This is guaranteed by the closure property of vector addition.\n\nTherefore, adding two vectors $\\mathbf{u}$ and $\\mathbf{v}$ yields a third vector $\\mathbf{u} + \\mathbf{v}$, which is also an element of the vector space $V$."
    },
    {
      "id": "4",
      "statement": "The magnitude of a vector is always non-negative.",
      "proof": "To prove that the magnitude of a vector is always non-negative, consider a vector $\\mathbf{v}$ in $\\mathbb{R}^n$ given by $\\mathbf{v} = \\langle v_1, v_2, \\ldots, v_n \\rangle$. The magnitude (or norm) of the vector $\\mathbf{v}$, denoted by $\\|\\mathbf{v}\\|$, is defined as:\n\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\n\\]\n\nWe need to show that $\\|\\mathbf{v}\\| \\geq 0$.\n\n1. **Non-negativity of squares**: For any real number $v_i$, the square $v_i^2$ is non-negative, i.e., $v_i^2 \\geq 0$. This is because the square of a real number is always non-negative.\n\n2. **Sum of non-negative numbers**: Since each $v_i^2 \\geq 0$, the sum $v_1^2 + v_2^2 + \\cdots + v_n^2$ is also non-negative.\n\n3. **Square root of a non-negative number**: The square root function $\\sqrt{x}$ is defined to be non-negative for any non-negative $x$. Therefore, $\\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\geq 0$.\n\nCombining these observations, we conclude that:\n\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\geq 0\n\\]\n\nThus, the magnitude of a vector is always non-negative."
    },
    {
      "id": "5",
      "statement": "The dot product of a vector with the zero vector is zero.",
      "proof": "To prove that the dot product of any vector with the zero vector is zero, let's consider a vector $\\mathbf{a} = (a_1, a_2, \\ldots, a_n)$ in $\\mathbb{R}^n$ and the zero vector $\\mathbf{0} = (0, 0, \\ldots, 0)$ in the same space.\n\nThe dot product of $\\mathbf{a}$ and $\\mathbf{0}$ is defined as:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{0} = a_1 \\cdot 0 + a_2 \\cdot 0 + \\cdots + a_n \\cdot 0\n\\]\n\nEach term in the sum is zero because multiplying any real number by zero results in zero. Therefore, the entire sum is zero:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{0} = 0 + 0 + \\cdots + 0 = 0\n\\]\n\nThus, the dot product of any vector $\\mathbf{a}$ with the zero vector $\\mathbf{0}$ is zero."
    },
    {
      "id": "6",
      "statement": "Any vector can be multiplied by a scalar to get another vector.",
      "proof": "To prove that any vector can be multiplied by a scalar to get another vector, we need to consider the definition of scalar multiplication in vector spaces.\n\nLet $\\mathbf{v}$ be a vector in a vector space $V$ over a field $\\mathbb{F}$, and let $c$ be a scalar in $\\mathbb{F}$. The scalar multiplication of the vector $\\mathbf{v}$ by the scalar $c$ is defined as:\n\n$$ c \\mathbf{v} = (c v_1, c v_2, \\ldots, c v_n), $$\n\nwhere $\\mathbf{v} = (v_1, v_2, \\ldots, v_n)$ is the representation of the vector in terms of its components.\n\nThe result of this operation, $c \\mathbf{v}$, is another vector in the same vector space $V$. This is because scalar multiplication is one of the operations that define a vector space, and it satisfies the following properties:\n\n1. **Closure under scalar multiplication**: For any vector $\\mathbf{v} \\in V$ and any scalar $c \\in \\mathbb{F}$, the product $c \\mathbf{v}$ is also in $V$.\n\n2. **Associativity of scalar multiplication**: For any scalars $a, b \\in \\mathbb{F}$ and any vector $\\mathbf{v} \\in V$, we have $a(b \\mathbf{v}) = (ab) \\mathbf{v}$.\n\n3. **Distributive properties**:\n   - For any scalars $a, b \\in \\mathbb{F}$ and any vector $\\mathbf{v} \\in V$, we have $(a + b) \\mathbf{v} = a \\mathbf{v} + b \\mathbf{v}$.\n   - For any scalar $a \\in \\mathbb{F}$ and any vectors $\\mathbf{u}, \\mathbf{v} \\in V$, we have $a (\\mathbf{u} + \\mathbf{v}) = a \\mathbf{u} + a \\mathbf{v}$.\n\n4. **Identity element of scalar multiplication**: For any vector $\\mathbf{v} \\in V$, $1 \\mathbf{v} = \\mathbf{v}$, where $1$ is the multiplicative identity in $\\mathbb{F}$.\n\nThese properties ensure that scalar multiplication is well-defined and that the result of multiplying a vector by a scalar is indeed another vector in the same vector space. Therefore, any vector can be multiplied by a scalar to yield another vector."
    },
    {
      "id": "7",
      "statement": "The cross product of two parallel vectors is zero.",
      "proof": "To prove that the cross product of two parallel vectors is zero, let's consider two vectors $\\mathbf{a}$ and $\\mathbf{b}$ in $\\mathbb{R}^3$. We say that $\\mathbf{a}$ and $\\mathbf{b}$ are parallel if there exists a scalar $\\lambda$ such that $\\mathbf{b} = \\lambda \\mathbf{a}$.\n\nThe cross product of two vectors $\\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix}$ is given by:\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{vmatrix} = \\begin{pmatrix} a_2b_3 - a_3b_2 \\\\ a_3b_1 - a_1b_3 \\\\ a_1b_2 - a_2b_1 \\end{pmatrix}\n\\]\n\nIf $\\mathbf{b} = \\lambda \\mathbf{a}$, then $b_1 = \\lambda a_1$, $b_2 = \\lambda a_2$, and $b_3 = \\lambda a_3$. Substituting these into the cross product formula, we have:\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} a_2(\\lambda a_3) - a_3(\\lambda a_2) \\\\ a_3(\\lambda a_1) - a_1(\\lambda a_3) \\\\ a_1(\\lambda a_2) - a_2(\\lambda a_1) \\end{pmatrix}\n\\]\n\nSimplifying each component, we get:\n\n1. $a_2(\\lambda a_3) - a_3(\\lambda a_2) = \\lambda (a_2 a_3 - a_3 a_2) = \\lambda (0) = 0$\n2. $a_3(\\lambda a_1) - a_1(\\lambda a_3) = \\lambda (a_3 a_1 - a_1 a_3) = \\lambda (0) = 0$\n3. $a_1(\\lambda a_2) - a_2(\\lambda a_1) = \\lambda (a_1 a_2 - a_2 a_1) = \\lambda (0) = 0$\n\nThus, the cross product $\\mathbf{a} \\times \\mathbf{b}$ is:\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\]\n\nTherefore, the cross product of two parallel vectors is indeed the zero vector."
    },
    {
      "id": "8",
      "statement": "The magnitude of the cross product of two vectors is zero if and only if the vectors are parallel.",
      "proof": "To prove that the magnitude of the cross product of two vectors is zero if and only if the vectors are parallel, we need to consider both directions of the statement: \n\n1. **If the magnitude of the cross product is zero, then the vectors are parallel.**\n\nLet $\\mathbf{a}$ and $\\mathbf{b}$ be two vectors in $\\mathbb{R}^3$. The cross product $\\mathbf{a} \\times \\mathbf{b}$ is defined as a vector that is orthogonal to both $\\mathbf{a}$ and $\\mathbf{b}$, and its magnitude is given by:\n\n\\[\n\\|\\mathbf{a} \\times \\mathbf{b}\\| = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\sin \\theta\n\\]\n\nwhere $\\theta$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$. If $\\|\\mathbf{a} \\times \\mathbf{b}\\| = 0$, then:\n\n\\[\n\\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\sin \\theta = 0\n\\]\n\nSince $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ are non-zero for non-zero vectors, it must be that $\\sin \\theta = 0$. This implies that $\\theta = 0$ or $\\theta = \\pi$, meaning the vectors are parallel (or anti-parallel).\n\n2. **If the vectors are parallel, then the magnitude of the cross product is zero.**\n\nIf $\\mathbf{a}$ and $\\mathbf{b}$ are parallel, then $\\theta = 0$ or $\\theta = \\pi$. In both cases, $\\sin \\theta = 0$. Therefore, the magnitude of the cross product is:\n\n\\[\n\\|\\mathbf{a} \\times \\mathbf{b}\\| = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\sin \\theta = 0\n\\]\n\nThus, the magnitude of the cross product of two vectors is zero if and only if the vectors are parallel."
    },
    {
      "id": "9",
      "statement": "For any vector, multiplying by 1 leaves the vector unchanged.",
      "proof": "To prove that multiplying any vector by 1 leaves the vector unchanged, consider a vector $\\mathbf{v}$ in a vector space. The vector $\\mathbf{v}$ can be expressed in terms of its components as $\\mathbf{v} = (v_1, v_2, \\ldots, v_n)$, where $v_i$ are the components of the vector.\n\nThe scalar multiplication of a vector by a scalar $c$ is defined as:\n\n\\[\nc \\mathbf{v} = (c v_1, c v_2, \\ldots, c v_n)\n\\]\n\nIn this case, we want to multiply the vector $\\mathbf{v}$ by the scalar $1$. Applying the definition of scalar multiplication, we have:\n\n\\[\n1 \\mathbf{v} = (1 \\cdot v_1, 1 \\cdot v_2, \\ldots, 1 \\cdot v_n) = (v_1, v_2, \\ldots, v_n) = \\mathbf{v}\n\\]\n\nThus, multiplying the vector $\\mathbf{v}$ by 1 results in the same vector $\\mathbf{v}$, leaving it unchanged. This property is consistent with the identity property of scalar multiplication in vector spaces, which states that multiplying any vector by the scalar 1 yields the original vector."
    },
    {
      "id": "10",
      "statement": "The dot product of a vector with a unit vector along the same direction gives the magnitude of the vector.",
      "proof": "To prove this statement, let's consider a vector $\\mathbf{v}$ and a unit vector $\\mathbf{u}$ that is in the same direction as $\\mathbf{v}$. By definition, a unit vector in the direction of $\\mathbf{v}$ is given by:\n\n\\[\n\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n\\]\n\nwhere $\\|\\mathbf{v}\\|$ denotes the magnitude (or norm) of the vector $\\mathbf{v}$.\n\nThe dot product of $\\mathbf{v}$ with $\\mathbf{u}$ is given by:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{u} = \\mathbf{v} \\cdot \\left(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\right)\n\\]\n\nUsing the properties of the dot product, we can factor out the scalar $\\frac{1}{\\|\\mathbf{v}\\|}$:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{u} = \\frac{1}{\\|\\mathbf{v}\\|} (\\mathbf{v} \\cdot \\mathbf{v})\n\\]\n\nThe dot product of a vector with itself is equal to the square of its magnitude:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{v} = \\|\\mathbf{v}\\|^2\n\\]\n\nSubstituting this into the equation, we have:\n\n\\[\n\\mathbf{v} \\cdot \\mathbf{u} = \\frac{1}{\\|\\mathbf{v}\\|} \\|\\mathbf{v}\\|^2 = \\|\\mathbf{v}\\|\n\\]\n\nThus, the dot product of a vector $\\mathbf{v}$ with a unit vector $\\mathbf{u}$ in the same direction as $\\mathbf{v}$ is equal to the magnitude of $\\mathbf{v}$. This completes the proof."
    },
    {
      "id": "11",
      "statement": "The sum of vectors is commutative: adding vector \\(\\mathbf{a}\\) to vector \\(\\mathbf{b}\\) is the same as adding \\(\\mathbf{b}\\) to \\(\\mathbf{a}\\).",
      "proof": "To prove that vector addition is commutative, we need to show that for any two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), the equation \\(\\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a}\\) holds.\n\nLet \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_n)\\) and \\(\\mathbf{b} = (b_1, b_2, \\ldots, b_n)\\) be two vectors in \\(\\mathbb{R}^n\\).\n\nThe sum of the vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is given by:\n\n\\[\n\\mathbf{a} + \\mathbf{b} = (a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n)\n\\]\n\nSimilarly, the sum of the vectors \\(\\mathbf{b}\\) and \\(\\mathbf{a}\\) is:\n\n\\[\n\\mathbf{b} + \\mathbf{a} = (b_1 + a_1, b_2 + a_2, \\ldots, b_n + a_n)\n\\]\n\nBy the commutative property of real numbers, we know that for any real numbers \\(a_i\\) and \\(b_i\\), \\(a_i + b_i = b_i + a_i\\). Applying this property to each component of the vectors, we have:\n\n\\[\na_1 + b_1 = b_1 + a_1, \\quad a_2 + b_2 = b_2 + a_2, \\quad \\ldots, \\quad a_n + b_n = b_n + a_n\n\\]\n\nThus, we can conclude that:\n\n\\[\n\\mathbf{a} + \\mathbf{b} = (a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n) = (b_1 + a_1, b_2 + a_2, \\ldots, b_n + a_n) = \\mathbf{b} + \\mathbf{a}\n\\]\n\nTherefore, vector addition is commutative, as required."
    },
    {
      "id": "12",
      "statement": "The magnitude of the sum of two vectors is never greater than the sum of their magnitudes.",
      "proof": "To prove that the magnitude of the sum of two vectors is never greater than the sum of their magnitudes, we will use the triangle inequality for vectors. Let $\\mathbf{a}$ and $\\mathbf{b}$ be two vectors in $\\mathbb{R}^n$. We want to show that:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\| \\leq \\|\\mathbf{a}\\| + \\|\\mathbf{b}\\|\n\\]\n\nThe magnitude (or norm) of a vector $\\mathbf{v}$ is given by $\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}$, where $\\cdot$ denotes the dot product.\n\nFirst, consider the square of the magnitude of the sum of the vectors:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\|^2 = (\\mathbf{a} + \\mathbf{b}) \\cdot (\\mathbf{a} + \\mathbf{b})\n\\]\n\nExpanding the dot product, we have:\n\n\\[\n(\\mathbf{a} + \\mathbf{b}) \\cdot (\\mathbf{a} + \\mathbf{b}) = \\mathbf{a} \\cdot \\mathbf{a} + 2 \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{b} \\cdot \\mathbf{b}\n\\]\n\nThis can be rewritten using the magnitudes of the vectors:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\|^2 = \\|\\mathbf{a}\\|^2 + 2 \\mathbf{a} \\cdot \\mathbf{b} + \\|\\mathbf{b}\\|^2\n\\]\n\nBy the Cauchy-Schwarz inequality, we know that:\n\n\\[\n|\\mathbf{a} \\cdot \\mathbf{b}| \\leq \\|\\mathbf{a}\\| \\|\\mathbf{b}\\|\n\\]\n\nThus, we have:\n\n\\[\n2 \\mathbf{a} \\cdot \\mathbf{b} \\leq 2 \\|\\mathbf{a}\\| \\|\\mathbf{b}\\|\n\\]\n\nSubstituting this into our expression for $\\|\\mathbf{a} + \\mathbf{b}\\|^2$, we get:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\|^2 \\leq \\|\\mathbf{a}\\|^2 + 2 \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| + \\|\\mathbf{b}\\|^2\n\\]\n\nThe right-hand side is a perfect square:\n\n\\[\n\\|\\mathbf{a}\\|^2 + 2 \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| + \\|\\mathbf{b}\\|^2 = (\\|\\mathbf{a}\\| + \\|\\mathbf{b}\\|)^2\n\\]\n\nTherefore, we have:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\|^2 \\leq (\\|\\mathbf{a}\\| + \\|\\mathbf{b}\\|)^2\n\\]\n\nTaking the square root of both sides, we obtain:\n\n\\[\n\\|\\mathbf{a} + \\mathbf{b}\\| \\leq \\|\\mathbf{a}\\| + \\|\\mathbf{b}\\|\n\\]\n\nThis completes the proof that the magnitude of the sum of two vectors is never greater than the sum of their magnitudes."
    },
    {
      "id": "13",
      "statement": "The dot product of a vector with another vector along the same direction is equal to the product of their magnitudes.",
      "proof": "To prove this statement, let's consider two vectors $\\mathbf{a}$ and $\\mathbf{b}$, where $\\mathbf{b}$ is in the same direction as $\\mathbf{a}$. This means that $\\mathbf{b}$ can be expressed as a scalar multiple of $\\mathbf{a}$, i.e., $\\mathbf{b} = k \\mathbf{a}$ for some scalar $k > 0$.\n\nThe dot product of two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is given by:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta\n\\]\n\nwhere $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ are the magnitudes of vectors $\\mathbf{a}$ and $\\mathbf{b}$, respectively, and $\\theta$ is the angle between them.\n\nSince $\\mathbf{b}$ is in the same direction as $\\mathbf{a}$, the angle $\\theta$ between them is $0$ degrees. Therefore, $\\cos \\theta = \\cos 0 = 1$.\n\nSubstituting this into the dot product formula, we have:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cdot 1 = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\|\n\\]\n\nNow, since $\\mathbf{b} = k \\mathbf{a}$, the magnitude of $\\mathbf{b}$ is:\n\n\\[\n\\|\\mathbf{b}\\| = \\|k \\mathbf{a}\\| = |k| \\|\\mathbf{a}\\|\n\\]\n\nGiven that $k > 0$, we have $|k| = k$. Therefore:\n\n\\[\n\\|\\mathbf{b}\\| = k \\|\\mathbf{a}\\|\n\\]\n\nSubstituting this back into the expression for the dot product, we get:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| (k \\|\\mathbf{a}\\|) = k \\|\\mathbf{a}\\|^2\n\\]\n\nSince $\\mathbf{b} = k \\mathbf{a}$, the dot product can also be calculated as:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a} \\cdot (k \\mathbf{a}) = k (\\mathbf{a} \\cdot \\mathbf{a}) = k \\|\\mathbf{a}\\|^2\n\\]\n\nThus, the dot product of a vector with another vector along the same direction is indeed equal to the product of their magnitudes, confirming the statement."
    },
    {
      "id": "14",
      "statement": "The vector resulting from the cross product of two vectors is perpendicular to both original vectors.",
      "proof": "To prove that the vector resulting from the cross product of two vectors is perpendicular to both original vectors, let's consider two vectors $\\mathbf{a} = \\langle a_1, a_2, a_3 \\rangle$ and $\\mathbf{b} = \\langle b_1, b_2, b_3 \\rangle$ in $\\mathbb{R}^3$. The cross product $\\mathbf{a} \\times \\mathbf{b}$ is defined as:\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = \\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1 \\rangle\n\\]\n\nWe need to show that $\\mathbf{a} \\times \\mathbf{b}$ is perpendicular to both $\\mathbf{a}$ and $\\mathbf{b}$. A vector $\\mathbf{v}$ is perpendicular to another vector $\\mathbf{w}$ if their dot product is zero, i.e., $\\mathbf{v} \\cdot \\mathbf{w} = 0$.\n\nFirst, let's compute the dot product of $\\mathbf{a} \\times \\mathbf{b}$ with $\\mathbf{a}$:\n\n\\[\n\\mathbf{a} \\cdot (\\mathbf{a} \\times \\mathbf{b}) = \\langle a_1, a_2, a_3 \\rangle \\cdot \\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1 \\rangle\n\\]\n\n\\[\n= a_1(a_2b_3 - a_3b_2) + a_2(a_3b_1 - a_1b_3) + a_3(a_1b_2 - a_2b_1)\n\\]\n\nExpanding this expression, we get:\n\n\\[\n= a_1a_2b_3 - a_1a_3b_2 + a_2a_3b_1 - a_2a_1b_3 + a_3a_1b_2 - a_3a_2b_1\n\\]\n\nNotice that each term cancels with another:\n\n\\[\n= (a_1a_2b_3 - a_2a_1b_3) + (a_2a_3b_1 - a_3a_2b_1) + (a_3a_1b_2 - a_1a_3b_2) = 0\n\\]\n\nThus, $\\mathbf{a} \\cdot (\\mathbf{a} \\times \\mathbf{b}) = 0$, which means $\\mathbf{a} \\times \\mathbf{b}$ is perpendicular to $\\mathbf{a}$.\n\nNext, let's compute the dot product of $\\mathbf{a} \\times \\mathbf{b}$ with $\\mathbf{b}$:\n\n\\[\n\\mathbf{b} \\cdot (\\mathbf{a} \\times \\mathbf{b}) = \\langle b_1, b_2, b_3 \\rangle \\cdot \\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1 \\rangle\n\\]\n\n\\[\n= b_1(a_2b_3 - a_3b_2) + b_2(a_3b_1 - a_1b_3) + b_3(a_1b_2 - a_2b_1)\n\\]\n\nExpanding this expression, we get:\n\n\\[\n= b_1a_2b_3 - b_1a_3b_2 + b_2a_3b_1 - b_2a_1b_3 + b_3a_1b_2 - b_3a_2b_1\n\\]\n\nAgain, notice that each term cancels with another:\n\n\\[\n= (b_1a_2b_3 - b_3a_2b_1) + (b_2a_3b_1 - b_1a_3b_2) + (b_3a_1b_2 - b_2a_1b_3) = 0\n\\]\n\nThus, $\\mathbf{b} \\cdot (\\mathbf{a} \\times \\mathbf{b}) = 0$, which means $\\mathbf{a} \\times \\mathbf{b}$ is perpendicular to $\\mathbf{b}$.\n\nTherefore, the vector resulting from the cross product of two vectors is perpendicular to both original vectors."
    },
    {
      "id": "15",
      "statement": "The projection of a vector onto another vector is a scalar multiple of the second vector.",
      "proof": "To prove that the projection of a vector $\\mathbf{a}$ onto another vector $\\mathbf{b}$ is a scalar multiple of the vector $\\mathbf{b}$, we start by recalling the formula for the projection of $\\mathbf{a}$ onto $\\mathbf{b}$.\n\nThe projection of vector $\\mathbf{a}$ onto vector $\\mathbf{b}$ is given by:\n\n\\[\n\\text{proj}_{\\mathbf{b}} \\mathbf{a} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{b} \\cdot \\mathbf{b}} \\mathbf{b}\n\\]\n\nHere, $\\mathbf{a} \\cdot \\mathbf{b}$ denotes the dot product of vectors $\\mathbf{a}$ and $\\mathbf{b}$, and $\\mathbf{b} \\cdot \\mathbf{b}$ is the dot product of $\\mathbf{b}$ with itself, which is equivalent to the square of the magnitude of $\\mathbf{b}$, i.e., $\\|\\mathbf{b}\\|^2$.\n\nThe expression for the projection can be rewritten as:\n\n\\[\n\\text{proj}_{\\mathbf{b}} \\mathbf{a} = \\left( \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2} \\right) \\mathbf{b}\n\\]\n\nNotice that the term $\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2}$ is a scalar. Let's denote this scalar by $k$:\n\n\\[\nk = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2}\n\\]\n\nThus, the projection of $\\mathbf{a}$ onto $\\mathbf{b}$ can be expressed as:\n\n\\[\n\\text{proj}_{\\mathbf{b}} \\mathbf{a} = k \\mathbf{b}\n\\]\n\nThis shows that the projection of vector $\\mathbf{a}$ onto vector $\\mathbf{b}$ is indeed a scalar multiple of the vector $\\mathbf{b}$."
    }
  ]
}

