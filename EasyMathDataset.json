[
    {
        "id": "1",
        "topic": "Linear Algebra",
        "problem": "The sum of two vectors in a vector space is also a vector in that same space.",
        "proof": "To prove that the sum of two vectors in a vector space is also a vector in that same space, we need to refer to the definition of a vector space and its axioms.\n\nLet $V$ be a vector space over a field $\\mathbb{F}$. By definition, a vector space is a set equipped with two operations: vector addition and scalar multiplication, satisfying certain axioms.\n\nOne of these axioms is the **closure under addition**. This axiom states that for any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $V$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $V$.\n\n**Proof:**\n\n1. **Given:** $\\mathbf{u}, \\mathbf{v} \\in V$.\n\n2. **Axiom of Closure under Addition:** By the definition of a vector space, for any vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $V$, the sum $\\mathbf{u} + \\mathbf{v}$ must also be in $V$.\n\n3. **Conclusion:** Therefore, the sum of two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in a vector space $V$ is also a vector in $V$.\n\nThis property is fundamental to the structure of a vector space and is one of the key reasons why vector spaces are useful in mathematics and its applications."
    },
    {
        "id": "2",
        "topic": "Linear Algebra",
        "problem": "The zero vector is the only vector that when added to another vector does not change that vector.",
        "proof": "To prove that the zero vector is the only vector that, when added to another vector, does not change that vector, we need to consider the properties of vector addition in a vector space.\n\nLet $\\mathbf{v}$ be any vector in a vector space $V$. We want to show that if there exists a vector $\\mathbf{z}$ such that $\\mathbf{v} + \\mathbf{z} = \\mathbf{v}$, then $\\mathbf{z}$ must be the zero vector, denoted by $\\mathbf{0}$.\n\n**Proof:**\n\n1. **Existence of the Zero Vector:**\n   By the definition of a vector space, there exists a unique zero vector $\\mathbf{0}$ in $V$ such that for any vector $\\mathbf{v} \\in V$, the equation $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ holds.\n\n2. **Assumption:**\n   Assume there exists a vector $\\mathbf{z} \\in V$ such that $\\mathbf{v} + \\mathbf{z} = \\mathbf{v}$.\n\n3. **Subtracting $\\mathbf{v}$ from Both Sides:**\n   We can subtract $\\mathbf{v}$ from both sides of the equation $\\mathbf{v} + \\mathbf{z} = \\mathbf{v}$ using the additive inverse property. The additive inverse of $\\mathbf{v}$ is a vector $-\\mathbf{v}$ such that $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$.\n\n   \\[\n   (\\mathbf{v} + \\mathbf{z}) + (-\\mathbf{v}) = \\mathbf{v} + (-\\mathbf{v})\n   \\]\n\n4. **Associativity of Vector Addition:**\n   By the associativity of vector addition, we have:\n\n   \\[\n   \\mathbf{v} + (\\mathbf{z} + (-\\mathbf{v})) = \\mathbf{0}\n   \\]\n\n5. **Simplifying:**\n   Since $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$, the equation simplifies to:\n\n   \\[\n   \\mathbf{z} + \\mathbf{0} = \\mathbf{0}\n   \\]\n\n6. **Identity Property:**\n   By the identity property of the zero vector, $\\mathbf{z} + \\mathbf{0} = \\mathbf{z}$.\n\n7. **Conclusion:**\n   Therefore, $\\mathbf{z} = \\mathbf{0}$.\n\nThus, the zero vector $\\mathbf{0}$ is the only vector that, when added to another vector, does not change that vector. This completes the proof."
    },
    {
        "id": "3",
        "topic": "Linear Algebra",
        "problem": "A scalar multiple of a vector in a vector space is also a vector in that same space.",
        "proof": "To prove that a scalar multiple of a vector in a vector space is also a vector in that same space, we need to refer to the axioms that define a vector space.\n\nLet $V$ be a vector space over a field $\\mathbb{F}$. By definition, $V$ satisfies certain axioms, including closure under scalar multiplication. This means that for any vector $\\mathbf{v} \\in V$ and any scalar $c \\in \\mathbb{F}$, the product $c\\mathbf{v}$ is also in $V$.\n\n**Proof:**\n\n1. **Definition of a Vector Space:**\n   A vector space $V$ over a field $\\mathbb{F}$ is a set equipped with two operations: vector addition and scalar multiplication. These operations satisfy the following axioms:\n   - Closure under addition: For all $\\mathbf{u}, \\mathbf{v} \\in V$, the sum $\\mathbf{u} + \\mathbf{v} \\in V$.\n   - Closure under scalar multiplication: For all $\\mathbf{v} \\in V$ and $c \\in \\mathbb{F}$, the product $c\\mathbf{v} \\in V$.\n   - Other axioms (associativity, commutativity of addition, existence of additive identity and inverses, distributive properties, etc.).\n\n2. **Scalar Multiplication Axiom:**\n   The axiom of closure under scalar multiplication directly states that for any vector $\\mathbf{v} \\in V$ and any scalar $c \\in \\mathbb{F}$, the scalar multiple $c\\mathbf{v}$ is also in $V$.\n\n3. **Conclusion:**\n   Since the closure under scalar multiplication is one of the defining properties of a vector space, it follows that a scalar multiple of any vector in $V$ is also a vector in $V$. Therefore, if $\\mathbf{v} \\in V$ and $c \\in \\mathbb{F}$, then $c\\mathbf{v} \\in V$.\n\nThis completes the proof that a scalar multiple of a vector in a vector space is also a vector in that same space."
    },
    {
        "id": "4",
        "topic": "Linear Algebra",
        "problem": "A matrix multiplied by the identity matrix results in the original matrix.",
        "proof": "To prove that a matrix multiplied by the identity matrix results in the original matrix, let's consider a matrix $A$ of size $m \\times n$ and the identity matrix $I_n$ of size $n \\times n$. The identity matrix $I_n$ is defined as a square matrix with ones on the diagonal and zeros elsewhere:\n\n\\[\nI_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\\]\n\nWe want to show that $A I_n = A$. \n\nThe product $A I_n$ is defined as follows: for each element $(i, j)$ of the resulting matrix, we have\n\n\\[\n(A I_n)_{ij} = \\sum_{k=1}^{n} A_{ik} (I_n)_{kj}\n\\]\n\nSince $I_n$ is the identity matrix, $(I_n)_{kj} = 1$ if $k = j$ and $(I_n)_{kj} = 0$ otherwise. Therefore, the sum simplifies to:\n\n\\[\n(A I_n)_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot \\delta_{kj} = A_{ij}\n\\]\n\nwhere $\\delta_{kj}$ is the Kronecker delta, which is 1 if $k = j$ and 0 otherwise. Thus, each element of the product $A I_n$ is equal to the corresponding element of $A$.\n\nTherefore, $A I_n = A$, proving that multiplying a matrix by the identity matrix results in the original matrix."
    },
    {
        "id": "5",
        "topic": "Linear Algebra",
        "problem": "The transpose of the transpose of a matrix is the original matrix.",
        "proof": "To prove that the transpose of the transpose of a matrix is the original matrix, let's consider a matrix $A$ of size $m \\times n$. The transpose of $A$, denoted by $A^T$, is an $n \\times m$ matrix where the rows of $A$ become the columns of $A^T$.\n\nNow, we need to show that the transpose of $A^T$, denoted by $(A^T)^T$, is equal to $A$.\n\nLet $A = [a_{ij}]$ be the original matrix, where $1 \\leq i \\leq m$ and $1 \\leq j \\leq n$. The transpose $A^T = [b_{ji}]$ is defined such that $b_{ji} = a_{ij}$ for all $i$ and $j$.\n\nNow, consider the transpose of $A^T$, which is $(A^T)^T = [c_{ij}]$. By the definition of the transpose, we have $c_{ij} = b_{ji}$ for all $i$ and $j$.\n\nSubstituting the expression for $b_{ji}$, we get $c_{ij} = a_{ij}$ for all $i$ and $j$.\n\nTherefore, $(A^T)^T = A$, which completes the proof that the transpose of the transpose of a matrix is the original matrix."
    },
    {
        "id": "6",
        "topic": "Linear Algebra",
        "problem": "If two matrices are both invertible, then their product is also invertible.",
        "proof": "To prove that the product of two invertible matrices is also invertible, let's consider two invertible matrices $A$ and $B$, both of size $n \\times n$. By definition, since $A$ is invertible, there exists a matrix $A^{-1}$ such that:\n\n\\[ A A^{-1} = A^{-1} A = I_n, \\]\n\nwhere $I_n$ is the $n \\times n$ identity matrix. Similarly, since $B$ is invertible, there exists a matrix $B^{-1}$ such that:\n\n\\[ B B^{-1} = B^{-1} B = I_n. \\]\n\nWe need to show that the product $AB$ is invertible and find its inverse. Assume $C = AB$. We claim that the inverse of $C$ is $B^{-1}A^{-1}$.\n\nTo verify this, we need to show that:\n\n\\[ (AB)(B^{-1}A^{-1}) = I_n \\quad \\text{and} \\quad (B^{-1}A^{-1})(AB) = I_n. \\]\n\nFirst, consider the product $(AB)(B^{-1}A^{-1})$:\n\n\\[\n(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = A I_n A^{-1} = AA^{-1} = I_n.\n\\]\n\nNext, consider the product $(B^{-1}A^{-1})(AB)$:\n\n\\[\n(B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1} I_n B = B^{-1}B = I_n.\n\\]\n\nSince both products result in the identity matrix $I_n$, we have shown that $B^{-1}A^{-1}$ is indeed the inverse of $AB$. Therefore, the product $AB$ is invertible, and its inverse is $B^{-1}A^{-1}$. This completes the proof."
    },
    {
        "id": "7",
        "topic": "Linear Algebra",
        "problem": "The determinant of a diagonal matrix is the product of its diagonal elements.",
        "proof": "To prove that the determinant of a diagonal matrix is the product of its diagonal elements, let's consider a diagonal matrix $A$ of size $n \\times n$:\n\n\\[\nA = \\begin{bmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\n0 & a_{22} & 0 & \\cdots & 0 \\\\\n0 & 0 & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\\]\n\nThe determinant of a matrix $A$, denoted $\\det(A)$, is defined as the sum of all products of elements of $A$ such that each product contains exactly one element from each row and each column, multiplied by the sign of the permutation of the columns.\n\nFor a diagonal matrix, the only non-zero products are those that involve selecting the diagonal elements $a_{11}, a_{22}, \\ldots, a_{nn}$, because all off-diagonal elements are zero. Therefore, the only non-zero term in the determinant expansion is:\n\n\\[\n\\det(A) = a_{11} \\cdot a_{22} \\cdot \\cdots \\cdot a_{nn}\n\\]\n\nThis is because any permutation of the columns that selects an off-diagonal element will result in a product that includes a zero, making the entire product zero.\n\nThus, the determinant of a diagonal matrix is simply the product of its diagonal elements:\n\n\\[\n\\det(A) = a_{11} \\times a_{22} \\times \\cdots \\times a_{nn}\n\\]\n\nThis completes the proof."
    },
    {
        "id": "8",
        "topic": "Linear Algebra",
        "problem": "The trace of a matrix is the sum of its diagonal elements.",
        "proof": "To prove that the trace of a matrix is the sum of its diagonal elements, let's consider an $n \\times n$ square matrix $A = [a_{ij}]$, where $a_{ij}$ represents the element in the $i$-th row and $j$-th column of the matrix.\n\nThe trace of a matrix, denoted as $\\text{tr}(A)$, is defined as the sum of the elements on the main diagonal of the matrix. The main diagonal of a matrix consists of the elements $a_{11}, a_{22}, \\ldots, a_{nn}$.\n\nTherefore, the trace of the matrix $A$ is given by:\n\n\\[\n\\text{tr}(A) = a_{11} + a_{22} + \\cdots + a_{nn} = \\sum_{i=1}^{n} a_{ii}\n\\]\n\nThis expression clearly shows that the trace of the matrix $A$ is indeed the sum of its diagonal elements. Thus, the statement is proven."
    },
    {
        "id": "9",
        "topic": "Linear Algebra",
        "problem": "The rank of a matrix is the dimension of the vector space generated by its rows.",
        "proof": "To prove that the rank of a matrix is the dimension of the vector space generated by its rows, we need to show that the rank of a matrix, defined as the maximum number of linearly independent rows (or columns), is equal to the dimension of the row space of the matrix.\n\nLet $A$ be an $m \\times n$ matrix. The row space of $A$ is the subspace of $\\mathbb{R}^n$ spanned by the row vectors of $A$. The rank of $A$, denoted as $\\text{rank}(A)$, is defined as the maximum number of linearly independent row vectors in $A$.\n\n**Proof:**\n\n1. **Row Space and Linear Independence:**\n\n   The row space of $A$ is the set of all linear combinations of the row vectors of $A$. If we denote the rows of $A$ as $\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m$, then the row space is:\n   \\[\n   \\text{Row Space}(A) = \\text{span}\\{\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m\\}.\n   \\]\n\n2. **Dimension of the Row Space:**\n\n   The dimension of the row space is the number of vectors in a basis for the row space. A basis is a set of linearly independent vectors that span the space. Therefore, the dimension of the row space is the maximum number of linearly independent row vectors in $A$.\n\n3. **Rank of the Matrix:**\n\n   By definition, the rank of the matrix $A$ is the maximum number of linearly independent row vectors in $A$. Therefore, the rank of $A$ is precisely the dimension of the row space of $A$.\n\n4. **Conclusion:**\n\n   Since the rank of $A$ is defined as the maximum number of linearly independent row vectors, and the dimension of the row space is the number of vectors in a basis for the row space, we conclude that:\n   \\[\n   \\text{rank}(A) = \\dim(\\text{Row Space}(A)).\n   \\]\n\nThus, the rank of a matrix is indeed the dimension of the vector space generated by its rows."
    },
    {
        "id": "10",
        "topic": "Linear Algebra",
        "problem": "The eigenvectors of a matrix corresponding to distinct eigenvalues are linearly independent.",
        "proof": "To prove that the eigenvectors of a matrix corresponding to distinct eigenvalues are linearly independent, let's consider a matrix $A \\in \\mathbb{C}^{n \\times n}$ and suppose it has $k$ distinct eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_k$ with corresponding eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k$. We want to show that the set $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}$ is linearly independent.\n\nAssume, for the sake of contradiction, that the eigenvectors are linearly dependent. Then there exist scalars $c_1, c_2, \\ldots, c_k$, not all zero, such that:\n\n\\[\nc_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k = \\mathbf{0}.\n\\]\n\nWithout loss of generality, assume $c_1 \\neq 0$. We can express $\\mathbf{v}_1$ as a linear combination of the other eigenvectors:\n\n\\[\n\\mathbf{v}_1 = -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k.\n\\]\n\nNow, apply the matrix $A$ to both sides of the equation:\n\n\\[\nA \\mathbf{v}_1 = A \\left( -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k \\right).\n\\]\n\nSince $\\mathbf{v}_1$ is an eigenvector corresponding to the eigenvalue $\\lambda_1$, we have:\n\n\\[\n\\lambda_1 \\mathbf{v}_1 = -\\frac{c_2}{c_1} A \\mathbf{v}_2 - \\frac{c_3}{c_1} A \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} A \\mathbf{v}_k.\n\\]\n\nSubstituting $A \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$ for each $i$, we get:\n\n\\[\n\\lambda_1 \\mathbf{v}_1 = -\\frac{c_2}{c_1} \\lambda_2 \\mathbf{v}_2 - \\frac{c_3}{c_1} \\lambda_3 \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\lambda_k \\mathbf{v}_k.\n\\]\n\nSubstitute the expression for $\\mathbf{v}_1$ from the linear dependence relation:\n\n\\[\n\\lambda_1 \\left( -\\frac{c_2}{c_1} \\mathbf{v}_2 - \\frac{c_3}{c_1} \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\mathbf{v}_k \\right) = -\\frac{c_2}{c_1} \\lambda_2 \\mathbf{v}_2 - \\frac{c_3}{c_1} \\lambda_3 \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} \\lambda_k \\mathbf{v}_k.\n\\]\n\nThis simplifies to:\n\n\\[\n-\\frac{c_2}{c_1} (\\lambda_1 - \\lambda_2) \\mathbf{v}_2 - \\frac{c_3}{c_1} (\\lambda_1 - \\lambda_3) \\mathbf{v}_3 - \\cdots - \\frac{c_k}{c_1} (\\lambda_1 - \\lambda_k) \\mathbf{v}_k = \\mathbf{0}.\n\\]\n\nSince the eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_k$ are distinct, each term $(\\lambda_1 - \\lambda_i) \\neq 0$ for $i = 2, 3, \\ldots, k$. Therefore, each coefficient $-\\frac{c_i}{c_1} (\\lambda_1 - \\lambda_i)$ must be zero, implying $c_i = 0$ for $i = 2, 3, \\ldots, k$. This contradicts our assumption that not all $c_i$ are zero. Hence, the eigenvectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}$ must be linearly independent."
    },
    {
        "id": "11",
        "topic": "Geometry",
        "problem": "The sum of the angles in a triangle is 180 degrees.",
        "proof": "To prove that the sum of the angles in a triangle is 180 degrees, we can use a geometric approach involving parallel lines.\n\nConsider a triangle $\\triangle ABC$. We want to show that the sum of the interior angles $\\angle A + \\angle B + \\angle C = 180^\\circ$.\n\n1. Draw a line parallel to the side $BC$ of the triangle through the vertex $A$. Let this line intersect the extensions of sides $AB$ and $AC$ at points $D$ and $E$, respectively.\n\n2. Since $DE$ is parallel to $BC$, by the Alternate Interior Angles Theorem, we have:\n   - $\\angle DAB = \\angle ACB$ (since they are alternate interior angles),\n   - $\\angle EAC = \\angle ABC$ (since they are alternate interior angles).\n\n3. The angles $\\angle DAB$, $\\angle BAC$, and $\\angle EAC$ form a straight line at point $A$, so their sum is $180^\\circ$:\n   \\[\n   \\angle DAB + \\angle BAC + \\angle EAC = 180^\\circ.\n   \\]\n\n4. Substituting the equal angles from step 2, we have:\n   \\[\n   \\angle ACB + \\angle BAC + \\angle ABC = 180^\\circ.\n   \\]\n\nThus, the sum of the angles in triangle $\\triangle ABC$ is $180^\\circ$. This completes the proof."
    },
    {
        "id": "12",
        "topic": "Geometry",
        "problem": "The area of a rectangle is the product of its length and width.",
        "proof": "To prove that the area of a rectangle is the product of its length and width, we start by considering a rectangle with length $l$ and width $w$. \n\nThe area of a rectangle is defined as the amount of two-dimensional space it occupies. We can determine this area by considering the rectangle as a collection of unit squares.\n\n1. **Unit Squares Approach:**\n   - Imagine dividing the rectangle into a grid of $1 \\times 1$ unit squares.\n   - Along the length of the rectangle, there are $l$ unit squares.\n   - Along the width of the rectangle, there are $w$ unit squares.\n   - Therefore, the total number of unit squares that fit into the rectangle is $l \\times w$.\n\n2. **Mathematical Justification:**\n   - Each row of the rectangle contains $l$ unit squares, and there are $w$ such rows.\n   - Thus, the total number of unit squares, which is the area of the rectangle, is $l \\times w$.\n\nTherefore, the area $A$ of the rectangle is given by the formula:\n\n\\[ A = l \\times w \\]\n\nThis formula holds for any rectangle, as it is based on the fundamental definition of area in terms of unit squares. Thus, the area of a rectangle is indeed the product of its length and width."
    },
    {
        "id": "13",
        "topic": "Geometry",
        "problem": "The circumference of a circle is twice its radius multiplied by \u03c0.",
        "proof": "To prove that the circumference of a circle is twice its radius multiplied by $\\pi$, we start with the definition of the circumference of a circle.\n\nThe circumference $C$ of a circle is the distance around the circle. By definition, the circumference is given by the formula:\n\n\\[ C = 2\\pi r \\]\n\nwhere $r$ is the radius of the circle.\n\n**Proof:**\n\n1. **Definition of $\\pi$:**\n\n   The number $\\pi$ is defined as the ratio of the circumference of any circle to its diameter. That is:\n\n   \\[ \\pi = \\frac{C}{d} \\]\n\n   where $d$ is the diameter of the circle.\n\n2. **Relation between diameter and radius:**\n\n   The diameter $d$ of a circle is twice the radius $r$. Therefore, we have:\n\n   \\[ d = 2r \\]\n\n3. **Substitute the diameter in the definition of $\\pi$:**\n\n   Substitute $d = 2r$ into the definition of $\\pi$:\n\n   \\[ \\pi = \\frac{C}{2r} \\]\n\n4. **Solve for the circumference $C$:**\n\n   Multiply both sides of the equation by $2r$ to solve for $C$:\n\n   \\[ C = 2\\pi r \\]\n\nThus, we have shown that the circumference of a circle is indeed twice its radius multiplied by $\\pi$. This completes the proof."
    },
    {
        "id": "14",
        "topic": "Geometry",
        "problem": "A tangent to a circle is perpendicular to the radius at the point of tangency.",
        "proof": "To prove that a tangent to a circle is perpendicular to the radius at the point of tangency, consider a circle with center $O$ and a tangent line $L$ that touches the circle at point $P$. We need to show that the radius $OP$ is perpendicular to the tangent line $L$ at point $P$.\n\n**Proof:**\n\n1. **Definition of a Tangent Line:**\n   By definition, a tangent line to a circle is a line that intersects the circle at exactly one point. In this case, the tangent line $L$ intersects the circle at point $P$.\n\n2. **Assume the Contrary:**\n   Assume, for the sake of contradiction, that the radius $OP$ is not perpendicular to the tangent line $L$. This means there exists some point $Q$ on the line $L$ such that $OQ < OP$.\n\n3. **Construct a Triangle:**\n   Consider the triangle $\\triangle OPQ$. Since $Q$ lies on the tangent line $L$, and $L$ is assumed to be tangent at $P$, $Q$ must lie outside the circle. Therefore, $OQ > OP$ for any point $Q$ on $L$ other than $P$.\n\n4. **Contradiction:**\n   The assumption that $OQ < OP$ contradicts the fact that $OQ > OP$ for any point $Q$ on the tangent line $L$ other than $P$. Therefore, our assumption that $OP$ is not perpendicular to $L$ must be false.\n\n5. **Conclusion:**\n   Since the assumption leads to a contradiction, the radius $OP$ must be perpendicular to the tangent line $L$ at the point of tangency $P$.\n\nThus, we have shown that a tangent to a circle is perpendicular to the radius at the point of tangency."
    },
    {
        "id": "15",
        "topic": "Geometry",
        "problem": "The diagonals of a rectangle bisect each other.",
        "proof": "To prove that the diagonals of a rectangle bisect each other, let's consider a rectangle $ABCD$ with vertices $A$, $B$, $C$, and $D$. We need to show that the diagonals $AC$ and $BD$ intersect at a point $E$ such that $AE = EC$ and $BE = ED$.\n\n### Proof:\n\n1. **Coordinates of the vertices:**\n   - Place the rectangle in the coordinate plane with $A = (0, 0)$, $B = (a, 0)$, $C = (a, b)$, and $D = (0, b)$, where $a$ and $b$ are the lengths of the sides of the rectangle.\n\n2. **Equations of the diagonals:**\n   - The diagonal $AC$ has endpoints $A(0, 0)$ and $C(a, b)$. The midpoint $E$ of $AC$ is given by:\n     \\[\n     E = \\left( \\frac{0 + a}{2}, \\frac{0 + b}{2} \\right) = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)\n     \\]\n\n   - The diagonal $BD$ has endpoints $B(a, 0)$ and $D(0, b)$. The midpoint $E$ of $BD$ is given by:\n     \\[\n     E = \\left( \\frac{a + 0}{2}, \\frac{0 + b}{2} \\right) = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)\n     \\]\n\n3. **Conclusion:**\n   - The midpoints of both diagonals $AC$ and $BD$ are the same point $E = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)$.\n   - Therefore, the diagonals $AC$ and $BD$ bisect each other at point $E$.\n\nThis completes the proof that the diagonals of a rectangle bisect each other."
    },
    {
        "id": "16",
        "topic": "Geometry",
        "problem": "The sum of the interior angles of a quadrilateral is 360 degrees.",
        "proof": "To prove that the sum of the interior angles of a quadrilateral is 360 degrees, we can use the fact that a quadrilateral can be divided into two triangles.\n\nConsider a quadrilateral $ABCD$. We can draw a diagonal, say $AC$, which divides the quadrilateral into two triangles: $\\triangle ABC$ and $\\triangle ACD$.\n\nWe know that the sum of the interior angles of a triangle is 180 degrees. Therefore, for $\\triangle ABC$, we have:\n\n\\[\n\\angle BAC + \\angle ABC + \\angle ACB = 180^\\circ\n\\]\n\nSimilarly, for $\\triangle ACD$, we have:\n\n\\[\n\\angle CAD + \\angle ACD + \\angle ADC = 180^\\circ\n\\]\n\nAdding these two equations gives the sum of the interior angles of the quadrilateral $ABCD$:\n\n\\[\n(\\angle BAC + \\angle ABC + \\angle ACB) + (\\angle CAD + \\angle ACD + \\angle ADC) = 180^\\circ + 180^\\circ = 360^\\circ\n\\]\n\nNotice that $\\angle BAC$ and $\\angle CAD$ are the same angle, as they are both angles at vertex $A$. Therefore, the sum of the interior angles of the quadrilateral $ABCD$ is:\n\n\\[\n\\angle ABC + \\angle BCD + \\angle CDA + \\angle DAB = 360^\\circ\n\\]\n\nThus, the sum of the interior angles of a quadrilateral is 360 degrees."
    },
    {
        "id": "17",
        "topic": "Geometry",
        "problem": "An equilateral triangle has all sides of equal length and all angles equal to 60 degrees.",
        "proof": "To prove that an equilateral triangle has all sides of equal length and all angles equal to 60 degrees, we can use the properties of triangles and the definition of an equilateral triangle.\n\n**Proof:**\n\n1. **Definition of an Equilateral Triangle:**\n   An equilateral triangle is defined as a triangle in which all three sides are of equal length. Let the triangle be $\\triangle ABC$ with sides $AB = BC = CA = s$.\n\n2. **Sum of Angles in a Triangle:**\n   The sum of the interior angles in any triangle is $180^\\circ$. Therefore, for $\\triangle ABC$, we have:\n   \\[\n   \\angle A + \\angle B + \\angle C = 180^\\circ\n   \\]\n\n3. **Equal Angles in an Equilateral Triangle:**\n   Since all sides of the triangle are equal, by the Isosceles Triangle Theorem, the angles opposite those sides must also be equal. Therefore, we have:\n   \\[\n   \\angle A = \\angle B = \\angle C\n   \\]\n\n4. **Calculate Each Angle:**\n   Since the sum of the angles is $180^\\circ$ and all angles are equal, each angle must be:\n   \\[\n   \\angle A = \\angle B = \\angle C = \\frac{180^\\circ}{3} = 60^\\circ\n   \\]\n\nThus, in an equilateral triangle, not only are all sides equal, but all angles are also equal to $60^\\circ$. This completes the proof."
    },
    {
        "id": "18",
        "topic": "Geometry",
        "problem": "If two sides of a triangle are equal, the angles opposite those sides are also equal.",
        "proof": "To prove this statement, we will use the properties of isosceles triangles. Consider a triangle $\\triangle ABC$ where $AB = AC$. We need to show that the angles opposite these sides, namely $\\angle B$ and $\\angle C$, are equal.\n\n**Proof:**\n\n1. **Given:** Triangle $\\triangle ABC$ with $AB = AC$.\n\n2. **To Prove:** $\\angle B = \\angle C$.\n\n3. **Construction:** Draw the angle bisector of $\\angle A$, which intersects side $BC$ at point $D$.\n\n4. **Proof:**\n\n   - Since $AB = AC$, triangle $\\triangle ABC$ is isosceles.\n   \n   - By the Angle Bisector Theorem, the angle bisector of an angle in a triangle divides the opposite side into segments that are proportional to the adjacent sides. Therefore, $\\frac{BD}{DC} = \\frac{AB}{AC}$.\n   \n   - Since $AB = AC$, it follows that $\\frac{BD}{DC} = 1$, which implies $BD = DC$.\n   \n   - Now, consider triangles $\\triangle ABD$ and $\\triangle ACD$:\n     - $AB = AC$ (Given)\n     - $AD = AD$ (Common side)\n     - $BD = DC$ (From the Angle Bisector Theorem and $AB = AC$)\n   \n   - By the Side-Side-Side (SSS) Congruence Theorem, $\\triangle ABD \\cong \\triangle ACD$.\n   \n   - Therefore, by the Corresponding Parts of Congruent Triangles (CPCTC), $\\angle ABD = \\angle ACD$.\n\n5. **Conclusion:** Since $\\angle ABD = \\angle ACD$, we have $\\angle B = \\angle C$.\n\nThus, in a triangle with two equal sides, the angles opposite those sides are also equal. This completes the proof."
    },
    {
        "id": "19",
        "topic": "Geometry",
        "problem": "Parallel lines never intersect.",
        "proof": "To prove that parallel lines never intersect, we rely on the definition of parallel lines and the properties of Euclidean geometry.\n\n**Definition**: In Euclidean geometry, two lines are said to be parallel if they are coplanar and do not intersect, no matter how far they are extended in either direction.\n\n**Proof**:\n\n1. **Assumption**: Let lines $L_1$ and $L_2$ be parallel lines in a plane. By definition, $L_1$ and $L_2$ have the same direction and are equidistant from each other at all points.\n\n2. **Contradiction Approach**: Assume, for the sake of contradiction, that $L_1$ and $L_2$ intersect at a point $P$.\n\n3. **Implication of Intersection**: If $L_1$ and $L_2$ intersect at point $P$, then by definition of intersection, there exists a point that lies on both $L_1$ and $L_2$. This implies that the distance between $L_1$ and $L_2$ at point $P$ is zero.\n\n4. **Contradiction with Parallel Definition**: However, the definition of parallel lines states that they are equidistant at all points, which means the distance between them cannot be zero at any point unless they are the same line. Since $L_1$ and $L_2$ are distinct lines, this leads to a contradiction.\n\n5. **Conclusion**: The assumption that $L_1$ and $L_2$ intersect leads to a contradiction with the definition of parallel lines. Therefore, our initial assumption must be false, and parallel lines do not intersect.\n\nThus, in Euclidean geometry, parallel lines never intersect."
    },
    {
        "id": "20",
        "topic": "Geometry",
        "problem": "The perpendicular bisector of a segment is the set of all points equidistant from the segment's endpoints.",
        "proof": "To prove that the perpendicular bisector of a segment is the set of all points equidistant from the segment's endpoints, consider a segment $AB$ with midpoint $M$. The perpendicular bisector of $AB$ is the line that passes through $M$ and is perpendicular to $AB$.\n\n**Proof:**\n\n1. **Show that any point on the perpendicular bisector is equidistant from $A$ and $B$:**\n\n   Let $P$ be any point on the perpendicular bisector of $AB$. By definition, $P$ is on the line that is perpendicular to $AB$ and passes through the midpoint $M$ of $AB$. \n\n   Since $M$ is the midpoint, $AM = MB$. Also, since $P$ is on the perpendicular bisector, the triangles $\\triangle AMP$ and $\\triangle BMP$ are right triangles with $AM = MB$ and $MP$ as a common side.\n\n   By the Pythagorean Theorem, we have:\n   \\[\n   AP^2 = AM^2 + MP^2\n   \\]\n   \\[\n   BP^2 = MB^2 + MP^2\n   \\]\n\n   Since $AM = MB$, it follows that $AP^2 = BP^2$, and thus $AP = BP$. Therefore, any point $P$ on the perpendicular bisector is equidistant from $A$ and $B$.\n\n2. **Show that any point equidistant from $A$ and $B$ lies on the perpendicular bisector:**\n\n   Let $Q$ be a point such that $QA = QB$. We need to show that $Q$ lies on the perpendicular bisector of $AB$.\n\n   Since $QA = QB$, the point $Q$ must lie on the locus of points equidistant from $A$ and $B$, which is a line. This line is the perpendicular bisector of $AB$.\n\n   To see why, consider the triangles $\\triangle QAM$ and $\\triangle QBM$. Since $QA = QB$ and $AM = MB$, and $QM$ is a common side, by the SSS (Side-Side-Side) congruence criterion, $\\triangle QAM \\cong \\triangle QBM$. Therefore, $\\angle QAM = \\angle QBM$, which implies that $QM$ is perpendicular to $AB$.\n\n   Thus, $Q$ lies on the perpendicular bisector of $AB$.\n\nCombining both parts, we conclude that the perpendicular bisector of a segment is indeed the set of all points equidistant from the segment's endpoints."
    }
]